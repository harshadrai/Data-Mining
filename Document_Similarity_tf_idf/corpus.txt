the combination of flit buffer flow control methods and latency insensitive protocols is an effective solution for networks on chip noc since they both rely on backpressure the two techniques are easy to combine while offering complementary advantages low complexity of router design and the ability to cope with long communication channels via automatic wire pipelining we study various alternative implementations of this idea by considering the combination of three different types of flit buffer flow control methods and two different classes of channel repeaters based respectively on flip flops and relay stations we characterize the area and performance of the two most promising alternative implementations for nocs by completing the rtl design and logic synthesis of the repeaters and routers for different channel parallelisms finally we derive high level abstractions of our circuit designs and we use them to perform system level simulations under various scenarios for two distinct noc topologies and various applications based on our comparative analysis and experimental results we propose noc design approach that combines the reduction of the router queues to minimum size with the distribution of flit buffering onto the channels this approach provides precious flexibility during the physical design phase for many nocs particularly in those systems on chip that must be designed to meet tight constraint on the target clock frequency
we present an easy to use model that addresses the practical issues in designing bus based shared memory multiprocessor systems the model relates the shared bus width bus cycle time cache memory the features of program execution and the number of processors on shared bus to metric called request utilization the request utilization is treated as the scaling factor for the effective average waiting processors in computing the queuing delay cycles simulation study shows that the model performs very well in estimating the shared bus response time using the model system designer can quickly decide the number of the processors that shared bus is able to support effectively the size of the cache memory system should use and the bus cycle time that the main memory system should provide with the model we show that the design favors caching the requests for contention based medium instead of speeding up the transfers although the same performance can be respectively achieved by the two techniques in contention free situation
software product lines spls are used to create tailor made software products by managing and composing reusable assets generating software product from the assets of an spl is possible statically before runtime or dynamically at load time or runtime both approaches have benefits and drawbacks with respect to composition flexibility performance and resource consumption which type of composition is preferable should be decided by taking the application scenario into account current tools and languages however force programmer to decide between static and dynamic composition during development in this paper we present an approach that employs code generation to support static and dynamic composition of features of single code base we offer an implementation on top of featurec an extension of the programming language that supports software composition based on features to simplify dynamic composition and to avoid creation of invalid products we furthermore provide means to validate the correctness of composition at runtime automatically instantiate spls in case of stand alone applications and automatically apply interaction code of crosscutting concerns
experience has proved that interactive applications delivered through digital tv must provide personalized information to the viewers in order to be perceived as valuable service due to the limited computational power of dtv receivers either domestic set top boxes or mobile devices most of the existing systems have opted to place the personalization engines in dedicated servers assuming that return channel is always available for bidirectional communication however in domain where most of the information is transmitted through broadcast there are still many cases of intermittent sporadic or null access to return channel in such situations it is impossible for the servers to learn who is watching tv at the moment and so the personalization features become unavailable to solve this problem without sacrificing much personalization quality this paper introduces solutions to run downsized semantic reasoning process in the dtv receivers supported by pre selection of material driven by audience stereotypes in the head end evaluation results are presented to prove the feasibility of this approach and also to assess the quality it achieves in comparison with previous ones
model based testing techniques play vital role in producing quality software however compared to the testing of functional requirements these techniques are not prevalent that much in testing software security this paper presents model based approach to automatic testing of attack scenarios an attack testing framework is proposed to model attack scenarios and test the system with respect to the modeled attack scenarios the techniques adopted in the framework are applicable in general to the systems where the potential attack scenarios can be modeled in formalism based on extended abstract state machines the attack events ie attack test vectors chosen from the attacks happening in real world are converted to the test driver specific events ready to be tested against the attack signatures the proposed framework is implemented and evaluated using the most common attack scenarios the framework is useful to test software with respect to potential attacks which can significantly reduce the risk of security vulnerabilities
in this paper we address the problem of cache replacement for transcoding proxy caching transcoding proxy is proxy that has the functionality of transcoding multimedia object into an appropriate format or resolution for each client we first propose an effective cache replacement algorithm for transcoding proxy in general when new object is to be cached cache replacement algorithms evict some of the cached objects with the least profit to accommodate the new object our algorithm takes into account of the inter relationships among different versions of the same multimedia object and selects the versions to replace according to their aggregate profit which usually differs from simple summation of their individual profits as assumed in the existing algorithms it also considers cache consistency which is not considered in the existing algorithms we then present complexity analysis to show the efficiency of our algorithm finally we give extensive simulation results to compare the performance of our algorithm with some existing algorithms the results show that our algorithm outperforms others in terms of various performance metrics
distributed proof construction protocols have been shown to be valuable for reasoning about authorization decisions in open distributed environments such as pervasive computing spaces unfortunately existing distributed proof protocols offer only limited support for protecting the confidentiality of sensitive facts which limits their utility in many practical scenarios in this paper we propose distributed proof construction protocol in which the release of fact's truth value can be made contingent upon facts managed by other principals in the system we formally prove that our protocol can safely prove conjunctions of facts without leaking the truth values of individual facts even in the face of colluding adversaries and fact release policies with cyclical dependencies this facilitates the definition of context sensitive release policies that enable the conditional use of sensitive facts in distributed proofs
in this paper we introduce novel approach to image completion which we call structure propagation in our system the user manually specifies important missing structure information by extending few curves or line segments from the known to the unknown regions our approach synthesizes image patches along these user specified curves in the unknown region using patches selected around the curves in the known region structure propagation is formulated as global optimization problem by enforcing structure and consistency constraints if only single curve is specified structure propagation is solved using dynamic programming when multiple intersecting curves are specified we adopt the belief propagation algorithm to find the optimal patches after completing structure propagation we fill in the remaining unknown regions using patch based texture synthesis we show that our approach works well on number of examples that are challenging to state of the art techniques
as text documents are explosively increasing in the internet the process of hierarchical document clustering has been proven to be useful for grouping similar documents for versatile applications however most document clustering methods still suffer from challenges in dealing with the problems of high dimensionality scalability accuracy and meaningful cluster labels in this paper we will present an effective fuzzy frequent itemset based hierarchical clustering ihc approach which uses fuzzy association rule mining algorithm to improve the clustering accuracy of frequent itemset based hierarchical clustering fihc method in our approach the key terms will be extracted from the document set and each document is pre processed into the designated representation for the following mining process then fuzzy association rule mining algorithm for text is employed to discover set of highly related fuzzy frequent itemsets which contain key terms to be regarded as the labels of the candidate clusters finally these documents will be clustered into hierarchical cluster tree by referring to these candidate clusters we have conducted experiments to evaluate the performance based on classic hitech re reuters and wap datasets the experimental results show that our approach not only absolutely retains the merits of fihc but also improves the accuracy quality of fihc
this paper presents novel scheme for maintaining accurate information about distributed data in message passing programs we describe static single assignment ssa based algorithms to build up an intermediate representation of sequential program while targeting code generation for distributed memory machines employing the single program multiple data spmd model of programming this ssa based intermediate representation helps in variety of optimizations performed by our automatic parallelizing compiler paradigm which generates message passing programs and targets distributed memory machines in this paper we concentrate on the semantics and implementation of this ssa form for message passing programs while giving some examples of the kind of optimizations they enable we describe in detail the need for various kinds of merge functions to maintain the single assignment property of distributed data we give algorithms for placement and semantics of these merge functions and show how the requirements are substantially different owing to the presence of distributed data and arbitrary array addressing functions this scheme has been incorporated in our compiler framework which can use uniform methods to compile parallelize and optimize sequential program irrespective of the subscripts used in array addressing functions experimental results for number of benchmarks on an ibm sp show significant improvement in the total runtimes owing to some of the optimizations enabled by the ssa based intermediate representation we have observed up to around ndash reduction in total runtimes in our ssa based schemes compared to non ssa based schemes on processors
super resolution reconstruction of face image is the problem of reconstructing high resolution face image from one or more low resolution face images assuming that high and low resolution images share similar intrinsic geometries various recent super resolution methods reconstruct high resolution images based on weights determined from nearest neighbors in the local embedding of low resolution images these methods suffer disadvantages from the finite number of samples and the nature of manifold learning techniques and hence yield unrealistic reconstructed images to address the problem we apply canonical correlation analysis cca which maximizes the correlation between the local neighbor relationships of high and low resolution images we use it separately for reconstruction of global face appearance and facial details experiments using collection of frontal human faces show that the proposed algorithm improves reconstruction quality over existing state of the art super resolution algorithms both visually and using quantitative peak signal to noise ratio assessment
we consider substantial subset ofc named we develop mathematical specification for by formalizing its abstract syntax execution environment well typedness conditions and operational evaluation semantics based on this specification we prove that is type safe by showing that the execution of programs preserves the types up to subtype relationship
web site presents graph like spatial structure composed of pages connected by hyperlinks this structure may represent an environment in which situated agents associated to visitors of the web site user agents are positioned and moved in order to monitor their navigation this paper presents heterogeneous multi agent system supporting the collection of information related to user's behaviour in web site by specific situated reactive user agents the acquired information is then exploited by interface agents supporting advanced adaptive functionalities based on the history of user's movement in the web site environment interface agents also interact with user agents to acquire information on other visitors of the web site and to support context aware form of interaction among web site visitors
in many advanced database applications eg multimedia databases data objects are transformed into high dimensional points and manipulated in high dimensional space one of the most important but costly operations is the similarity join that combines similar points from multiple datasets in this paper we examine the problem of processing nearest neighbor similarity join knn join knn join between two datasets and returns for each point in its most similar points in we propose new index based knn join approach using the idistance as the underlying index structure we first present its basic algorithm and then propose two different enhancements in the first enhancement we optimize the original knn join algorithm by using approximation bounding cubes in the second enhancement we exploit the reduced dimensions of data space we conducted an extensive experimental study using both synthetic and real datasets and the results verify the performance advantage of our schemes over existing knn join algorithms
caches exploits locality of references to reduce memory access latencies and thereby improve processor performance when an operating system switches application task or performs other kernel services the assumption of locality may be violated because the instructions and data may no longer be in the cache when the preempted operation is resumed thus these operations have an additional cache interference cost that must be taken into account when calculating or estimating the performance and responsiveness of the systemin this paper we present simulation framework suitable for examining the cache interference cost in preemptive real time systems using this framework we measure the interference cost for operating system services and set of embedded benchmarksthe simulations show that there are significant performance gap between the best and worst case execution times even for simple hardware architectures also the worst case performance of some software modules was found to be more or less independent of the cache configuration these results can be used to get better understanding of the execution behavior of preemptive real time systems and can serve as guidelines for choosing suitable cache configurations
in this paper we consider the restless bandit problem which is one of the most well studied generalizations of the celebrated stochastic multi armed bandit problem in decision theory in its ultimate generality the restless bandit problem is known to be pspace hard to approximate to any non trivial factor and little progress has been made on this problem despite its significance in modeling activity allocation under uncertainty we make progress on this problem by showing that for an interesting and general subclass that we term monotone bandits surprisingly simple and intuitive greedy policy yields factor approximation such greedy policies are termed index policies and are popular due to their simplicity and their optimality for the stochastic multi armed bandit problem the monotone bandit problem strictly generalizes the stochastic multi armed bandit problem and naturally models multi project scheduling where the state of project becomes increasingly uncertain when the project is not scheduled we develop several novel techniques in the design and analysis of the index policy our algorithm proceeds by introducing novel balance constraint to the dual of well known lp relaxation to the restless bandit problem this is followed by structural characterization of the optimal solution by using both the exact primal as well as dual complementary slackness conditions this yields an interpretation of the dual variables as potential functions from which we derive the index policy and the associated analysis
we introduce straightforward robust and efficient algorithm for rendering high quality soft shadows in dynamic scenes each frame points in the scene visible from the eye are inserted into spatial acceleration structure shadow umbrae are computed by sampling the scene from the light at the image plane coordinates given by the stored points penumbrae are computed at the same set of points per silhouette edge in two steps first the set of points affected by given edge is estimated from the expected light view screen space bounds of the corresponding penumbra second the actual overlap between these points and the penumbra is computed analytically directly from the occluding geometry the umbral and penumbral sources of occlusion are then combined to determine the degree of shadow at the eye view pixel corresponding to each sample point an implementation of this algorithm for the larrabee architecture yields from to frames per second in simulation for scenes from modern game and produces significantly higher image quality than other recent methods in the real time domain
in this paper we present system using computational linguistic techniques to extract metadata for image access we discuss the implementation functionality and evaluation of an image catalogers toolkit developed in the computational linguistics for metadata building climb research project we have tested components of the system including phrase finding for the art and architecture domain functional semantic labeling using machine learning and disambiguation of terms in domain specific text vis vis rich thesaurus of subject terms geographic and artist names we present specific results on disambiguation techniques and on the nature of the ambiguity problem given the thesaurus resources and domain specific text resource with comparison of domain general resources and text our primary user group for evaluation has been the cataloger expert with specific expertise in the fields of painting sculpture and vernacular and landscape architecture
data mining is new technology that helps businesses to predict future trends and behaviours allowing them to make proactive knowledge driven decisions when data mining tools and techniques are applied on the data warehouse based on customer records they search for the hidden patterns and trends these can be further used to improve customer understanding and acquisition customer relationship management crm systems are adopted by the organisations in order to achieve success in the business and also to formulate business strategies which can be formulated based on the predictions given by the data mining tools basically three major areas of data mining research are identified implementation of crm systems evaluation criteria for data mining software and crm systems and methods to improve data quality for data mining the paper is concluded with proposed integrated model for the crm systems evaluation and implementation this paper focuses on these areas where there is need for more explorations and will provide framework for analysis of the data mining research for crm systems
in this paper we propose the novel concept of probabilistic design for multimedia embedded systems which is motivated by the challenge of how to design but not overdesign such systems while systematically incorporating performance requirements of multimedia application uncertainties in execution time and tolerance for reasonable execution failures unlike most present techniques that are based on either worst or average case execution times of application tasks where the former guarantees the completion of each execution but often leads to overdesigned systems and the latter fails to provide any completion guarantees the proposed probabilistic design method takes advantage of unique features mentioned above of multimedia systems to relax the rigid hardware requirements for software implementation and avoid overdesigning the system in essence this relaxation expands the design space and we further develop an off line on line minimum effort algorithm for quick exploration of the enlarged design space at early design stages this is the first step toward our goal of bridging the gap between real time analysis and embedded software implementation for rapid and economic multimedia system design it is our belief that the proposed method has great potential in reducing system resource while meeting performance requirements the experimental results confirm this as we achieve significant saving in system's energy consumption to provide statistical completion ratio guarantee ie the expected number of completions over large number of iterations is greater than given value
column statistics are an important element of cardinality estimation frameworks more accurate estimates allow the optimizer of rdbms to generate better plans and improve the overall system's efficiency this paper introduces filtered statistics which model value distribution over set of rows restricted by predicate this feature available in microsoft sql server can be used to handle column correlation as well as focus on interesting data ranges in particular it fits well for scenarios with logical subtables like flexible schema or multi tenant applications integration with the existing cardinality estimation infrastructure is presented
to keep up with the explosive internet packet processing demands modern network processors nps employ highly parallel multi threaded and multi core architecture in such parallel paradigm accesses to the shared variables in the external memory and the associated memory latency are contained in the critical sections so that they can be executed atomically and sequentially by different threads in the network processor in this paper we present novel program transformation that is used in the intel auto partitioning compiler for ixp to exploit the inherent finer grained parallelism of those critical sections using the software controlled caching mechanism available in the nps consequently those critical sections can be executed in pipelined fashion by different threads thereby effectively hiding the memory latency and improving the performance of network applications experimental results show that the proposed transformation provides impressive speedup up to and scalability up to threads of the performance for the real world network application lgbps efhernet core metro router
we show that if connected graph with nodes has conductance then rumour spreading also known as randomized broadcast successfully broadcasts message within log many rounds with high probability regardless of the source by using the push pull strategy the notation hides polylog factor this result is almost tight since there exists graph of nodes and conductance with diameter log if in addition the network satisfies some kind of uniformity condition on the degrees our analysis implies that both both push and pull by themselves successfully broadcast the message to every node in the same number of rounds
although database design tools have been developed that attempt to automate or semiautomate the design process these tools do not have the capability to capture common sense knowledge about business applications and store it in context specific manner as result they rely on the user to provide great deal of trivial details and do not function as well as human designer who usually has some general knowledge of how an application might work based on his or her common sense knowledge of the real world common sense knowledge could be used by database design system to validate and improve the quality of an existing design or even generate new designs this requires that context specific information about different database design applications be stored and generalized into information about specific application domains eg pharmacy daycare hospital university manufacturing such information should be stored at the appropriate level of generality in hierarchically structured knowledge base so that it can be inherited by the subdomains below for this to occur two types of learning must take place first knowledge about particular application domain that is acquired from specific applications within that domain are generalized into domain node eg entities relationships and attributes from various hospital applications are generalized to hospital node this is referred to as within domain learning second the information common to two or more related application domain nodes is generalized to higher level node for example knowledge from the car rental and video rental domains may be generalized to rental node this is called across domain learning this paper presents methodology for learning across different application domains based on distance measure the parameters used in this methodology were refined by testing on set of representative cases empirical testing provided further validation
in this paper we address the problem of clustering graphs in object oriented databases unlike previous studies which focused only on workload consisting of single operation this study tackles the problem when the workload is set of operations method and queries that occur with certain probability thus the goal is to minimize the expected cost of an operation in the workload while maintaining similarly low cost for each individual operation classto this end we present new clustering policy based on the nearest neighbor graph partitioning algorithm we then demonstrate that this policy provides considerable gains when compared to suite of well known clustering policies proposed in the literature our results are based on two widely referenced object oriented database benchmarks namely the tektronix hypermodel and oo
integrating several legacy software systems together is commonly performed with multiple applications of the adapter design pattern in oo languages such as java the integration is based on specifying bi directional translations between pairs of apis from different systems yet manual development of wrappers to implement these translations is tedious expensive and error prone in this paper we explore how models aspects and generative techniques can be used in conjunction to alleviate the implementation of multiple wrappers briefly the steps are the automatic reverse engineering of relevant concepts in apis to high level models the manual definition of mapping relationships between concepts in different models of apis using an ad hoc dsl the automatic generation of wrappers from these mapping specifications using aop this approach is weighted against manual development of wrappers using an industrial case study criteria are the relative code length and the increase of automation
higher order abstract syntax is simple technique for implementing languages with functional programming object variables and binders are implemented by variables and binders in the host language by using this technique one can avoid implementing common and tricky routines dealing with variables such as capture avoiding substitution however despite the advantages this technique provides it is not commonly used because it is difficult to write sound elimination forms such as folds or catamorphisms for higher order abstract syntax to fold over such data type one must either simultaneously define an inverse operation which may not exist or show that all functions embedded in the data type are parametric in this paper we show how first class polymorphism can be used to guarantee the parametricity of functions embedded in higher order abstract syntax with this restriction we implement library of iteration operators over data structures containing functionals from this implementation we derive fusion laws that functional programmers may use to reason about the iteration operator finally we show how this use of parametric polymorphism corresponds to the schürmann despeyroux and pfenning method of enforcing parametricity through modal types we do so by using this library to give sound and complete encoding of their calculus into system private char inline graphic mime subtype gif xlink schar private char this encoding can serve as starting point for reasoning about higher order structures in polymorphic languages
we introduce theoretical framework for discovering relationships between two database instances over distinct and unknown schemata this framework is grounded in the context of data exchange we formalize the problem of understanding the relationship between two instances as that of obtaining schema mapping so that minimum repair of this mapping provides perfect description of the target instance given the source instance we show that this definition yields intuitive results when applied on database instances derived from each other by basic operations we study the complexity of decision problems related to this optimality notion in the context of different logical languages and show that even in very restricted cases the problem is of high complexity
mining association rules is an important technique for discovering meaningful patterns in transaction databases many different measures of interestingness have been proposed for association rules however these measures fail to take the probabilistic properties of the mined data into account we start this paper with presenting simple probabilistic framework for transaction data which can be used to simulate transaction data when no associations are present we use such data and real world database from grocery outlet to explore the behavior of confidence and lift two popular interest measures used for rule mining the results show that confidence is systematically influenced by the frequency of the items in the left hand side of rules and that lift performs poorly to filter random noise in transaction data based on the probabilistic framework we develop two new interest measures hyper lift and hyper confidence which can be used to filter or order mined association rules the new measures show significantly better performance than lift for applications where spurious rules are problematic
in spite of impressive gains by pl fortran and cobol remain the languages in which most of the world's production programs are written and will remain so into the foreseeable future there is great deal of theoretical interest in algol and in extensible languages but so far at least they have had little practical impact problem oriented languages may very well become the most important language development area in the next five to ten years in the operating system area all major computer manufacturers set out to produce very ambitious multiprogramming systems and they all ran into similar problems number of university projects though not directly comparable to those of the manufacturers have contributed greatly to better understanding of operating system principles important trends include the increased interest in the development of system measurement and evaluation techniques and increased use of microprogramming for some programming system functions
this paper describes an ambient intelligent prototype known as socio ec socio ec explores the design and implementation of system for sensing and display user modeling and interaction models based on game structure the game structure includes word puzzles levels body states goals and game skills body states are body movements and positions that players must discover in order to complete level and in turn represent learned game skill the paper provides an overview of background concepts and related research we describe the prototype and game structure provide technical description of the prototype and discuss technical issues related to sensing reasoning and display the paper contributes by providing method for constructing group parameters from individual parameters with real time motion capture data and model for mapping the trajectory of participant's actions in order to determine an intensity level used to manage the experience flow of the game and its representation in audio and visual display we conclude with discussion of known and outstanding technical issues and future research
we describe forward rasterization class of rendering algorithms designed for small polygonal primitives the primitive is efficiently rasterized by interpolation between its vertices the interpolation factors are chosen to guarantee that each pixel covered by the primitive receives at least one sample which avoids holes the location of the samples is recorded with subpixel accuracy using pair of offsets which are then used to reconstruct resample the output image offset reconstruction has good static and temporal antialiasing properties we present two forward rasterization algorithms one that renders quadrilaterals and is suitable for scenes modeled with depth images like in image based rendering by warping and one that renders triangles and is suitable for scenes modeled conventionally when compared to conventional rasterization forward rasterization is more efficient for small primitives and has better temporal antialiasing properties
there is common misconception that the automobile industry is slow to adapt new technologies such as artificial intelligence ai and soft computing the reality is that many new technologies are deployed and brought to the public through the vehicles that they drive this paper provides an overview and sampling of many of the ways that the automotive industry has utilized ai soft computing and other intelligent system technologies in such diverse domains like manufacturing diagnostics on board systems warranty analysis and design
as memory hierarchy becomes deeper and shared by more processors locality increasingly determines system performance as rigorous and precise locality model reuse distance has been used in program optimizations performance prediction memory disambiguation and locality phase prediction however the high cost of measurement has been severely impeding its uses in scenarios requiring high efficiency such as product compilers performance debugging run time optimizationswe recently discovered the statistical connection between time and reuse distance which led to an efficient way to approximate reuse distance using time however not exposed are some algorithmic and implementation techniques that are vital for the efficiency and scalability of the approximation model this paper presents these techniques it describes an algorithm that approximates reuse distance on arbitrary scales it explains portable scheme that employs memory controller to accelerate the measure of time distance it uncovers the algorithm and proof of trace generator that can facilitate various locality studies
retrieving images from large image collection has been an active area of research most of the existing works have focused on content representation in this paper we address the issue of identifying relevant images quickly this is important in order to meet the users performance requirements we propose framework for fast image retrieval based on object shapes extracted from objects within images the framework builds hierarchy of approximations on object shapes such that shape representation at higher level is coarser representation of shape at the lower level in other words multiple shapes at lower level can be mapped into single shape at higher level in this way the hierarchy serves to partition the database at various granularities given query shape by searching only the relevant paths in the hierarchy large portion of the database can thus be pruned away we propose the angle mapping am method to transform shape from one level to another higher level am essentially replaces some edges of shape by smaller number of edges based on the angles between the edges thus reducing the complexity of the original shape based on the framework we also propose two hierarchical structures to facilitate speedy retrieval the first called hierarchical partitioning on shape representation hpsr uses the shape representation as the indexing key the second called hierarchical partitioning on angle vector hpav captures the angle information from the shape representation we conducted an extensive study on both methods to see their quality and efficiency our experiments on sets of images each of which has objects around from to showed that the framework can provide speedy image retrieval without sacrificing on the quality both proposed schemes can improve the efficiency by as much as hundreds of times to sequential scanning the improvement grows as image database size objects per image or object dimension increase
with the advent of extensive wireless networks that blanket physically compact urban enclaves such as office complexes shopping centers or university campuses it is possible to create software applications that provide location based mobile online services one such application is campuswiki which integrates location information into wiki structure in the design science research reported in this paper we employed form of action research in which we engaged users as participants in an iterative process of designing and evaluating campuswiki two qualitative studies were undertaken early in the design process in which semi structured interviews were used to assess potential users reactions to campuswiki through this research the designers were able to assess whether their intentions matched the mental models of potential users of the application the results showed that although many of the perceived benefits were as designed by the developers misunderstanding of the location aware feature led users to unanticipated concerns and expectations these findings are important in guiding designers and implementers on the desirable and possibly undesirable features of such systems
existing template independent web data extraction approaches adopt highly ineffective decoupled strategies attempting to do data record detection and attribute labeling in two separate phases in this paper we propose an integrated web data extraction paradigm with hierarchical models the proposed model is called dynamic hierarchical markov random fields dhmrfs dhmrfs take structural uncertainty into consideration and define joint distribution of both model structure and class labels the joint distribution is an exponential family distribution as conditional model dhmrfs relax the independence assumption as made in directed models since exact inference is intractable variational method is developed to learn the model's parameters and to find the map model structure and label assignments we apply dhmrfs to real world web data extraction task experimental results show that integrated web data extraction models can achieve significant improvements on both record detection and attribute labeling compared to decoupled models in diverse web data extraction dhmrfs can potentially address the blocky artifact issue which is suffered by fixed structured hierarchical models
as the total amount of traffic data in networks has been growing at an alarming rate there is currently substantial body of research that attempts to mine traffic data with the purpose of obtaining useful information for instance there are some investigations into the detection of internet worms and intrusions by discovering abnormal traffic patterns however since network traffic data contain information about the internet usage patterns of users network users privacy may be compromised during the mining process in this paper we propose an efficient and practical method that preserves privacy during sequential pattern mining on network traffic data in order to discover frequent sequential patterns without violating privacy our method uses the repository server model which operates as single mining server and the retention replacement technique which changes the answer to query probabilistically in addition our method accelerates the overall mining process by maintaining the meta tables in each site so as to determine quickly whether candidate patterns have ever occurred in the site or not extensive experiments with real world network traffic data revealed the correctness and the efficiency of the proposed method
this paper describes llvm low level virtual machine compiler framework designed to support transparent lifelongprogram analysis and transformation for arbitrary programs by providing high level information to compilertransformations at compile time link time run time and inidle time between runsllvm defines common low levelcode representation in static single assignment ssa form with several novel features simple language independenttype system that exposes the primitives commonly used toimplement high level language features an instruction fortyped address arithmetic and simple mechanism that canbe used to implement the exception handling features ofhigh level languages and setjmp longjmp in uniformlyand efficientlythe llvm compiler framework and coderepresentation together provide combination of key capabilitiesthat are important for practical lifelong analysis andtransformation of programsto our knowledge no existingcompilation approach provides all these capabilitieswe describethe design of the llvm representation and compilerframework and evaluate the design in three ways thesize and effectiveness of the representation including thetype information it provides compiler performance forseveral interprocedural problems and illustrative examplesof the benefits llvm provides for several challengingcompiler problems
caching frequently accessed data items on the client side is an effective technique for improving performance in mobile environment classical cache invalidation strategies are not suitable for mobile environments due to frequent disconnections and mobility of the clients one attractive cache invalidation technique is based on invalidation reports irs however the ir based cache invalidation solution has two major drawbacks which have not been addressed in previous research first there is long query latency associated with this solution since client cannot answer the query until the next ir interval second when the server updates hot data item all clients have to query the server and get the data from the server separately which wastes large amount of bandwidth in this paper we propose an ir based cache invalidation algorithm which can significantly reduce the query latency and efficiently utilize the broadcast bandwidth detailed analytical analysis and simulation experiments are carried out to evaluate the proposed methodology compared to previous ir based schemes our scheme can significantly improve the throughput and reduce the query latency the number of uplink request and the broadcast bandwidth requirements
physical database design is important for query performance in shared nothing parallel database system in which data is horizontally partitioned among multiple independent nodes we seek to automate the process of data partitioning given workload of sql statements we seek to determine automatically how to partition the base data across multiple nodes to achieve overall optimal or close to optimal performance for that workload previous attempts use heuristic rules to make those decisions these approaches fail to consider all of the interdependent aspects of query performance typically modeled by today's sophisticated query optimizerswe present comprehensive solution to the problem that has been tightly integrated with the optimizer of commercial shared nothing parallel database system our approach uses the query optimizer itself both to recommend candidate partitions for each table that will benefit each query in the workload and to evaluate various combinations of these candidates we compare rank based enumeration method with random based one our experimental results show that the former is more effective
considering the constraint brought by mobility and resources it is important for routing protocols to efficiently deliver data in intermittently connected mobile network icmn different from previous works that use the knowledge of previous encounters to predict the future contact we propose storagefriendly region based protocol namely rena in this paper instead of using temporal information rena builds routing tables based on regional movement history which avoids excessive storage for tracking encounter history we validate the generality of rena through time variant community mobility model with parameters extracted from the mit wlan trace and the vehicular network based on bus routes of the city of helsinki the comprehensive simulation results show that rena is not only storage friendly but also more efficient than the epidemic routing the restricted replication protocol snw and the encounter based protocol rapid under various conditions
we propose distributed on demand power management protocol for collecting data in sensor networks the protocol aims to reduce power consumption while supporting fluctuating demand in the network and provide local routing information and synchronicity without global control energy savings are achieved by powering down nodes during idle times identified through dynamic scheduling we present real implementation on wireless sensor nodes based on novel two level architecture we evaluate our approach through measurements and simulation and show how the protocol allows adaptive scheduling and enables smooth trade off between energy savings and latency an example current measurement shows an energy savings of on an intermediate node
we study new research problem where an implicit information retrieval query is inferred from eye movements measured when the user is reading and used to retrieve new documents in the training phase the user's interest is known and we learn mapping from how the user looks at term to the role of the term in the implicit query assuming the mapping is universal that is the same for all queries in given domain we can use it to construct queries even for new topics for which no learning data is available we constructed controlled experimental setting to show that when the system has no prior information as to what the user is searching the eye movements help significantly in the search this is the case in proactive search for instance where the system monitors the reading behaviour of the user in new topic in contrast during search or reading session where the set of inspected documents is biased towards being relevant stronger strategy is to search for content wise similar documents than to use the eye movements
in recent years network of workstations pcs so called now are becoming appealing vehicles for cost effective parallel computing due to the commodity nature of workstations and networking equipment lan environments are gradually becoming heterogeneous the diverse sources of heterogeneity in now systems pose challenge on the design of efficient communication algorithms for this class of systems in this paper we propose efficient algorithms for multiple multicast on heterogeneous now systems focusing on heterogeneity in processing speeds of workstations pcs multiple multicast is an important operation in many scientific and industrial applications multicast on heterogeneous systems has not been investigated until recently our work distinguishes itself from others in two aspects in contrast to the blocking communication model used in prior works we model communication in heterogeneous cluster more accurately by non blocking communication model and design multicast algorithms that can fully take advantage of non blocking communication while prior works focus on single multicast problem we propose efficient algorithms for general multiple multicast in which single multicast is special case on heterogeneous now systems to our knowledge our work is the earliest effort that addresses multiple multicast for heterogeneous now systems these algorithms are evaluated using network simulator for heterogeneous now systems our experimental results on system of up to nodes show that some of the algorithms outperform others in many cases the best algorithm achieves completion time that is within times of the lower bound
supporting quality of service qos in wireless networks has been very rich and interesting area of research many significant advances have been made in supporting qos in single wireless networks however the support for the qos across multiple heterogeneous wireless networks will be required in the future wireless networks in connections spanning multiple wireless networks the end to end qos will depend on several factors such as mobility and connection patterns of users and the qos policies in each of the wireless networks the end to end qos is also affected by multiple decisions that must be made by several different network entities for resource allocation the paper has two objectives one is to demonstrate the decision making process for resource allocation in multiple heterogeneous wireless networks and the second is to present novel concept of composite qos in such wireless environment more specifically we present an architecture for multiple heterogeneous wireless networks decision making process for resource request and allocation simulation model to study composite qos and several interesting results we also present potential implications of composite qos on users and network service providers we also show how the qos ideas presented in this paper can be used by wireless carriers for improved qos support and management the paper can form the basis for significant further research in dss for emerging wireless networks supporting qos for range of sophisticated and resource intensive mobile applications
this work addresses the problem of optimizing the deployment of sensors in order to ensure the quality of the readings of the value of interest in given critical geographic region as usual we assume that each sensor is capable of reading particular physical phenomenon eg concentration of toxic materials in the air and transmitting it to server or peer however the key assumptions considered in this work are each sensor is capable of moving where the motion may be remotely controlled and the spatial range for which the individual sensor's reading is guaranteed to be of desired quality is limited in scenarios like disaster management and homeland security in case some of the sensors dispersed in larger geographic area report value higher than certain threshold one may want to ensure quality of the readings for the affected region this in turn implies that one may want to ensure that there are enough sensors there and consequently guide subset of the rest of the sensors towards the affected region in this paper we explore variants of the problem of optimizing the guidance of the mobile sensors towards the affected geographic region and we present algorithms for their solutions
we describe in place reconfiguration ipr for lut based fpgas an algorithm that maximizes identical configuration bits for complementary inputs of lut thereby reducing the propagation of faults seen at pair of complementary inputs based on ipr we develop fault tolerant logic resynthesis algorithm which decreases the circuit fault rate while preserving functionality and topology of the lut based logic network since the topology is preserved the resynthesis algorithm can be applied post layout and without changes in physical design compared to the state of the art academic technology mapper berkeley abc ipr reduces the relative fault rate by and increases mttf by with the same area and performance and ipr combined with previous fault tolerant logic resynthesis algorithm rose reduces the relative fault rate by and increases mttf by with less area but same performance the above improvement assumes stochastic single fault and more improvement is expected for multi fault models
in this paper an interactive and realistic virtual head oriented to human computer interaction and social robotics is presented it has been designed following hybrid approach taking robotic characteristics into account and searching for convergence between these characteristics real facial actions and animation techniques an initial head model is first obtained from real person using laser scanner then the model is animated using hierarchical skeleton based procedure the proposed rig structure is close to real facial muscular anatomy and its behaviour follows the facial action coding system speech synthesis and visual human face tracking capabilities are also integrated for providing the head with further interaction ability using the said hybrid approach the head can be readily linked to social robot architecture the opinions of number of persons interacting with this social avatar have been evaluated and are reported in the paper as against their reactions when interacting with social robot with mechatronic face results show the suitability of the avatar for on screen real time interfacing in human computer interaction the proposed technique could also be helpful in the future for designing and parameterizing mechatronic human like heads for social robots
the emerging paradigm of electronic services promises to bring to distributed computation and services the flexibility that the web has brought to the sharing of documents an understanding of fundamental properties of service composition is required in order to take full advantage of the paradigm this paper examines proposals and standards for services from the perspectives of xml data management workflow and process models key areas for study are identified including behavioral service signatures verification and synthesis techniques for composite services analysis of service data manipulation commands and xml analysis applied to service specifications we give sample of the relevant results and techniques in each of these areas
generic database replication algorithms do not scale linearly in throughput as all update deletion and insertion udi queries must be applied to every database replica the throughput is therefore limited to the point where the number of udi queries alone is sufficient to overload one server in such scenarios partial replication of database can help as udi queries are executed only by subset of all servers in this paper we propose globetp system that employs partial replication to improve database throughput globetp exploits the fact that web application's query workload is composed of small set of read and write templates using knowledge of these templates and their respective execution costs globetp provides database table placements that produce significant improvements in database throughput we demonstrate the efficiency of this technique using two different industry standard benchmarks in our experiments globetp increases the throughput by to compared to full replication while using identical hardware configuration furthermore adding single query cache improves the throughput by another to
many artificial intelligence tasks such as automated question answering reasoning or heterogeneous database integration involve verification of semantic category eg coffee is drink red is color while steak is not drink and big is not color we present novel algorithm to automatically validate semantic category contrary to the methods suggested earlier our approach does not rely on any manually codified knowledge but instead capitalizes on the diversity of topics and word usage on the world wide web we have tested our approach within our online fact seeking question answering environment when tested on the trec questions that expect the answer to belong to specific semantic category our approach has improved the accuracy by up to depending on the model and metrics used
in the rank join problem we are given set of relations and scoring function and the goal is to return the join results with the top scores it is often the case in practice that the inputs may be accessed in ranked order and the scoring function is monotonic these conditions allow for efficient algorithms that solve the rank join problem without reading all of the input in this article we present thorough analysis of such rank join algorithms strong point of our analysis is that it is based on more general problem statement than previous work making it more relevant to the execution model that is employed by database systems one of our results indicates that the well known hrjn algorithm has shortcomings because it does not stop reading its input as soon as possible we find that it is np hard to overcome this weakness in the general case but cases of limited query complexity are tractable we prove the latter with an algorithm that infers provably tight bounds on the potential benefit of reading more input in order to stop as soon as possible as result the algorithm achieves cost that is within constant factor of optimal
in this paper we present method for organizing and indexing logo digital libraries like the ones of the patent and trademark offices we propose an efficient queried by example retrieval system which is able to retrieve logos by similarity from large databases of logo images logos are compactly described by variant of the shape context descriptor these descriptors are then indexed by locality sensitive hashing data structure aiming to perform approximate nn search in high dimensional spaces in sub linear time the experiments demonstrate the effectiveness and efficiency of this system on realistic datasets as the tobacco logo database
early applications of smart cards have focused in the area of personal security recently there has been an increasing demand for networked multi application cards in this new scenario enhanced application specific on card java applets and complex cryptographic services are executed through the smart card java virtual machine jvm in order to support such computation intensive applications contemporary smart cards are designed with built in microprocessors and memory as smart cards are highly area constrained environments with memory cpu and peripherals competing for very small die space the vm execution engine of choice is often small slow interpreter in addition support for multiple applications and cryptographic services demands high performance vm execution engine the above necessitates the optimization of the jvm for java cardsin this paper we present the concept of an annotation aware interpreter that optimizes the interpreted execution of java code using java bytecode superoperators sos sos are groups of bytecode operations that are executed as specialized vm instruction simultaneous translation of all the bytecode operations in an so reduces the bytecode dispatch cost and the number of stack accesses data transfer to from the java operand stack and stack pointer updates furthermore sos help improve native code quality without hindering class file portability annotation attributes in the class files mark the occurrences of valuable sos thereby dispensing the expensive task of searching and selecting sos at runtime besides our annotation based approach incurs minimal memory overhead as opposed to just in time jit compilerswe obtain an average speedup of using an interpreter customized with the top sos formed from operation folding patterns further we show that greater speedups could be achieved by statically adding to the interpreter application specific sos formed by top basic blocks the effectiveness of our approach is evidenced by performance improvements of upto obtained using sos formed from optimized basic blocks
engineering knowledge is specific kind of knowledge that is oriented to the production of particular classes of artifacts is typically related to disciplined design methods and takes place in tool intensive contexts as consequence representing engineering knowledge requires the elaboration of complex models that combine functional and structural representations of the resulting artifacts with process and methodological knowledge the different categories used in the engineering domain vary in their status and in the way they should be manipulated when building applications that support engineering processes these categories include artifacts activities methods and models this paper surveys existing models of engineering knowledge and discusses an upper ontology that abstracts the categories that crosscut different engineering domains such an upper model can be reused for particular engineering disciplines the process of creating such elaborations is reported on the particular case study of software engineering as concrete application example
recent works have shown the benefits of keyword proximity search in querying xml documents in addition to text documents for example given query keywords over shakespeare's plays in xml the user might be interested in knowing how the keywords cooccur in this paper we focus on xml trees and define xml keyword proximity queries to return the possibly heterogeneous set of minimum connecting trees mcts of the matches to the individual keywords in the query we consider efficiently executing keyword proximity queries on labeled trees xml in various settings when the xml database has been preprocessed and when no indices are available on the xml database we perform detailed experimental evaluation to study the benefits of our approach and show that our algorithms considerably outperform prior algorithms and other applicable approaches
many current research efforts address the problem of personalizing the web experience for each user with respect to user's identity and or context in this paper we propose new high level model for the specification of web applications that takes into account the manner in which users interact with the application for supplying appropriate contents or gathering profile data we therefore consider entire behaviors rather than single properties as the smallest information units allowing for automatic restructuring of application components for this purpose high level event condition action eca paradigm is proposed which enables capturing arbitrary and timed clicking behaviors also the architecture and components of first prototype implementation are discussed
one reason that researchers may wish to demonstrate that an external software quality attribute can be measured consistently is so that they can validate prediction system for the attribute however attempts at validating prediction systems for external subjective quality attributes have tended to rely on experts indicating that the values provided by the prediction systems informally agree with the experts intuition about the attribute these attempts are undertaken without pre defined scale on which it is known that the attribute can be measured consistently consequently valid unbiased estimate of the predictive capability of the prediction system cannot be given because the experts measurement process is not independent of the prediction system's values usually no justification is given for not checking to see if the experts can measure the attribute consistently it seems to be assumed that subjective measurement isn't proper measurement or subjective measurement cannot be quantified or no one knows the true values of the attributes anyway and they cannot be estimated however even though the classification of software systems or software artefacts quality attributes is subjective it is possible to quantify experts measurements in terms of conditional probabilities it is then possible using statistical approach to assess formally whether the experts measurements can be considered consistent if the measurements are consistent it is also possible to identify estimates of the true values which are independent of the prediction system these values can then be used to assess the predictive capability of the prediction system in this paper we use bayesian inference markov chain monte carlo simulation and missing data imputation to develop statistical tests for consistent measurement of subjective ordinal scale attributes
for robots operating in real world environments the ability to deal with dynamic entities such as humans animals vehicles or other robots is of fundamental importance the variability of dynamic objects however is large in general which makes it hard to manually design suitable models for their appearance and dynamics in this paper we present an unsupervised learning approach to this model building problem we describe an exemplar based model for representing the time varying appearance of objects in planar laser scans as well as clustering procedure that builds set of object classes from given observation sequences extensive experiments in real environments demonstrate that our system is able to autonomously learn useful models for eg pedestrians skaters or cyclists without being provided with external class information
we consider the problem of establishing route and sending packets between source destination pair in ad hoc networks composed of rational selfish nodes whose purpose is to maximize their own utility in order to motivate nodes to follow the protocol specification we use side payments that are made to the forwarding nodes our goal is to design fully distributed algorithm such that node is always better off participating in the protocol execution individual rationality ii node is always better off behaving according to the protocol specification truthfulness iii messages are routed along the most energy efficient least cost path and iv the message complexity is reasonably low we introduce the commit protocol for individually rational truthful and energy efficient routing in ad hoc networks to the best of our knowledge this is the first ad hoc routing protocol with these features commit is based on the vcg payment scheme in conjunction with novel game theoretic technique to achieve truthfulness for the sender node by means of simulation we show that the inevitable economic inefficiency is small as an aside our work demonstrates the advantage of using cross layer approach to solving problems leveraging the existence of an underlying topology control protocol we are able to simplify the design and analysis of our routing protocol and to reduce its message complexity on the other hand our investigation of the routing problem in presence of selfish nodes disclosed new metric under which topology control protocols can be evaluated the cost of cooperation
modern presentation software is still built around interaction metaphors adapted from traditional slide projectors we provide an analysis of the problems in this application genre that presentation authors face and present fly presentation tool that is based on the idea of planar information structures inspired by the natural human thought processes of data chunking association and spatial memory fly explores authoring of presentation documents evaluation of paper prototype showed that the planar ui is easily grasped by users and leads to presentations more closely resembling the information structure of the original content thus providing better authoring support than the slide metaphor our software prototype confirmed these results and outperformed powerpoint in second study for tasks such as prototyping presentations and generating meaningful overviews users reported that this interface helped them better to express their concepts and expressed significant preference for fly over the traditional slide model
we study generalization of the constraint satisfaction problem csp the periodic constraint satisfaction problem an input instance of the periodic csp is finite set of generating constraints over structured variable set that implicitly specifies larger possibly infinite set of constraints the problem is to decide whether or not the larger set of constraints has satisfying assignment this model is natural for studying constraint networks consisting of constraints obeying high degree of regularity or symmetry our main contribution is the identification of two broad polynomial time tractable subclasses of the periodic csp
suppose we are given graph and set of terminals we consider the problem of constructing graph eh that approximately preserves the congestion of every multicommodity flow with endpoints supported in we refer to such graph as flow sparsifier we prove that there exist flow sparsifiers that simultaneously preserve the congestion of all multicommodity flows within an log log log factor where this bound improves to if excludes any fixed minor this is strengthening of previous results which consider the problem of finding graph eh cut sparsifier that approximately preserves the value of minimum cuts separating any partition of the terminals indirectly our result also allows us to give construction for better quality cut sparsifiers and flow sparsifiers thereby we immediately improve all approximation ratios derived using vertex sparsification in we also prove an log log lower bound for how well flow sparsifier can simultaneously approximate the congestion of every multicommodity flow in the original graph the proof of this theorem relies on technique which we refer to as oblivious dual certifcates for proving super constant congestion lower bounds against many multicommodity flows at once our result implies that approximation algorithms for multicommodity flow type problems designed by black box reduction to uniform case on nodes see for examples must incur super constant cost in the approximation ratio
similarity search is important in information retrieval applications where objects are usually represented as vectors of high dimensionality this paper proposes new dimensionality reduction technique and an indexing mechanism for high dimensional datasets the proposed technique reduces the dimensions for which coordinates are less than critical value with respect to each data vector this flexible datawise dimensionality reduction contributes to improving indexing mechanisms for high dimensional datasets that are in skewed distributions in all coordinates to apply the proposed technique to information retrieval cva file compact va file which is revised version of the va file is developed by using cva file the size of index files is reduced further while the tightness of the index bounds is held maximally the effectiveness is confirmed by synthetic and real data
many of today's high level parallel languages support dynamic fine grained parallelism these languages allow the user to expose all the parallelism in the program which is typically of much higher degree than the number of processors hence an efficient scheduling algorithm is required to assign computations to processors at runtime besides having low overheads and good load balancing it is important for the scheduling algorithm to minimize the space usage of the parallel program this paper presents scheduling algorithm that is provably space efficient and time efficient for nested parallel languages in addition to proving the space and time bounds of the parallel schedule generated by the algorithm we demonstrate that it is efficient in practice we have implemented runtime system that uses our algorithm to schedule parallel threads the results of executing parallel programs on this system show that our scheduling algorithm significantly reduces memory usage compared to previous techniques without compromising performance
in this paper we study generalization of standard property testing where the algorithms are required to be more tolerant with respect to objects that do not have but are close to having the property specifically tolerant property testing algorithm is required to accept objects that are close to having given property and reject objects that are far from having for some parameters another related natural extension of standard property testing that we study is distance approximation here the algorithm should output an estimate of the distance of the object to where this estimate is sufficiently close to the true distance of the object to we first formalize the notions of tolerant property testing and distance approximation and discuss the relationship between the two tasks as well as their relationship to standard property testing we then apply these new notions to the study of two problems tolerant testing of clustering and distance approximation for monotonicity we present and analyze algorithms whose query complexity is either polylogarithmic or independent of the size of the input
despite their popularity and importance pointer based programs remain major challenge for program verification in this paper we propose an automated verification system that is concise precise and expressive for ensuring the safety of pointer based programs our approach uses user definable shape predicates to allow programmers to describe wide range of data structures with their associated size properties to support automatic verification we design new entailment checking procedure that can handle well founded inductive predicates using unfold fold reasoning we have proven the soundness and termination of our verification system and have built prototype system
this paper introduces simple real time distributed computing model for message passing systems which reconciles the distributed computing and the real time systems perspective by just replacing instantaneous computing steps with computing steps of non zero duration we obtain model that both facilitates real time scheduling analysis and retains compatibility with classic distributed computing analysis techniques and results we provide general simulations and validity conditions for transforming algorithms from the classic synchronous model to our real time model and vice versa and investigate whether which properties of real systems are inaccurately or even wrongly captured when resorting to zero step time models we revisit the well studied problem of deterministic drift and failure free internal clock synchronization for this purpose and show that no clock synchronization algorithm with constant running time can achieve optimal precision in our real time model since such an algorithm is known for the classic model this is an instance of problem where the standard distributed computing analysis gives too optimistic results we prove that optimal precision is only achievable with algorithms that take time in our model and establish several additional algorithms and lower bounds
advances in microsensor and radio technology will enable small but smart sensors to be deployed for wide range of environmental monitoring applications the low per node cost will allow these wireless networks of sensors and actuators to be densely distributed the nodes in these dense networks will coordinate to perform the distributed sensing and actuation tasks moreover as described in this paper the nodes can also coordinate to exploit the redundancy provided by high density so as to extend overall system lifetime the large number of nodes deployed in these systems will preclude manual configuration and the environmental dynamics will preclude design time preconfiguration therefore nodes will have to self configure to establish topology that provides communication under stringent energy constraints ascent builds on the notion that as density increases only subset of the nodes are necessary to establish routing forwarding backbone in ascent each node assesses its connectivity and adapts its participation in the multihop network topology based on the measured operating region this paper motivates and describes the ascent algorithm and presents analysis simulation and experimental measurements we show that the system achieves linear increase in energy savings as function of the density and the convergence time required in case of node failures while still providing adequate connectivity
an error occurs when software cannot complete requested action as result of some problem with its input configuration or environment high quality error report allows user to understand and correct the problem unfortunately the quality of error reports has been decreasing as software becomes more complex and layered end users take the cryptic error messages given to them by programsand struggle to fix their problems using search engines and support websites developers cannot improve their error messages when they receive an ambiguous or otherwise insufficient error indicator from black box software component we introduce clarify system that improves error reporting by classifying application behavior clarify uses minimally invasive monitoring to generate behavior profile which is summary of the program's execution history machine learning classifier uses the behavior profile to classify the application's behavior thereby enabling more precise error report than the output of the application itself we evaluate prototype clarify system on ambiguous error messages generated by large modern applications like gcc la tex and the linux kernel for performance cost of less than on user applications and on the linux kernel the proto type correctly disambiguates at least of application behaviors that result in ambiguous error reports this accuracy does not degrade significantly with more behaviors clarify classifier for la tex error messages is at most less accurate than classifier for latex error messages finally we show that without any human effort to build classifier clarify can provide nearest neighbor software support where users who experience problem are told about other users who might have had the same problem on average of the users that clarify identifies have experienced the same problem
distributional measures of lexical similarity and kernel methods for classification are well known tools in natural language processing we bring these two methods together by introducing distributional kernels that compare co occurrence probability distributions we demonstrate the effectiveness of these kernels by presenting state of the art results on datasets for three semantic classification compound noun interpretation identification of semantic relations between nominals and semantic classification of verbs finally we consider explanations for the impressive performance of distributional kernels and sketch some promising generalisations
we survey recent results on wireless networks that are based on analogies with various branches of physics we address among others the problems of optimally arranging the flow of traffic in wireless sensor networks finding minimum cost routes performing load balancing optimizing and analyzing cooperative transmissions calculating the capacity finding routes that avoid bottlenecks and developing distributed anycasting protocols the results are based on establishing analogies between wireless networks and settings from various branches of physics such as electrostatics optics percolation theory diffusion and others many of the results we present hinge on the assumption that the network is massive ie it consists of so many nodes that it can be described in terms of novel macroscopic view the macroscopic view is not as detailed as the standard microscopic one but nevertheless contains enough details to permit meaningful optimization
when meeting someone new the first impression is often influenced by someone's physical appearance and other types of prejudice in this paper we present touchmedare an interactive canvas which aims to provide an experience when meeting new people while preventing visual prejudice and lowering potential thresholds the focus of the designed experience was to stimulate people to get acquainted through the interactive canvas touchmedare consists of flexible opaque canvas which plays music when touched simultaneously from both sides dynamic variation of this bodily contact is reflected through real time adaptations of the musical compositions two redesigns were qualitatively and quantitatively evaluated and final version was placed in the lowlands festival as case study evaluation results showed that some explanation was needed for the initial interaction with the installation on the other hand after this initial unfamiliarity passed results showed that making bodily contact through the installation did help people to get acquainted with each other and increased their social interaction
the standard formalism for explaining abstract types is existential quantification while it provides sufficient model for type abstraction in entirely statically typed languages it proves to be too weak for languages enriched with forms of dynamic typing where parametricity is violated as an alternative approach to type abstraction that addresses this shortcoming we present calculus for dynamic type generation it features an explicit construct for generating new type names and relies on coercions for managing abstraction boundaries between generated types and their designated representation sealing is represented as generalized form of these coercions the calculus maintains abstractions dynamically without restricting type analysis
many applications require randomized ordering of input data examples include algorithms for online aggregation data mining and various randomized algorithms most existing work seems to assume that accessing the records from large database in randomized order is not difficult problem however it turns out to be extremely difficult in practice using existing methods randomization is either extremely expensive at the front end as data are loaded or at the back end as data are queried this paper presents simple file structure which supports both efficient online random shuffling of large database as well as efficient online sampling or randomization of the database when it is queried the key innovation of our method is the introduction of small degree of carefully controlled rigorously monitored nonrandomness into the file
moments before the launch of every space vehicle engineering discipline specialists must make critical go no go decision the cost of false positive allowing launch in spite of fault or false negative stopping potentially successful launch can be measured in the tens of millions of dollars not including the cost in morale and other more intangible detriments the aerospace corporation is responsible for providing engineering assessments critical to the go no go decision for every department of defense space vehicle these assessments are made by constantly monitoring streaming telemetry data in the hours before launch we will introduce viztree novel time series visualization tool to aid the aerospace analysts who must make these engineering assessments viztree was developed at the university of california riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry the use of single tool for both aspects of the task allows natural and intuitive transfer of mined knowledge to the monitoring task our visualization approach works by transforming the time series into symbolic representation and encoding the data in modified suffix tree in which the frequency and other properties of patterns are mapped onto colors and other visual properties we demonstrate the utility of our system by comparing it with state of the art batch algorithms on several real and synthetic datasets
numerous context aware mobile communication systems have emerged for individuals and groups calling for the identification of critical success factors related to the design of such systems at different levels the effective system design cannot be achieved without the understanding of situated user behaviour in using context aware systems drawing on activity theory this article advances cross level but coherent conceptualisations of context awareness as enabled by emerging systems grounded in the activities of using context aware systems these conceptualisations provide implications for system design at individual and group levels in terms of critical success factors including contextualisation interactivity and personalisation
many materials including water plastic and metal have specular surface characteristics specular reflections have commonly been considered nuisance for the recovery of object shape however the way that reflections are distorted across the surface depends crucially on curvature suggesting that they could in fact be useful source of information indeed observers can have vivid impression of shape when an object is perfectly mirrored ie the image contains nothing but specular reflections this leads to the question what are the underlying mechanisms of our visual system to extract this shape information from perfectly mirrored object in this paper we propose biologically motivated recurrent model for the extraction of visual features relevant for the perception of shape information from images of mirrored objects we qualitatively and quantitatively analyze the results of computational model simulations and show that bidirectional recurrent information processing leads to better results than pure feedforward processing furthermore we utilize the model output to create rough nonphotorealistic sketch representation of mirrored object which emphasizes image features that are mandatory for shape perception eg occluding contour and regions of high curvature moreover this sketch illustrates that the model generates representation of object features independent of the surrounding scene reflected in the mirrored object
we address the problem of answering conjunctive queries over extended entity relationship schemata which we call eer extended er schemata with is among entities and relationships and cardinality constraints this is common setting in conceptual data modelling where reasoning over incomplete data with respect to knowledge base is required we adopt semantics for eer schemata based on their relational representation we identify wide class of eer schemata for which query answering is tractable in data complexity the crucial condition for tractability is the separability between maximum cardinality constraints represented as key constraints in relational form and the other constraints we provide by means of graph based representation syntactic condition for separability we show that our conditions is not only sufficient but also necessary thus precisely identifying the class of separable schemata we present an algorithm based on query rewriting that is capable of dealing with such eer schemata while achieving tractability we show that further negative constraints can be added to the eer formalism while still keeping query answering tractable we show that our formalism is general enough to properly generalise the most widely adopted knowledge representation languages
we present set of algorithms and an associated display system capable of producing correctly rendered eye contact between three dimensionally transmitted remote participant and group of observers in teleconferencing system the participant's face is scanned in at hz and transmitted in real time to an autostereoscopic horizontal parallax display displaying him or her over more than deg field of view observable to multiple observers to render the geometry with correct perspective we create fast vertex shader based on lookup table for projecting scene vertices to range of subject angles heights and distances we generalize the projection mathematics to arbitrarily shaped display surfaces which allows us to employ curved concave display surface to focus the high speed imagery to individual observers to achieve two way eye contact we capture video from cross polarized camera reflected to the position of the virtual participant's eyes and display this video feed on large screen in front of the real participant replicating the viewpoint of their virtual self to achieve correct vertical perspective we further leverage this image to track the position of each audience member's eyes allowing the display to render correct vertical perspective for each of the viewers around the device the result is one to many teleconferencing system able to reproduce the effects of gaze attention and eye contact generally missing in traditional teleconferencing systems
common deficiency of discretized datasets is that detail beyond the resolution of the dataset has been irrecoverably lost this lack of detail becomes immediately apparent once one attempts to zoom into the dataset and only recovers blur here we describe method that generates the missing detail from any available and plausible high resolution data using texture synthesis since the detail generation process is guided by the underlying image or volume data and is designed to fill in plausible detail in accordance with the coarse structure and properties of the zoomed in neighborhood we refer to our method as constrained texture synthesis regular zooms become semantic zooms where each level of detail stems from data source attuned to that resolution we demonstrate our approach by medical application the visualization of human liver but its principles readily apply to any scenario as long as data at all resolutions are available we will first present viewing application called the virtual microscope and then extend our technique to volumetric viewing
we present new technique that employs support vector machines svms and gaussian mixture densities gmds to create generative discriminative object classification technique using local image features in the past several approaches to fuse the advantages of generative and discriminative approaches were presented often leading to improved robustness and recognition accuracy support vector machines are well known discriminative classification framework but similar to other discriminative approaches suffer from lack of robustness with respect to noise and overfitting gaussian mixtures on the contrary are widely used generative technique we present method to directly fuse both approaches effectively allowing to fully exploit the advantages of both the fusion of svms and gmds is done by representing svms in the framework of gmds without changing the training and without changing the decision boundary the new classifier is evaluated on the pascal voc data additionally we perform experiments on the usps dataset and on four tasks from the uci machine learning repository to obtain additional insights into the properties of the proposed approach it is shown that for the relatively rare cases where svms have problems the combined method outperforms both individual ones
in this paper processor scheduling policies that save processors are introduced and studied in multiprogrammed parallel system processor saving scheduling policy purposefully keeps some of the available processors idle in the presence of work to be done the conditions under which processor saving policies can be more effective than their greedy counterparts ie policies that never leave processors idle in the presence of work to be done are examined sensitivity analysis is performed with respect to application speedup system size coefficient of variation of the applications execution time variability in the arrival process and multiclass workloads analytical simulation and experimental results show that processor saving policies outperform their greedy counterparts under variety of system and workload characteristics
principles of the unitesk test development technology based on the use of formal models of target software are presented this technology was developed by the redverst group in the institute for system programming russian academy of sciences ispras lsqb rsqb which obtained rich experience in testing and verification of complex commercial software
recovering from malicious attacks in survival database systems is vital in mission critical information systems traditional rollback and re execute techniques are too time consuming and can not be applied in survival environments in this paper two efficient approaches transaction dependency based and data dependency based are proposed comparing to transaction dependency based approach data dependency recovery approaches need not undo innocent operations in malicious and affected transactions even some benign blind writes on bad data item speed up recovery process
information system engineering has become under increasing pressure to come up with software solutions that endow systems with the agility that is required to evolve in continually changing business and technological environment in this paper we suggest that software engineering has contribution to make in terms of concepts and techniques that have been recently developed for parallel program design and software architectures we show how such mechanisms can be encapsulated in new modelling primitive coordination contract that can be used for extending component based development approaches in order to manage such levels of change
this article provides detailed implementation study on the behavior of web serves that serve static requests where the load fluctuates over time transient overload various external factors are considered including wan delays and losses and different client behavior models we find that performance can be dramatically improved via kernel level modification to the web server to change the scheduling policy at the server from the standard fair processor sharing scheduling to srpt shortest remaining processing time scheduling we find that srpt scheduling induces no penalties in particular throughput is not sacrificed and requests for long files experience only negligibly higher response times under srpt than they did under the original fair scheduling
recent work demonstrates the potential for extracting patterns from users behavior as detected by sensors since there is currently no generalized framework for reasoning about activity aware applications designers can only rely on the existing systems for guidance however these systems often use custom domain specific definition of activity pattern consequently the guidelines designers can extract from individual systems are limited to the specific application domains of those applications in this paper we introduce five high level guidelines or commandments for designing activity aware applications by considering the issues we outlined in this paper designers will be able to avoid common mistakes inherent in designing activity aware applications
cast shadows are an informative cue to the shape of objects they are particularly valuable for discovering object's concavities which are not available from other cues such as occluding boundaries we propose new method for recovering shape from shadows which we call shadow carving given conservative estimate of the volume occupied by an object it is possible to identify and carve away regions of this volume that are inconsistent with the observed pattern of shadows we prove theorem that guarantees that when these regions are carved away from the shape the shape still remains conservative shadow carving overcomes limitations of previous studies on shape from shadows because it is robust with respect to errors in shadows detection and it allows the reconstruction of objects in the round rather than just bas reliefs we propose reconstruction system to recover shape from silhouettes and shadow carving the silhouettes are used to reconstruct the initial conservative estimate of the object's shape and shadow carving is used to carve out the concavities we have simulated our reconstruction system with commercial rendering package to explore the design parameters and assess the accuracy of the reconstruction we have also implemented our reconstruction scheme in table top system and present the results of scanning of several objects
information from which knowledge can be discovered is frequently distributed due to having been recorded at different times or to having arisen from different sources such information is often subject to both imprecision and uncertainty the dempster shafer representation of evidence offers way of representing uncertainty in the presence of imprecision and may therefore be used to provide mechanism for storing imprecise and uncertain information in databases we consider an extended relational data model that allows the imprecision and uncertainty associated with attribute values to be quantified using mass function distribution when query is executed it may be necessary to combine imprecise and uncertain data from distributed sources in order to answer that query mechanism is therefore required both for combining the data and for generating measures of uncertainty to be attached to the imprecise combined data in this paper we provide such mechanism based on aggregation of evidence we show first how this mechanism can be used to resolve inconsistencies and hence provide an essential database capability to perform the operations necessary to respond to queries on imprecise and uncertain data we go on to exploit the aggregation operator in an attribute driven approach to provide information on properties of and patterns in the data this is fundamental to rule discovery and hence such an aggregation operator provides facility that is central requirement in providing distributed information system with the capability to perform the operations necessary for knowledge discovery
one view of computational learning theory is that of learner acquiring the knowledge of teacher we introduce formal model of learning capturing the idea that teachers may have gaps in their knowledge the goal of the learner is still to acquire the knowledge of the teacher but now the learner must also identify the gaps this is the notion of learning from consistently ignorant teacher we consider the impact of knowledge gaps on learning for example monotone dnf and dimensional boxes and show that learning is still possible negatively we show that knowledge gaps make learning conjunctions of horn clauses as hard as learning dnf we also present general results describing when known learning algorithms can be used to obtain learning algorithms using consistently ignorant teacher
in this paper we study the challenges and evaluate the effectiveness of data collected from the web for recommendations we provide experimental results including user study showing that our methods produce good recommendations in realistic applications we propose new evaluation metric that takes into account the difficulty of prediction we show that the new metric aligns well with the results from user study
this paper proposes new clothing segmentation method using foreground clothing and background non clothing estimation based on the constrained delaunay triangulation cdt without any pre defined clothing model in our method the clothing is extracted by graph cuts where the foreground seeds and background seeds are determined automatically the foreground seeds are found by torso detection based on dominant colors determination and the background seeds are estimated based on cdt with the determined seeds the color distributions of the foreground and background are modeled by gaussian mixture models and filtered by cdt based noise suppression algorithm for more robust and accurate segmentation experimental results show that our clothing segmentation method is able to extract different clothing from static images with variations in backgrounds and lighting conditions
in this paper we address the issue of feeding future superscalar processor cores with enough instructions hardware techniques targeting an increase in the instruction fetch bandwidth have been proposed such as the trace cache microarchitecture we present microarchitecture solution based on register file holding basic blocks of instructions this solution places the instruction memory hierarchy out of the cycle determining path we call our approach instruction register file irf we estimate our approach with simplescalar based simulator run on the mediabench benchmark suite and compare to the trace cache performance on the same benchmarks we show that on this benchmark suite an irf based processor fetching up to three basic blocks per cycle outperforms trace cache based processor fetching instructions long traces by on the average
the authors propose method for personalizing the flexible widget layout fwl by adjusting the desirability of widgets with pairwise comparison method and show its implementation and that it actually works personalization of graphical user interfaces guis is important from perspective of usability and it is challenge in the field of model based user interface designs the fwl is model and optimization based layout framework of guis offering possibility for personalization but it has not actually realized it with any concrete method yet in this paper the authors implement method for personalization as dialog box and incorporate it into the existing system of the fwl thus users can personalize layouts generated by the fwl system at run time
within the context of the relational model general technique for establishing that the translation of view update defined by constant complement is independent of the choice of complement is presented in contrast to previous results the uniqueness is not limited to order based updates those constructed from insertions and deletions nor is it limited to those well behaved complements which define closed update strategies rather the approach is based upon optimizing the change of information in the main schema which the view update entails the only requirement is that the view and its complement together possess property called semantic bijectivity relative to the information measure it is furthermore established that very wide range of views have this property this results formalizes the intuition long observed in examples that it is difficult to find different complements which define distinct but reasonable update strategies
in wireless networks bandwidth is relatively scarce especially for supporting on demand media streaming in wired networks multicast stream merging is well known technique for scalable on demand streaming also caching proxies are widely used on the internet to offload servers and reduce network traffic this paper uses simulation to examine caching hierarchy for wireless streaming video distribution in combination with multicast stream merging the main purpose is to gain insight into the filtering effects caused by caching and merging using request frequencies entropy and inter reference times as metrics we illustrate how merging caching and traffic aggregation affect the traffic characteristics at each level the simulation results provide useful insights into caching performance in video streaming hierarchy
the power of high level languages lies in their abstraction over hardware and software complexity leading to greater security better reliability and lower development costs however opaque abstractions are often show stoppers for systems programmers forcing them to either break the abstraction or more often simply give up and use different language this paper addresses the challenge of opening up high level language to allow practical low level programming without forsaking integrity or performance the contribution of this paper is three fold we draw together common threads in diverse literature we identify framework for extending high level languages for low level programming and we show the power of this approach through concrete case studies our framework leverages just three core ideas extending semantics via intrinsic methods extending types via unboxing and architectural width primitives and controlling semantics via scoped semantic regimes we develop these ideas through the context of rich literature and substantial practical experience we show that they provide the power necessary to implement substantial artifacts such as high performance virtual machine while preserving the software engineering benefits of the host language the time has come for high level low level programming to be taken more seriously more projects now use high level languages for systems programming increasing architectural heterogeneity and parallelism heighten the need for abstraction and new generation of high level languages are under development and ripe to be influenced
collaboration and information sharing between organizations that share common goal is becoming increasingly important effective sharing promotes efficiency and productivity as well as enhances customer service with internet connectivity widely available sharing and access to information is relatively simple to implement however the abundance causes another problem the difficulty of determining where truly useful and relevant information is housed information resources such as data documents multimedia objects and services stored in different agencies need to be easily discovered and shared we propose collaborative semantic and pragmatic annotation environment where resources of each agency can be annotated by users in the government social network this collaborative annotation captures not only the semantics but also the pragmatics of the resources such as who when where how and why the resources are used the benefits of semantic and pragmatic annotation tags will include an ability to filter discover and search new and dynamic as well as hidden resources to navigate between resources in search by traversing semantic relationships and to recommend the most relevant government information distributed over different agencies distributed architecture of tagging system is shown and tag based search is illustrated
number of supervised learning methods have been introduced in the last decade unfortunately the last comprehensive empirical evaluation of supervised learning was the statlog project in the early we present large scale empirical comparison between ten supervised learning methods svms neural nets logistic regression naive bayes memory based learning random forests decision trees bagged trees boosted trees and boosted stumps we also examine the effect that calibrating the models via platt scaling and isotonic regression has on their performance an important aspect of our study is the use of variety of performance criteria to evaluate the learning methods
model driven development using languages such as uml and bon often makes use of multiple diagrams eg class and sequence diagrams when modeling systems these diagrams presenting different views of system of interest may be inconsistent metamodel provides unifying framework in which to ensure and check consistency while at the same time providing the means to distinguish between valid and invalid models that is conformance two formal specifications of the metamodel for an object oriented modeling language are presented and it is shown how to use these specifications for model conformance and multiview consistency checking comparisons are made in terms of completeness and the level of automation each provide for checking multiview consistency and model conformance the lessons learned from applying formal techniques to the problems of metamodeling model conformance and multiview consistency checking are summarized
disk caching algorithm is presented that uses an adaptive prefetching scheme to optimize the system performance in disk controllers for traces with different data localities the algorithm uses on line measurements of disk transfer times and of inter page fault rates to adjust the level of prefetching dynamically and its performance is evaluated through trace driven simulations using real workloads the results confirm the effectiveness and efficiency of the new adaptive prefetching algorithm
grids offer dramatic increase in the number of available processing and storing resources that can be delivered to applications however efficient job submission and management continue being far from accessible to ordinary scientists and engineers due to their dynamic and complex nature this paper describes new globus based framework that allows an easier and more efficient execution of jobs in submit and forget fashion the framework automatically performs the steps involved in job submission and also watches over its efficient execution in order to obtain reasonable degree of performance job execution is adapted to dynamic resource conditions and application demands adaptation is achieved by supporting automatic application migration following performance degradation better resource discovery requirement change owner decision or remote resource failure the framework is currently functional on any grid testbed based on globus because it does not require new system software to be installed in the resources the paper also includes practical experiences of the behavior of our framework on the trgp and ucm cab testbeds
debugging refers to the laborious process of finding causes of program failures often such failures are introduced when program undergoes changes and evolves from stable version to new modified version in this paper we propose an automated approach for debugging evolving programs given two programs reference stable program and new modified program and an input that fails on the modified program our approach uses concrete as well as symbolic execution to synthesize new inputs that differ marginally from the failing input in their control flow behavior comparison of the execution traces of the failing input and the new inputs provides critical clues to the root cause of the failure notable feature of our approach is that it handles hard to explain bugs like code missing errors by pointing to the relevant code in the reference program we have implemented our approach in tool called darwin we have conducted experiments with several real life case studies including real world web servers and the libpng library for manipulating png images our experience from these experiments points to the efficacy of darwin in pinpointing bugs moreover while localizing given observable error the new inputs synthesized by darwin can reveal other undiscovered errors
os kernels have been written in weakly typed or non typed programming languages for example therefore it is extremely hard to verify even simple memory safety of the kernels the difficulty could be resolved by writing os kernels in strictly typed programming languages but existing strictly typed languages are not flexible enough to implement important os facilities eg memory management and multi thread management facilities to address the problem we designed and implemented talk new strictly and statically typed assembly language which is flexible enough to implement os facilities and wrote an os kernel with talk in our approach the safety of the kernel can be verified automatically through static type checking at the level of binary executables without source code
existing research has most often relied on simulation and considered the uniform traffic distribution when investigating the performance properties of multicomputer networks eg the torus however there are numerous parallel applications that generate non uniform traffic patterns such as hot spot furthermore much more attention has been paid to capturing the impact of non uniform traffic on network performance resulting in the development of number of analytical models for predicting message latency in the presence of hot spots in the network for instance analytical models have been reported for the adaptively routed torus with uni directional as well as bi directional channels however models for the deterministically routed torus have considered uni directional channels only in an effort to fill in this gap this paper describes an analytical model for the deterministically routed torus with bi directional channels when subjected to hot spot traffic the modelling approach adopted for deterministic routing is totally different from that for adaptive routing due to the inherently different nature of the two types of routing the validity of the model is demonstrated by comparing analytical results against those obtained through extensive simulation experiments
the task of obtaining an optimal set of parameters to fit mixture model has many applications in science and engineering domains and is computationally challenging problem novel algorithm using convolution based smoothing approach to construct hierarchy or family of smoothed log likelihood surfaces is proposed this approach smooths the likelihood function and applies the em algorithm to obtain promising solution on the smoothed surface using the most promising solutions as initial guesses the em algorithm is applied again on the original likelihood though the results are demonstrated using only two levels the method can potentially be applied to any number of levels in the hierarchy theoretical insight demonstrates that the smoothing approach indeed reduces the overall gradient of modified version of the likelihood surface this optimization procedure effectively eliminates extensive searching in non promising regions of the parameter space results on some benchmark datasets demonstrate significant improvements of the proposed algorithm compared to other approaches empirical results on the reduction in the number of local maxima and improvements in the initialization procedures are provided
an object detection method from line drawings is presented the method adopts the local neighborhood structure as the elementary descriptor which is formed by grouping several nearest neighbor lines curves around one reference with this representation both the appearance and the geometric structure of the line drawing are well described the detection algorithm is hypothesis test scheme the top most similar local structures in the drawing are firstly obtained for each local structure of the model and the transformation parameters are estimated for each of the candidates such as object center scale and rotation factors by treating each estimation result as point in the parameter space dense region around the ground truth is then formed provided that there exist model in the drawing the mean shift method is used to detect the dense regions and the significant modes are accepted as the occurrence of object instances
we describe our experiences of designing digital community display with members of rural community these experiences are highlighted by the development of printed and digital postcard features for the wray photo display public photosharing display designed with the community which was trialled during popular village fair where both local residents and visitors interacted with the system this trial allowed us to examine the relative popularity and differences in usage between printed and digital postcard and offer insights into the uses of these features with community generated content and potential problems encountered
the interconnection network considered in this paper is the generalized base hypercube that is an attractive variance of the well known hypercube the generalized base hypercube is superior to the hypercube in many criteria such as diameter connectivity and fault diameter in this paper we study the hamiltonian connectivity and pancyclicity of the generalized base hypercube by the algorithmic approach we show that generalized base hypercube is hamiltonian connected for that is there exists hamiltonian path joining each pair of vertices in generalized base hypercube for we also show that generalized base hypercube is pancyclic for that is it embeds cycles of all lengths ranging from to the order of the graph for
in this paper we approach the problem of constructing ensembles of classifiers from the point of view of instance selection instance selection is aimed at obtaining subset of the instances available for training capable of achieving at least the same performance as the whole training set in this way instance selection algorithms try to keep the performance of the classifiers while reducing the number of instances in the training set meanwhile boosting methods construct an ensemble of classifiers iteratively focusing each new member on the most difficult instances by means of biased distribution of the training instances in this work we show how these two methodologies can be combined advantageously we can use instance selection algorithms for boosting using as objective to optimize the training error weighted by the biased distribution of the instances given by the boosting method our method can be considered as boosting by instance selection instance selection has mostly been developed and used for nearest neighbor nn classifiers so as first step our methodology is suited to construct ensembles of nn classifiers constructing ensembles of classifiers by means of instance selection has the important feature of reducing the space complexity of the final ensemble as only subset of the instances is selected for each classifier however the methodology is not restricted to nn classifier other classifiers such as decision trees and support vector machines svms may also benefit from smaller training set as they produce simpler classifiers if an instance selection algorithm is performed before training in the experimental section we show that the proposed approach is able to produce better and simpler ensembles than random subspace method rsm method for nn and standard ensemble methods for and svms
we present an expectation maximization learning algorithm em for estimating the parameters of partially constrained bayesian trees the bayesian trees considered here consist of an unconstrained subtree and set of constrained subtrees in this tree structure constraints are imposed on some of the parameters of the parametrized conditional distributions such that all conditional distributions within the same subtree share the same constraint we propose learning method that uses the unconstrained subtree to guide the process of discovering set of relevant constrained substructures substructure discovery and constraint enforcement are simultaneously accomplished using an em algorithm we show how our tree substructure discovery method can be applied to the problem of learning representative pose models from set of unsegmented video sequences our experiments demonstrate the potential of the proposed method for human motion classification
in multimedia retrieval query is typically interactively refined towards the optimal answers by exploiting user feedback however in existing work in each iteration the refined query is re evaluated this is not only inefficient but fails to exploit the answers that may be common between iterations furthermore it may also take too many iterations to get the optimal answers in this paper we introduce new approach called optrfs optimizing relevance feedback search by query prediction for iterative relevance feedback search optrfs aims to take users to view the optimal results as fast as possible it optimizes relevance feedback search by both shortening the searching time during each iteration and reducing the number of iterations optrfs predicts the potential candidates for the next iteration and maintains this small set for efficient sequential scan by doing so repeated candidate accesses ie random accesses can be saved hence reducing the searching time for the next iteration in addition efficient scan on the overlap before the next search starts also tightens the search space with smaller pruning radius as step forward optrfs also predicts the optimal query which corresponds to optimal answers based on the early executed iterations queries by doing so some intermediate iterations can be saved hence reducing the total number of iterations by taking the correlations among the early executed iterations into consideration optrfs investigates linear regression exponential smoothing and linear exponential smoothing to predict the next refined query so as to decide the overlap of candidates between two consecutive iterations considering the special features of relevance feedback optrfs further introduces adaptive linear exponential smoothing to self adjust the parameters for more accurate prediction we implemented optrfs and our experimental study on real life data sets show that it can reduce the total cost of relevance feedback search significantly some interesting features of relevance feedback search are also discovered and discussed
automated software engineering methods support the construction maintenance and analysis of both new and legacy systems their application is commonplace in desktop and enterprise class systems due to the productivity and reliability benefits they afford the contribution of this article is to present an applied foundation for extending the use of such methods to the flourishing domain of wireless sensor networks the objective is to enable developers to construct tools that aid in understanding both the static and dynamic properties of reactive event based systems we present static analysis and instrumentation toolkit for the nesc language the defacto standard for sensor network development we highlight the novel aspects of the toolkit analyze its performance and provide representative case studies that illustrate its use
real scale semantic web applications such as knowledge portals and marketplaces require the management of large volumes of metadata ie information describing the available web content and services better knowledge about their meaning usage accessibility or quality will considerably facilitate an automated processing of web resources the resource description framework rdf enables the creation and exchange of metadata as normal web data although voluminous rdf descriptions are already appearing sufficiently expressive declarative languages for querying both rdf descriptions and schemas are still missing in this paper we propose new rdf query language called rql it is typed functional language la oql and relies on formal model for directed labeled graphs permitting the interpretation of superimposed resource descriptions by means of one or more rdf schemas rql adapts the functionality of semistructured xml query languages to the peculiarities of rdf but foremost it enables to uniformly query both resource descriptions and schemas we illustrate the rql syntax semantics and typing system by means of set of example queries and report on the performance of our persistent rdf store employed by the rql interpreter
the web is now being used as general platform for hosting distributed applications like wikis bulletin board messaging systems and collaborative editing environments data from multiple applications originating at multiple sources all intermix in single web browser making sensitive data stored in the browser subject to broad milieu of attacks cross site scripting cross site request forgery and others the fundamental problem is that existing web infrastructure provides no means for enforcing end to end security on data to solve this we design an architecture using mandatory access control mac enforcement we overcome the limitations of traditional mac systems implemented solely at the operating system layer by unifying mac enforcement across virtual machine operating system networking and application layers we implement our architecture using xen virtual machine management selinux at the operating system layer labeled ipsec for networking and our own label enforcing web browser called flowwolf we tested our implementation and find that it performs well supporting data intermixing while still providing end to end security guarantees
paper augmented digital documents padds are digital documents that can be manipulated either on computer screen or on paper padds and the infrastructure supporting them can be seen as bridge between the digital and the paper worlds as digital documents padds are easy to edit distribute and archive as paper documents padds are easy to navigate annotate and well accepted in social settings the chimeric nature of padds make them well suited for many tasks such as proofreading editing and annotation of large format document like blueprintswe are presenting an architecture which supports the seamless manipulation of padds using today's technologies and reports on the lessons we learned while implementing the first padd system
we present statistical method called covering topic score cts to predict query performance for information retrieval estimation is based on how well the topic of user's query is covered by documents retrieved from certain retrieval system our approach is conceptually simple and intuitive and can be easily extended to incorporate features beyond bag of words such as phrases and proximity of terms experiments demonstrate that cts significantly correlates with query performance in variety of trec test collections and in particular cts gains more prediction power benefiting from features of phrases and proximity of terms we compare cts with previous state of the art methods for query performance prediction including clarity score and robustness score our experimental results show that cts consistently performs better than or at least as well as these other methods in addition to its high effectiveness cts is also shown to have very low computational complexity meaning that it can be practical for real applications
sensor networks consist of many small sensing devices that monitor an environment and communicate using wireless links the lifetime of these networks is severely curtailed by the limited battery power of the sensors one line of research in sensor network lifetime management has examined sensor selection techniques in which applications judiciously choose which sensors data should be retrieved and are worth the expended energy in the past many ad hoc approaches for sensor selection have been proposed in this paper we argue that sensor selection should be based upon tradeoff between application perceived benefit and energy consumption of the selected sensor setwe propose framework wherein the application can specify the utility of measuring data nearly concurrently at each set of sensors he goal is then to select sequence of sets to measure whose total utility is maximized while not exceeding the available energy alternatively we may look for the most cost effective sensor set maximizing the product of utility and system lifetimethis approach is very generic and permits us to model many applications of sensor networks we proceed to study two important classes of utility functions submodular and supermodular functions we show that the optimum solution for submodular functions can be found in polynomial time while optimizing the costeffectiveness of supermodular functions is np hard for practically important subclass of supermodular functions we present an lp based solution if nodes can send for different amounts of time and show that we can achieve an logn approximation ratio if each node has to send for the same amount of timefinally we study scenarios in which the quality of measurements is naturally expressed in terms of distances from targets we show that the utility based approach is analogous to penalty based approach in those scenarios and present preliminary results on some practically important special cases
while total order broadcast or atomic broadcast primitives have received lot of attention this paper concentrates on total order multicast to multiple groups in the context of asynchronous distributed systems in which processes may suffer crash failures multicast to multiple groups means that each message is sent to subset of the process groups composing the system distinct messages possibly having distinct destination groups total order means that all message deliveries must be totally ordered this paper investigates consensus based approach to solve this problem and proposes corresponding protocol to implement this multicast primitive this protocol is based on two underlying building blocks namely uniform reliable multicast and uniform consensus its design characteristics lie in the two following properties the first one is minimality property more precisely only the sender of message and processes of its destination groups have to participate in the total order multicast of the message the second property is locality property no execution of consensus has to involve processes belonging to distinct groups ie consensus is executed on per group basis this locality property is particularly useful when one is interested in using the total order multicast primitive in large scale distributed systems in addition to correctness proof an improvement that reduces the cost of the protocol is also suggested
as collaboration in virtual environments becomes more object focused and closely coupled the frequency of conflicts in accessing shared objects can increase in addition two kinds of concurrency control surprises become more disruptive to the collaboration undo surprises can occur when previously visible change is undone because of an access conflict intention surprises can happen when concurrent action by remote session changes the structure of shared object at the same perceived time as local access of that object such that the local user might not get what they expect because they have not had time to visually process the change hierarchy of three concurrency control mechanisms is presented in descending order of collaborative surprises which allows the concurrency scheme to be tailored to the tolerance for such surprises one mechanism is semioptimistic the other two are pessimistic designed for peer to peer vitual environments in which several threads have access to the shared scene graph these algorithms are straightforward and relatively simple they can be implemented using and java under windows and unix on both desktop and immersive systems in series of usability experiments the average performance of the most conservative concurrency control mechanism on local lan was found to be quite acceptable
using moving parabolic approximations mpa we reconstruct an improved point based model of curve or surface represented as an unorganized point cloud while also estimating the differential properties of the underlying smooth manifold we present optimization algorithms to solve these mpa models and examples which show that our reconstructions of the curve or surface and estimates of the normals and curvature information are accurate for precise point clouds and robust in the presence of noise
transactional memory tm is promising paradigm for concurrent programming this paper is an overview of our recent theoretical work on defining theory of tm we first recall some tm correctness properties and then overview results on the inherent power and limitations of tms
time sequences which are ordered sets of observations have been studied in various database applications in this paper we introduce new class of time sequences where each observation is represented by an interval rather than number such sequences may arise in many situations for instance we may not be able to determine the exact value at time point due to uncertainty or aggregation such observation may be represented better by range of possible values similarity search with interval time sequences as both query and data sequences poses new challenge for research we first address the issue of dis similarity measures for interval time sequences we choose an norm based measure because it effectively quantifies the degree of overlapping and remoteness between two intervals and is invariant irrespective of the position of an interval when it is enclosed within another interval we next propose an efficient indexing technique for fast retrieval of similar interval time sequences from large databases more specifically we propose to extract segment based feature vector for each sequence and to map each feature vector to either point or hyper rectangle in multi dimensional feature space we then show how we can use existing multi dimensional index structures such as the tree for efficient query processing the proposed method guarantees no false dismissals experimental results show that for synthetic and real stock data it is superior to sequential scanning in performance and scales well with the data size
application integration can be carried out on three different levels the data source level the business logic level and the user interface level with ontologies based integration on the data source level dating back to the and semantic web services for integrating on the business logic level coming of age it is time for the next logical step employing ontologies for integration on the user interface level such an approach supports both the developer in terms of reduced development times and the user in terms of better usability of integrated applications in this paper we introduce framework employing ontologies for integrating applications on the user interface level the acm portal is published by the association for computing machinery copyright acm inc terms of usage privacy policy code of ethics contact us useful downloads adobe acrobat quicktime windows media player real player
the handling of user preferences is becoming an increasingly important issue in present day information systems among others preferences are used for information filtering and extraction to reduce the volume of data presented to the user they are also used to keep track of user profiles and formulate policies to improve and automate decision makingwe propose here simple logical framework for formulating preferences as preference formulas the framework does not impose any restrictions on the preference relations and allows arbitrary operation and predicate signatures in preference formulas it also makes the composition of preference relations straightforward we propose simple natural embedding of preference formulas into relational algebra and sql through single winnow operator parameterized by preference formula the embedding makes possible the formulation of complex preference queries for example involving aggregation by piggybacking on existing sql constructs it also leads in natural way to the definition of further preference related concepts like ranking finally we present general algebraic laws governing the winnow operator and its interactions with other relational algebra operators the preconditions on the applicability of the laws are captured by logical formulas the laws provide formal foundation for the algebraic optimization of preference queries we demonstrate the usefulness of our approach through numerous examples
block correlations are common semantic patterns in storage systems these correlations can be exploited for improving the effectiveness of storage caching prefetching data layout and disk scheduling unfortunately information about block correlations is not available at the storage system level previous approaches for discovering file correlations in file systems do not scale well enough to be used for discovering block correlations in storage systems in this paper we propose miner an algorithm which uses data mining technique called frequent sequence mining to discover block correlations in storage systems miner runs reasonably fast with feasible space requirement indicating that it is practical tool for dynamically inferring correlations in storage system moreover we have also evaluated the benefits of block correlation directed prefetching and data layout through experiments our results using real system workloads show that correlation directed prefetching and data layout can reduce average response time by compared to the base case and compared to the commonly used sequential prefetching scheme
this paper describes onechip third generation reconfigurable processor architecture that integrates reconfigurable functional unit rfu into superscalar reduced instruction set computer risc processor's pipeline the architecture allows dynamic scheduling and dynamic reconfiguration it also provides support for pre loading configurations and for least recently used lru configuration managementto evaluate the performance of the onechip architecture several off the shelf software applications were compiled and executed on sim onechip an architecture simulator for onechip that includes software environment for programming the system the architecture is compared to similar one but without dynamic scheduling and without an rfu onechip achieves performance improvement and shows speedup range from up to for the different applications and data sizes used the results show that dynamic scheduling helps performance the most on average and that the rfu will always improve performance the best when most of the execution is in the rfu
most correlation clustering algorithms rely on principal component analysis pca as correlation analysis tool the correlation of each cluster is learned by applying pca to set of sample points since pca is rather sensitive to outliers if small fraction of these points does not correspond to the correct correlation of the cluster the algorithms are usually misled or even fail to detect the correct results in this paper we evaluate the influence of outliers on pca and propose general framework for increasing the robustness of pca in order to determine the correct correlation of each cluster we further show how our framework can be applied to pca based correlation clustering algorithms thorough experimental evaluation shows the benefit of our framework on several synthetic and real world data sets
existing dram controllers employ rigid non adaptive scheduling and buffer management policies when servicing prefetch requests some controllers treat prefetch requests the same as demand requests others always prioritize demand requests over prefetch requests however none of these rigid policies result in the best performance because they do not take into account the usefulness of prefetch requests if prefetch requests are useless treating prefetches and demands equally can lead to significant performance loss and extra bandwidth consumption in contrast if prefetch requests are useful prioritizing demands over prefetches can hurt performance by reducing dram throughput and delaying the service of useful requests this paper proposes new low cost memory controller called prefetch aware dram controller padc that aims to maximize the benefit of useful prefetches and minimize the harm caused by useless prefetches to accomplish this padc estimates the usefulness of prefetch requests and dynamically adapts its scheduling and buffer management policies based on the estimates the key idea is to adaptively prioritize between demand and prefetch requests and drop useless prefetches to free up memory system resources based on the accuracy of the prefetcher our evaluation shows that padc significantly outperforms previous memory controllers with rigid prefetch handling policies on both single and multi core systems with variety of prefetching algorithms across wide range of multiprogrammed spec cpu workloads it improves system performance by on core system and by on an core system while reducing dram bandwidth consumption by and respectively
most hardware predictors are table based eg two level branch predictors and have exponential size growth in the number of input bits or features eg previous branch outcomes this growth severely limits the amount of predictive information that such predictors can use to avoid exponential growth we introduce the idea of dynamic feature selection for building hardware predictors that can use large amount of predictive information based on this idea we design the dynamic decision tree ddt predictor which exhibits only linear size growth in the number of features our initial evaluation in branch prediction shows that the general purpose ddt using only branch history features is comparable on average to conventional branch predictors opening the door to practically using large numbers of additional features
referential integrity is an essential global constraint in relational database that maintains it in complete and consistent state in this work we assume the database may violate referential integrity and relations may be denormalized we propose set of quality metrics defined at four granularity levels database relation attribute and value that measure referential completeness and consistency quality metrics are efficiently computed with standard sql queries that incorporate two query optimizations left outer joins on foreign keys and early foreign key grouping experiments evaluate our proposed metrics and sql query optimizations on real and synthetic databases showing they can help in detecting and explaining referential errors
cluster based replication solutions are an attractive mechanism to provide both high availability and scalability for the database backend within the multi tier information systems of service oriented businesses an important issue that has not yet received sufficient attention is how database replicas that have failed can be reintegrated into the system or how completely new replicas can be added in order to increase the capacity of the system ideally recovery takes place online ie while transaction processing continues at the replicas that are already running in this paper we present complete online recovery solution for database clusters one important issue is to find an efficient way to transfer the data the joining replica needs in this paper we present two data transfer strategies the first transfers the latest copy of each data item the second transfers the updates rejoining replica has missed during its downtime second challenge is to coordinate this transfer with ongoing transaction processing such that the joining node does not miss any updates we present coordination protocol that can be used with postgres replication tool which uses group communication system for replica control we have implemented and compared our transfer solutions against set of parameters and present heuristics which allow an automatic selection of the optimal strategy for given configuration
influence of items on some other items might not be the same as the association between these sets of items many tasks of data analysis are based on expressing influence of items on other items in this paper we introduce the notion of an overall influence of set of items on another set of items we also propose an extension to the notion of overall association between two items in database using the notion of overall influence we have designed two algorithms for influence analysis involving specific items in database as the number of databases increases on yearly basis we have adopted incremental approach in these algorithms experimental results are reported for both synthetic and real world databases
the growing number of information security breaches in electronic and computing systems calls for new design paradigms that consider security as primary design objective this is particularly relevant in the embedded domain where the security solution should be customized to the needs of the target system while considering other design objectives such as cost performance and power due to the increasing complexity and shrinking design cycles of embedded software most embedded systems present host of software vulnerabilities that can be exploited by security attacks many attacks are initiated by causing violation in the properties of data eg integrity privacy access control rules etc associated with trusted program that is executing on the system leading to range of undesirable effectsin this work we develop general framework that provides security assurance against wide class of security attacks our work is based on the observation that program's permissible behaviorwith respect to data accesses can be characterized by certain properties we present hardware software approach wherein such properties can be encoded as data attributes and enforced as security policies during program execution these policies may be application specific eg access control for certain data structures compiler generated eg enforcing that variables are accessed only within their scope or universally applicable to all programs eg disallowing writes to unallocated memory we show how an embedded system architecture can support such policies by enhancing the memory hierarchy to represent the attributes of each datum as security tags that are linked to it through its lifetime and ii adding configurable hardware checker that interprets the semantics of the tags and enforces the desired security policies we evaluated the effectiveness of the proposed architecture in enforcing various security policies for several embedded benchmarks our experiments in the context of the simplescalar framework demonstrate that the proposed solution ensures run time validation of program data properties with minimal execution time overheads
given two sets of moving objects future timestamp tq and distance threshold spatio temporal join retrieves all pairs of objects that are within distance at tq the selectivity of join equals the number of retrieved pairs divided by the cardinality of the cartesian product this paper develops model for spatio temporal join selectivity estimation based on rigorous probabilistic analysis and reveals the factors that affect the selectivity initially we solve the problem for id point and rectangle objects whose location and velocities distribute uniformly and then extend the results to multi dimensional spaces finally we deal with non uniform distributions using specialized spatio temporal histogram extensive experiments confirm that the proposed formulae are highly accurate average error below
we give semantics to polymorphic effect analysis that tracks possibly thrown exceptions and possible non termination for higher order language the semantics is defined using partial equivalence relations over standard monadic domain theoretic model of the original language and establishes the correctness of both the analysis itself and of the contextual program transformations that it enables
the need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available unfortunately the visualizations in existing systems do not satisfactorily resolve the basic dilemma of being readable both for the global structure of thenetwork and also for detailed analysis of local communities to address this problem we present nodetrix hybrid representation for networks that combines the advantages of two traditional representations node link diagrams are used to show the global structure of network while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities key contribution is set of interaction techniques these allow analysts to create nodetrix visualization by dragging selections to and from node link and matrix forms and to flexibly manipulate the nodetrix representation to explore the dataset andcreate meaningful summary visualizations of their findings finally we present case study applying nodetrix to the analysis of the infovis coauthorship dataset to illustrate the capabilities of nodetrix as both an exploration tool and an effective means of communicating results
the artifacts constituting software system often drift apart over time we have developed the software reflexion model technique to help engineers perform various software engineering tasks by exploiting rather than removing the drift between design and implementation more specifically the technique helps an engineer compare artifacts by summarizing where one artifact such as design is consistent with and inconsistent with another artifact such as source the technique can be applied to help software engineer evolve structural mental model of system to the point that it is good enough to be used for reasoning about task at hand the software reflexion model technique has been applied to support variety of tasks including design conformance change assessment and an experimental reengineering of the million lines of code microsoft excel product in this paper we provide formal characterization of the reflexion model technique discuss practical aspects of the approach relate experiences of applying the approach and tools and place the technique into the context of related work
abstraction and slicing are both techniques for reducing the size of the state space to be inspected during verification in this paper we present new model checking procedure for infinite state concurrent systems that interleaves automatic abstraction refinement which splits states according to new predicates obtained by craig interpolation with slicing which removes irrelevant states and transitions from the abstraction the effects of abstraction and slicing complement each other as the refinement progresses the increasing accuracy of the abstract model allows for more precise slice the resulting smaller representation gives room for additional predicates in the abstraction the procedure terminates when an error path in the abstraction can be concretized which proves that the system is erroneous or when the slice becomes empty which proves that the system is correct
compressing the instructions of an embedded program is important for cost sensitive low power control oriented embedded computing number of compression schemes have been proposed to reduce program size however the increased instruction density has an accompanying performance cost because the instructions must be decompressed before execution in this paper we investigate the performance penalty of hardware managed code compression algorithm recently introduced in ibm's powerpc this scheme is the first to combine many previously proposed code compression techniques making it an ideal candidate for study we find that code compression with appropriate hardware optimizations does not have to incur much performance loss furthermore our studies show this holds for architectures with wide range of memory configurations and issue widths surprisingly we find that performance increase over native code is achievable in many situations
detecting code clones has many software engineering applications existing approaches either do not scale to large code bases or are not robust against minor code modifications in this paper we present an efficient algorithm for identifying similar subtrees and apply it to tree representations of source code our algorithm is based on novel characterization of subtrees with numerical vectors in the euclidean space mathbb and an efficient algorithm to cluster these vectors wrt the euclidean distance metric subtrees with vectors in one cluster are considered similar we have implemented our tree similarity algorithm as clone detection tool called deckard and evaluated it on large code bases written in and java including the linux kernel and jdk our experiments show that deckard is both scalable and accurate it is also language independent applicable to any language with formally specified grammar
this paper presents an initial study of multimodal collaborative platform concerning user preferences and interaction technique adequacy towards task true collaborative interactions are missing aspect of the majority of nowadays multi user system on par with the lack of support towards impaired users in order to surpass these obstacles we provide an accessible platform for co located collaborative environments which aims at not only improving the ways users interact within them but also at exploring novel interaction patterns brief study regarding set of interaction techniques and tasks was conducted in order to assess the most suited modalities in certain settings we discuss the results drawn from this study detail some related conclusions and present future work directions
password authenticated key exchange pake protocols allow parties to share secret keys in an authentic manner based on an easily memorizable password recently lu and cao proposed three party password authenticated key exchange protocol so called pake based on ideas of the abdalla and pointcheval two party spake extended to three parties pake can be seen to have structure alternative to that of another three party pake protocol pake by abdalla and pointcheval furthermore simple improvement to pake was proposed very recently by chung and ku to resist the kind of attacks that applied to earlier versions of pake in this paper we show that pake falls to unknown key share attacks by any other client and undetectable online dictionary attacks by any adversary the latter attack equally applies to the recently improved pake indeed the provable security approach should be taken when designing pakes and furthermore our results highlight that extra cautions still be exercised when defining models and constructing proofs in this direction
efficient construction of inverted indexes is essential to provision of search over large collections of text data in this article we review the principal approaches to inversion analyze their theoretical cost and present experimental results we identify the drawbacks of existing inversion approaches and propose single pass inversion method that in contrast to previous approaches does not require the complete vocabulary of the indexed collection in main memory can operate within limited resources and does not sacrifice speed with high temporary storage requirements we show that the performance of the single pass approach can be improved by constructing inverted files in segments reducing the cost of disk accesses during inversion of large volumes of data
materialized xpath access control views are commonly used for enforcing access control when access control rules defining materialized xml access control view change the view must be adapted to reflect these changes the process of updating materialized view after its definition changes is referred to as view adaptation while xpath security views have been widely reported in literature the problem of view adaptation for xpath security views has not been addressed view adaptation results in view downtime during which users are denied access to security views to prevent unauthorized access thus efficient view adaptation is important for making xpath security views pragmatic in this work we show how to adapt an xpath access control view incrementally by re using the existing view which reduces computation and communication costs significantly and results in less downtime for the end user empirical evaluations confirm that the incremental view adaptation algorithms presented in this paper are efficient and scalable
class imbalance where the classes in dataset are not represented equally is common occurrence in machine learning classification models built with such datasets are often not practical since most machine learning algorithms would tend to perform poorly on the minority class instances we present unique evolutionary computing based data sampling approach as an effective solution for the class imbalance problem the genetic algorithm based approach evolutionary sampling works as majority undersampling technique where instances from the majority class are selectively removed this preserves the relative integrity of the majority class while maintaining the original minority class group our research prototype evann also implements genetic algorithm based optimization of modeling parameters for the machine learning algorithms considered in our study an extensive empirical investigation involving four real world datasets is performed comparing the proposed approach to other existing data sampling techniques that target the class imbalance problem our results demonstrate that evolutionary sampling both with and without learner optimization performs relatively better than other data sampling techniques detailed coverage of our case studies in this paper lends itself toward empirical replication
high level languages are growing in popularity however decades of software development have produced large libraries of fast time tested meritorious code that are impractical to recreate from scratch cross language bindings can expose low level code to high level languages unfortunately writing bindings by hand is tedious and error prone while mainstream binding generators require extensive manual annotation or fail to offer the language features that users of modern languages have come to expect we present an improved binding generation strategy based on static analysis of unannotated library source code we characterize three high level idioms that are not uniquely expressible in c's low level type system array parameters resource managers and multiple return values we describe suite of interprocedural analyses that recover this high level information and we show how the results can be used in binding generator for the python programming language in experiments with four large libraries we find that our approach avoids the mistakes characteristic of hand written bindings while offering level of python integration unmatched by prior automated approaches among the thousands of functions in the public interfaces of these libraries roughly exhibit the behaviors detected by our static analyses
the ability to predict at compile time the likelihood of particular branch being taken provides valuable information for several optimizations including global instruction scheduling code layout function inlining interprocedural register allocation and many high level optimizations previous attempts at static branch prediction have either used simple heuristics which can be quite inaccurate or put the burden onto the programmer by using execution profiling data or source code hintsthis paper presents new approach to static branch prediction called value range propagation this method tracks the weighted value ranges of variables through program much like constant propagation these value ranges may be either numeric of symbolic in nature branch prediction is then performed by simply consulting the value range of the appropriate variable heuristics are used as fallback for cases where the value range of the variable cannot be determined statically in the process value range propagationsubsumes both constant propagation and copy propagationexperimental results indicate that this approach produces significantly more accurate predictions than the best existing heuristic techniques the value range propagation method can be implemented over any ldquo factored rdquo dataflow representation with static single assignment property such as ssa form or dependence flow graph where the variables have been renamed to achieve single assignment experimental results indicate that the technique maintains the linear runtime behavior of constant propagation experienced in practice
in this paper we study the problem of effective keyword search over xml documents we begin by introducing the notion of valuable lowest common ancestor vlca to accurately and effectively answer keyword queries over xml documents we then propose the concept of compact vlca cvlca and compute the meaningful compact connected trees rooted as cvlcas as the answers of keyword queries to efficiently compute cvlcas we devise an effective optimization strategy for speeding up the computation and exploit the key properties of cvlca in the design of the stack based algorithm for answering keyword queries we have conducted an extensive experimental study and the experimental results show that our proposed approach achieves both high efficiency and effectiveness when compared with existing proposals
decision support systems help the decision making process with the use of olap on line analytical processing and data warehouses these systems allow the analysis of corporate data as olap and data warehousing evolve more and more complex data is being used xml extensible markup language is flexible text format allowing the interchange and the representation of complex data finding an appropriate model for an xml data warehouse tends to become complicated as more and more solutions appear hence in this survey paper we present an overview of the different proposals that use xml within data warehousing technology these proposals range from using xml data sources for regular warehouses to those using full xml warehousing solutions some researches merely focus on document storage facilities while others present adaptations of xml technology for olap even though there are growing number of researches on the subject many issues still remain unsolved
in this paper we propose an extension algorithm to closet one of the most efficient algorithms for mining frequent closed itemsets in static transaction databases to allow it to mine frequent closed itemsets in dynamic transaction databases in dynamic transaction database transactions may be added deleted and modified with time based on two variant tree structures our algorithm retains the previous mined frequent closed itemsets and updates them by considering the changes in the transaction databases only hence the frequent closed itemsets in the current transaction database can be obtained without rescanning the entire changed transaction database the performance of the proposed algorithm is compared with closet showing performance improvements for dynamic transaction databases compared to using mining algorithms designed for static transaction databases
this paper explores the suitability of dense circulant graphs of degree four for the design of on chip interconnection networks networks based on these graphs reduce the torus diameter in factor which translates into significant performance gains for unicast traffic in addition they are clearly superior to tori when managing collective communications this paper introduces new two dimensional node's labeling of the networks explored which simplifies their analysis and exploitation in particular it provides simple and optimal solutions to two important architectural issues routing and broadcasting other implementation issues such as network folding and scalability by using hierarchical networks are also explored in this work
wireless sensors are very small computers and understanding the timing and behavior of software written for them is crucial to ensuring that they perform correctly this paper outlines lightweight method for gathering behavioral and timing information from simulated executions of software written in the nesc tinyos environment the resulting data is used to generate both behavioral and timing profiles of the software using uml sequence diagrams to visualize the behavior and to present the timing information
repeated elements are ubiquitous and abundant in both manmade and natural scenes editing such images while preserving the repetitions and their relations is nontrivial due to overlap missing parts deformation across instances illumination variation etc manually enforcing such relations is laborious and error prone we propose novel framework where user scribbles are used to guide detection and extraction of such repeated elements our detection process which is based on novel boundary band method robustly extracts the repetitions along with their deformations the algorithm only considers the shape of the elements and ignores similarity based on color texture etc we then use topological sorting to establish partial depth ordering of overlapping repeated instances missing parts on occluded instances are completed using information from other instances the extracted repeated instances can then be seamlessly edited and manipulated for variety of high level tasks that are otherwise difficult to perform we demonstrate the versatility of our framework on large set of inputs of varying complexity showing applications to image rearrangement edit transfer deformation propagation and instance replacement
we investigate the problem of optimizing the routing performance of virtual network by adding extra random links our asynchronous and distributed algorithm ensures by adding single extra link per node that the resulting network is navigable small world ie in which greedy routing using the distance in the original network computes paths of polylogarithmic length between any pair of nodes with probability previously known small world augmentation processes require the global knowledge of the network and centralized computations which is unrealistic for large decentralized networks our algorithm based on careful multi layer sampling of the nodes and the construction of light overlay network bypasses these limitations for bounded growth graphs ie graphs where for any node and any radius the number of nodes within distance from is at most constant times the number of nodes within distance our augmentation process proceeds with high probability in log log communication rounds with log log messages of size log bits sent per node and requiring only log log bit space in each node where is the number of nodes and the diameter in particular with the only knowledge of original distances greedy routing computes between any pair of nodes in the augmented network path of length at most log log with probability and of expected length log log hence we provide distributed scheme to augment any bounded growth graph into small world with high probability in polylogarithmic time while requiring polylogarithmic memory we consider that the existence of such lightweight process might be first step towards the definition of more general construction process that would validate kleinberg's model as plausible explanation for the small world phenomenon in large real interaction networks
to achieve interoperability modern information systems and commerce applications use mappings to translate data from one representation to another in dynamic environments like the web data sources may change not only their data but also their schemas their semantics and their query capabilities such changes must be reflected in the mappings mappings left inconsistent by schema change have to be detected and updated as large complicated schemas become more prevalent and as data is reused in more applications manually maintaining mappings even simple mappings like view definitions is becoming impractical we present novel framework and tool tomas for automatically adapting mappings as schemas evolve our approach considers not only local changes to schema but also changes that may affect and transform many components of schema we consider comprehensive class of mappings for relational and xml schemas with choice types and nested constraints our algorithm detects mappings affected by structural or constraint change and generates all the rewritings that are consistent with the semantics of the mapped schemas our approach explicitly models mapping choices made by user and maintains these choices whenever possible as the schemas and mappings evolve we describe an implementation of mapping management and adaptation tool based on these ideas and compare it with mapping generation tool
in this paper we discuss the energy efficient multicast problem in ad hoc wireless networks each node in the network is assumed to have fixed level of transmission power the problem of our concern is given an ad hoc wireless network and multicast request how to find multicast tree such that the total energy cost of the multicast tree is minimized we first prove this problem is np hard and it is unlikely to have an approximation algorithm with constant performance ratio of the number of nodes in the network we then propose an algorithm based on the directed steiner tree method that has theoretically guaranteed approximation performance ratio we also propose two efficient heuristics node join tree njt and tree join tree tjt algorithms the njt algorithm can be easily implemented in distributed fashion extensive simulations have been conducted to compare with other methods and the results have shown significant improvement on energy efficiency of the proposed algorithms
the key notion in service oriented architecture is decoupling clients and providers of service based on an abstract service description which is used by the service broker to point clients to suitable service implementation client then sends service requests directly to the service implementation problem with the current architecture is that it does not provide trustworthy means for clients to specify service brokers to verify and service implementations to prove that certain desired non functional properties are satisfied during service request processing an example of such non functional property is access and persistence restrictions on the data received as part of the service requests in this work we propose an extension of the service oriented architecture that provides these facilities we also discuss prototype implementation of this architecture and report preliminary results that demonstrate the potential practical value of the proposed architecture in real world software applications
virtual humans are being increasingly used in different domains virtual human modeling requires to consider aspects belonging to different levels of abstractions for example at lower levels one has to consider aspects concerning the geometric definition of the virtual human model and appearance while at higher levels one should be able to define how the virtual human behaves into an environment anim the standard for representing humanoids in xd vrml worlds is mainly concerned with low level modeling aspects as result the developer has to face the problem of defining the virtual human behavior and translating it into lower levels eg geometrical and kinematic aspects in this paper we propose vha virtual human architecture software architecture that allows one to easily manage an interactive anim virtual human into xd vrml worlds the proposed solution allows the developer to focus mainly on high level aspects of the modeling process such as the definition of the virtual human behavior
robotic tape libraries are popular for applications with very high storage requirements such as video servers here we study the throughput of tape library system we design new scheduling algorithm the so called relief and compare it against some older straightforward ones like fcfs maximum queue length mql and an unfair one bypass roughly equivalent to shortest job first the proposed algorithm incorporates an aging mechanism in order to attain fairness and we prove that under certain assumptions it minimizes the average start up latency extensive simulation experiments show that relief outperforms its competitors fair and unfair alike with up to improvement in throughput for the same rejection ratio
this paper demonstrates the advantages of using controlled mobility in wireless sensor networks wsns for increasing their lifetime ie the period of time the network is able to provide its intended functionalities more specifically for wsns that comprise large number of statically placed sensor nodes transmitting data to collection point the sink we show that by controlling the sink movements we can obtain remarkable lifetime improvements in order to determine sink movements we first define mixed integer linear programming milp analytical model whose solution determines those sink routes that maximize network lifetime our contribution expands further by defining the first heuristics for controlled sink movements that are fully distributed and localized our greedy maximum residual energy gmre heuristic moves the sink from its current location to new site as if drawn toward the area where nodes have the highest residual energy we also introduce simple distributed mobility scheme random movement or rm according to which the sink moves uncontrolled and randomly throughout the network the different mobility schemes are compared through extensive ns based simulations in networks with different nodes deployment data routing protocols and constraints on the sink movements in all considered scenarios we observe that moving the sink always increases network lifetime in particular our experiments show that controlling the mobility of the sink leads to remarkable improvements which are as high as sixfold compared to having the sink statically and optimally placed and as high as twofold compared to uncontrolled mobility
the method described in this article evaluates case similarity in the retrieval stage of case based reasoning cbr it thus plays key role in deciding which case to select and therefore in deciding which solution will be eventually applied in cbr there are many retrieval techniques one feature shared by most is that case retrieval is based on attribute similarity and importance however there are other crucial factors that should be considered such as the possible consequences of given solution in other words its potential loss and gain as their name clearly implies these concepts are defined as functions measuring loss and gain when given retrieval case solution is applied moreover these functions help the user to choose the best solution so that when mistake is made the resulting loss is minimal in this way the highest benefit is always obtained
activity centric computing acc systems seek to address the fragmentation of office work across tools and documents by allowing users to organize work around the computational construct of an activity defining and structuring appropriate activities within system poses challenge for users that must be overcome in order to benefit from acc support we know little about how knowledge workers appropriate the activity construct to address this we studied users appropriation of production quality acc system lotus activities for everyday work by employees in large corporation we contribute to better understanding of how users articulate their individual and collaborative work in the system by providing empirical evidence of their patterns of appropriation we conclude by discussing how our findings can inform the design of other acc systems for the workplace
we present an interactive system for synthesizing urban layouts by example our method simultaneously performs both structure based synthesis and an image based synthesis to generate complete urban layout with plausible street network and with aerial view imagery our approach uses the structure and image data of real world urban areas and synthesis algorithm to provide several high level operations to easily and interactively generate complex layouts by example the user can create new urban layouts by sequence of operations such as join expand and blend without being concerned about low level structural details further the ability to blend example urban layout fragments provides powerful way to generate new synthetic content we demonstrate our system by creating urban layouts using example fragments from several real world cities each ranging from hundreds to thousands of city blocks and parcels
dimensionality reduction is an essential data preprocessing technique for large scale and streaming data classification tasks it can be used to improve both the efficiency and the effectiveness of classifiers traditional dimensionality reduction approaches fall into two categories feature extraction and feature selection techniques in the feature extraction category are typically more effective than those in feature selection category however they may break down when processing large scale data sets or data streams due to their high computational complexities similarly the solutions provided by the feature selection approaches are mostly solved by greedy strategies and hence are not ensured to be optimal according to optimized criteria in this paper we give an overview of the popularly used feature extraction and selection algorithms under unified framework moreover we propose two novel dimensionality reduction algorithms based on the orthogonal centroid algorithm oc the first is an incremental oc ioc algorithm for feature extraction the second algorithm is an orthogonal centroid feature selection ocfs method which can provide optimal solutions according to the oc criterion both are designed under the same optimization criterion experiments on reuters corpus volume data set and some public large scale text data sets indicate that the two algorithms are favorable in terms of their effectiveness and efficiency when compared with other state of the art algorithms
most large public displays have been used for providing information to passers by with the primary purpose of acting as one way information channels to individual users we have developed large public display to which users can send their own media content using mobile devices the display supports multi touch interaction thus enabling collaborative use of the display this display called citywall was set up in city center with the goal of showing information of events happening in the city we observed two user groups who used mobile phones with upload capability during two large scale events happening in the city our findings are that this kind of combined use of personal mobile devices and large public display as publishing forum used collaboratively with other users creates unique setting that extends the group's feeling of participation in the events we substantiate this claim with examples from user data
abstract almost all semantics for logic programs with negation identify set sem of models of program as the intended semantics of and any model in this class is considered possible meaning of with regard to the semantics the user has in mind thus for example in the case of stable models check end of sentence choice models check end of sentence answer sets check end of sentence etc different possible models correspond to different ways of completing the incomplete information in the logic program however different end users may have different ideas on which of these different models in sem is reasonable one from their point of view for instance given sem user may prefer model in sem to model in sem based on some evaluation criterion that she has in this paper we develop logic program semantics based on optimal models this semantics does not add yet another semantics to the logic programming arena it takes as input an existing semantics sem and user specified objective function obj and yields new semantics underline rm opt subseteq sem that realizes the objective function within the framework of preferred models identified already by sem thus the user who may or may not know anything about logic programming has considerable flexibility in making the system reflect her own objectives by building on top of existing semantics known to the system in addition to the declarative semantics we provide complete complexity analysis and algorithms to compute optimal models under varied conditions when sem is the stable model semantics the minimal models semantics and the all models semantics
the growing nature of databases and the flexibility inherent in the sql query language that allows arbitrarily complex formulations can result in queries that take inordinate amount of time to complete to mitigate this problem strategies that are optimized to return the first few rows or top rows in case of sorted results are usually employed however both these strategies can lead to unpredictable query processing times thus in this paper we propose supporting time constrained sql queries specifically user issues sql query as before but additionally provides nature of constraint soft or hard an upper bound for query processing time and acceptable nature of results partial or approximate the dbms takes the criteria constraint type time limit quality of result into account in generating the query execution plan which is expected guaranteed to complete in the allocated time for soft hard time constraint if partial results are acceptable then the technique of reducing result set cardinality ie returning first few or top rows is used whereas if approximate results are acceptable then sampling is used to compute query results within the specified time limit for the latter case we argue that trading off quality of results for predictable response time is quite useful however for this case we provide additional aggregate functions to estimate the aggregate values and to compute the associated confidence interval this paper presents the notion of time constrained sql queries discusses the challenges in supporting such construct describes framework for supporting such queries and outlines its implementation in oracle database by exploiting oracle's cost based optimizer and extensibility capabilities
scenarios have been advocated as means of improving requirements engineering yet few methods or tools exist to support scenario based re the paper reports method and software assistant tool for scenario based re that integrates with use case approaches to object oriented development the method and operation of the tool are illustrated with financial system case study scenarios are used to represent paths of possible behavior through use case and these are investigated to elaborate requirements the method commences by acquisition and modeling of use case the use case is then compared with library of abstract models that represent different application classes each model is associated with set of generic requirements for its class hence by identifying the class es to which the use case belongs generic requirements can be reused scenario paths are automatically generated from use cases then exception types are applied to normal event sequences to suggest possible abnormal events resulting from human error generic requirements are also attached to exceptions to suggest possible ways of dealing with human error and other types of system failure scenarios are validated by rule based frames which detect problematic event patterns the tool suggests appropriate generic requirements to deal with the problems encountered the paper concludes with review of related work and discussion of the prospects for scenario based re methods and tools
the new hybrid clone detection tool nicad combines the strengths and overcomes the limitations of both text based and ast based clone detection techniques and exploits novel applications of source transformation system to yield highly accurate identification of cloned code in software systems in this paper we present an in depth study of near miss function clones in open source software using nicad we examine more than open source java and num systems including the entire linux kernel apache httpd jsdk swing and dbo and compare their use of cloned code in several different dimensions including language clone size clone similarity clone location and clone density both by proportion of cloned functions and lines of cloned code we manually verify all detected clones and provide complete catalogue of different clones in an online repository in variety of formats these validated results can be used as cloning reference for these systems and as benchmark for evaluating other clone detection tools copyright copy john wiley sons ltd in this paper we provide an empirical study of function clones in more than open source java and num systems of varying kinds and sizes including the entire linux kernel using the new hybrid clone detection method nicad we manually verify all the detected clones and provide complete catalogue of the different clones in an online repository in variety of formats our studies show that there are large number of near miss function clones in those systems copyright copy john wiley sons ltd
mobile agents are becoming increasingly important in the highly distributed applications frameworks seen today their routing dispatching from node to node is very important issue as we need to safeguard application efficiency achieve better load balancing and resource utilization throughout the underlying network selecting the best target server for dispatching mobile agent is therefore multi faceted problem that needs to be carefully tackled in this paper we propose distributed adaptive routing schemes next node selection for mobile agents the proposed schemes overcome risks like load oscillations ie agents simultaneously abandoning congested node in search for other less saturated node we try to induce different routing decisions taken by agents to achieve load balancing and better utilization of network resources we consider five different algorithms and evaluate them through simulations our findings are quite promising both from the user application and the network infrastructure perspective
the evolution of geographic phenomena has been one of the concerns of spatiotemporal database research however in large spectrum of geographical applications users need more than mere representation of data evolution for instance in urban management applications mdash eg cadastral evolution mdash users often need to know why how and by whom certain changes have been performed as well as their possible impact on the environment answers to such queries are not possible unless supplementary information concerning real world events is associated with the corresponding changes in the database and is managed efficiently this paper proposes solution to this problem which is based on extending spatiotemporal database with mechanism for managing documentation on the evolution of geographic information this solution has been implemented in gis based prototype which is also discussed in the paper
we describe an efficient top down strategy for overlap removal and floorplan repair which repairs overlaps in floorplans produced by placement algorithms or rough floorplanning methodologies the algorithmic framework that we propose incorporates novel geometric shifting technique within top down flow the effect of our algorithm is quantified across broad range of floorplans produced by multiple tools our method succeeds in producing valid placements in almost all cases moreover compared to leading methods it requires only one fifth the run time and produces placements with to less hpwl and up to less cell movement
in this paper we describe conceptual framework and address the related issues and solutions in the identification of three major challenges for the development and evaluation of immersive digital educational games idegs these challenges are advancing adaptive educational technologies to shape learning experience ensuring the individualization of learning experiences adaptation to personal aims needs abilities and prerequisites ii providing technological approaches to reduce the development costs for idegs by enabling the creation of entirely different stories and games for variety of different learning domains each based more or less on the same pool of story units patterns and structures iii developing robust evaluation methodologies for idegs by the extension of iso to include user satisfaction motivation and learning progress and other user experience ux attributes while our research and development is by no means concluded we believe that we have arrived at stage where conclusions may be drawn which will be of considerable use to other researchers in this domain
recently lot of work has been done on formalization of business process specification in particular using petri nets and process algebra however these efforts usually do not explicitly address complex business process development which necessitates the specification coordination and synchronization of large number of business steps it is imperative that these atomic tasks are associated correctly and monitored for countless dependencies moreover as these business processes grow they become critically reliant on large number of split and merge points which additionally increases modeling complexity therefore one of the central challenges in complex business process modeling is the composition of dependent business steps we address this challenge and introduce formally correct method for automated composition of algebraic expressions in complex business process modeling based on acyclic directed graph reductions we show that our method generates an equivalent algebraic expression from an appropriate acyclic directed graph if the graph is well formed and series parallel additionally we encapsulate the reductions in an algorithm that transforms business step dependencies described by users into digraphs recognizes structural conflicts identifies wheatstone bridges and finally generates algebraic expressions
to narrow the semantic gap in content based image retrieval cbir relevance feedback is utilized to explore knowledge about the user's intention in finding target image or image category users provide feedback by marking images returned in response to query image as relevant or irrelevant existing research explores such feedback to refine querying process select features or learn image classifier however the vast amount of unlabeled images is ignored and often substantially limited examples are engaged into learning in this paper we address the two issues and propose novel effective method called relevance aggregation projections rap for learning potent subspace projections in semi supervised way given relevances and irrelevances specified in the feedback rap produces subspace within which the relevant examples are aggregated into single point and the irrelevant examples are simultaneously separated by large margin regarding the query plus its feedback samples as labeled data and the remainder as unlabeled data rap falls in special paradigm of imbalanced semi supervised learning through coupling the idea of relevance aggregation with semi supervised learning we formulate constrained quadratic optimization problem to learn the subspace projections which entail semantic mining and therefore make the underlying cbir system respond to the user's interest accurately and promptly experiments conducted over large generic image database show that our subspace approach outperforms existing subspace methods for cbir even with few iterations of user feedback
applications that analyze mine and visualize large datasets are considered an important class of applications in many areas of science engineering and business queries commonly executed in data analysis applications often involve user defined processing of data and application specific data structures if data analysis is employed in collaborative environment the data server should execute multiple such queries simultaneously to minimize the response time to clients in this paper we present the design of runtime system for executing multiple query workloads on shared memory machine we describe experimental results using an application for browsing digitized microscopy images
in recent years classification learning for data streams has become an important and active research topic major challenge posed by data streams is that their underlying concepts can change over time which requires current classifiers to be revised accordingly and timely to detect concept change common methodology is to observe the online classification accuracy if accuracy drops below some threshold value concept change is deemed to have taken place an implicit assumption behind this methodology is that any drop in classification accuracy can be interpreted as symptom of concept change unfortunately however this assumption is often violated in the real world where data streams carry noise that can also introduce significant reduction in classification accuracy to compound this problem traditional noise cleansing methods are incompetent for data streams those methods normally need to scan data multiple times whereas learning for data streams can only afford one pass scan because of data's high speed and huge volume another open problem in data stream classification is how to deal with missing values when new instances containing missing values arrive how learning model classifies them and how the learning model updates itself according to them is an issue whose solution is far from being explored to solve these problems this paper proposes novel classification algorithm flexible decision tree flexdt which extends fuzzy logic to data stream classification the advantages are three fold first flexdt offers flexible structure to effectively and efficiently handle concept change second flexdt is robust to noise hence it can prevent noise from interfering with classification accuracy and accuracy drop can be safely attributed to concept change third it deals with missing values in an elegant way extensive evaluations are conducted to compare flexdt with representative existing data stream classification algorithms using large suite of data streams and various statistical tests experimental results suggest that flexdt offers significant benefit to data stream classification in real world scenarios where concept change noise and missing values coexist
as more and more human motion data are becoming widely used to animate computer graphics figures in many applications the growing need for compact storage and fast transmission makes it imperative to compress motion data we propose data driven method for efficient compression of human motion sequences by exploiting both spatial and temporal coherences of the data we first segment motion sequence into subsequences such that the poses within subsequence lie near low dimensional linear space we then compress each segment using principal component analysis our method achieves further compression by storing only the key frames projections to the principal component space and interpolating the other frames in between via spline functions the experimental results show that our method can achieve significant compression rate with low reconstruction errors
wireless mesh networks wmns can provide seamless broadband connectivity to network users with low setup and maintenance costs to support next generation applications with real time requirements however these networks must provide improved quality of service guarantees current mesh protocols use techniques that fail to accurately predict the performance of end to end paths and do not optimize performance based on knowledge of mesh network structures in this paper we propose quorum routing protocol optimized for wmns that provides accurate qos properties by correctly predicting delay and loss characteristics of data traffic quorum integrates novel end to end packet delay estimation mechanism with stability aware routing policies allowing it to more accurately follow qos requirements while minimizing misbehavior of selfish nodes
this paper presents method for acquiring concession strategy of an agent in multi issue negotiation this method learns how to make concession to an opponent for realizing win win negotiation to learn the concession strategy we adopt reinforcement learning first an agent receives proposal from an opponent the agent recognizes negotiation state using the difference between their proposals and the difference between their concessions according to the state the agent makes proposal by reinforcement learning reward of the learning is profit of an agreement and punishment of negotiation breakdown the experimental results showed that the agents could acquire the negotiation strategy that avoids negotiation breakdown and increases profits of an agreement as result agents can acquire the action policy that strikes balance between cooperation and competition
the capabilities of current mobile devices especially pdas are making it possible to design and develop mobile applications that employ visual techniques for using geographic data in the field these applications can be extremely useful in areas as diverse as tourism business natural resources management and homeland security in this paper we present system aimed at supporting users in the exploratory analysis of geographic data on pdas through highly interactive interface based on visual dynamic queries we propose alternative visualizations to display query results and present an experimental evaluation aimed at comparing their effectiveness on pda in tourist scenario our findings provide an experimental confirmation of the unsuitability of the typical visualization employed by classic dynamic query systems which displays only those results that fully satisfy query in those cases where only sub optimal results are obtainable for such cases the results of our study highlight the usefulness of visualizations that display all results and their degree of satisfaction of the query
the problem confronted in the content based image retrieval research is the semantic gap between the low level feature representing and high level semantics in the images this paper describes way to bridge such gap by learning the similar images given from the user the system extracts the similar region pairs and classifies those similar region pairs either as object or non object semantics and either as object relation or non object relation semantics automatically which are obtained from comparing the distances and spatial relationships in the similar region pairs by themselves the system also extracts interesting parts of the features from the similar region pair and then adjusts each interesting feature and region pair weight dynamically using those objects and object relation semantics as well as the dynamic weights adjustment from the similar images the semantics of those similar images can be mined and used for searching the similar images the experiments show that the proposed system can retrieve the similar images well and efficient
in this paper some studies have been made on the essence of fuzzy linear discriminant analysis lda algorithm and fuzzy support vector machine fsvm classifier respectively as kernel based learning machine fsvm is represented with the fuzzy membership function while realizing the same classification results with that of the conventional pair wise classification it outperforms other learning machines especially when unclassifiable regions still remain in those conventional classifiers however serious drawback of fsvm is that the computation requirement increases rapidly with the increase of the number of classes and training sample size to address this problem an improved fsvm method that combines the advantages of fsvm and decision tree called dt fsvm is proposed firstly furthermore in the process of feature extraction reformative lda algorithm based on the fuzzy nearest neighbors fknn is implemented to achieve the distribution information of each original sample represented with fuzzy membership grade which is incorporated into the redefinition of the scatter matrices in particular considering the fact that the outlier samples in the patterns may have some adverse influence on the classification result we developed novel lda algorithm using relaxed normalized condition in the definition of fuzzy membership function thus the classification limitation from the outlier samples is effectively alleviated finally by making full use of the fuzzy set theory complete lda cf lda framework is developed by combining the reformative lda rf lda feature extraction method and dt fsvm classifier this hybrid fuzzy algorithm is applied to the face recognition problem extensive experimental studies conducted on the orl and nust face images databases demonstrate the effectiveness of the proposed algorithm
the region analysis of tofte and talpin is an attempt to determine statically the life span of dynamically allocated objects but the calculus is at once intuitively simple yet deceptively subtle and previous theoretical analyses have been frustratingly complex no analysis has revealed and explained in simple terms the connection between the subleties of the calculus and the imperative features it builds on we present novel approach for proving safety and correctness of simplified version of the region calculus we give stratified operational semantics composed of highlevel semantics dealing with the conceptual difficulties of effect annotations and low level one with explicit operations on region indexed store the main results of the paper are proof simpler than previous ones and modular approach to type safety and correctness the flexibility of this approach is demonstrated by the simplicity of the extension to the full calculus with type and region polymorphism
ontologies enable to directly encode domain knowledge in software applications so ontology based systems can exploit the meaning of information for providing advanced and intelligent functionalities one of the most interesting and promising application of ontologies is information extraction from unstructured documents in this area the extraction of meaningful information from pdf documents has been recently recognized as an important and challenging problem this paper proposes an ontology based information extraction system for pdf documents founded on well suited knowledge representation approach named self populating ontology spo the spo approach combines object oriented logic based features with formal grammar capabilities and allows expressing knowledge in term of ontology schemas instances and extraction rules called descriptors aimed at extracting information having also tabular form the novel aspect of the spo approach is that it allows to represent ontologies enriched by rules that enable them to populate them self with instances extracted from unstructured pdf documents in the paper the tractability of the spo approach is proven moreover features and behavior of the prototypical implementation of the spo system are illustrated by means of running example
using wireless peer to peer interactions between portable devices it is possible to locally share information and maintain spatial temporal knowledge emanating from the surroundings we consider the prospects for unleashing ambient data from the surrounding environment for information provision using two biological phenomena human mobility and human social interaction this leads to analogies with epidemiology and is highly relevant to future technology rich environments here embedded devices in the physical environment such as sensors and wireless enabled appliances represent information sources that can provide extensive situated information in this paper we address candidate scenario where isolated sensors in the environment provide real time data from fixed locations using simulation we examine what happens when information is greedily acquired and shared by mobile participants through peer to peer interaction this is assessed taking into account availability of source nodes and the effects of mobility with respect to temporal accuracy of information the results reaffirm the need to consider range of mobility models in testing and validating protocols
we study adding aggregate operators such as summing up elements of column of relation to logics with counting mechanisms the primary motivation comes from database applications where aggregate operators are present in all real life query languages unlike other features of query languages aggregates are not adequately captured by the existing logical formalisms consequently all previous approaches to analyzing the expressive power of aggregation were only capable of producing partial results depending on the allowed class of aggregate and arithmetic operationswe consider powerful counting logic and extend it with the set of all aggregate operators we show that the resulting logic satisfies analogs of hanf's and gaifman's theorems meaning that it can only express local properties we consider database query language that expresses all the standard aggregates found in commercial query languages and show how it can be translated into the aggregate logic thereby providing number of expressivity bounds that do not depend on particular class of arithmetic functions and that subsume all those previously known we consider restricted aggregate logic that gives us tighter capture of database languages and also use it to show that some questions on expressivity of aggregation cannot be answered without resolving some deep problems in complexity theory
while there have been advances in visualization systems particularly in multi view visualizations and visual exploration the process of building visualizations remains major bottleneck in data exploration we show that provenance metadata collected during the creation of pipelines can be reused to suggest similar content in related visualizations and guide semi automated changes we introduce the idea of query by example in the context of an ensemble of visualizations and the use of analogies as first class operations in system to guide scalable interactions we describe an implementation of these techniques in vistrails publicly available open source system
since the first results published in by liu and layland on the rate monotonic rm and earliest deadline first edf algorithms lot of progress has been made in the schedulability analysis of periodic task sets unfortunately many misconceptions still exist about the properties of these two scheduling methods which usually tend to favor rm more than edf typical wrong statements often heard in technical conferences and even in research papers claim that rm is easier to analyze than edf it introduces less runtime overhead it is more predictable in overload conditions and causes less jitter in task executionsince the above statements are either wrong or not precise it is time to clarify these issues in systematic fashion because the use of edf allows better exploitation of the available resources and significantly improves system's performancethis paper compares rm against edf under several aspects using existing theoretical results specific simulation experiments or simple counterexamples to show that many common beliefs are either false or only restricted to specific situations
we present methodology for data warehouse design and its application within the telecom italia information system the methodology is based on conceptual representation of the enterprise which is exploited both in the integration phase of the warehouse information sources and during the knowledge discovery activity on the information stored in the warehouse the application of the methodology in the telecom italia framework has been supported by prototype software tools both for conceptual modeling and for data integration and reconciliation
sprint is middleware infrastructure for high performance and high availability data management it extends the functionality of standalone in memory database imdb server to cluster of commodity shared nothing servers applications accessing an imdb are typically limited by the memory capacity of the machine running the imdb sprint partitions and replicates the database into segments and stores them in several data servers applications are then limited by the aggregated memory of the machines in the cluster transaction synchronization and commitment rely on total order multicast differently from previous approaches sprint does not require accurate failure detection to ensure strong consistency allowing fast reaction to failures experiments conducted on cluster with data servers using tpc and micro benchmark showed that sprint can provide very good performance and scalability
the effort in software process support has focused so far on modeling and enacting processes certain amount of work has been done but little has reached satisfactory level of maturity and acceptance in our opinion this is due to the difficulty for system to accommodate the very numerous aspects involved in software processes complete process support should cover topics ranging from low level tasks like compiling to organizational and strategic tasks this includes process enhancement resource management and control cooperative work etc the environment must also be convenient for software engineers team leaders managers and so on it must be able to describe details for efficient execution and be high level for capturing understanding etc as matter of fact the few tools that have reached sufficient maturity have focussed on single topic and addressed single class of usersit is our claim that no single system can provide satisfactory solution except in clearly defined subdomain thus we shifted our attention from finding the universal system to finding ways to make many different systems cooperate with their associated formalisms and process enginesthis paper presents novel approach for software process support environments based on federation of heterogeneous and autonomous components the approach has been implemented and experimented in the apel environment it is shown which architecture and technology is involved how it works which interoperability paradigms have been used which problems we have solved and which issues are still under study
as the internet grows in size it becomes crucial to understand how the speeds of links in the network must improve in order to sustain the pressure of new end nodes being added each day although the speeds of links in the core and at the edges improve roughly according to moore's law this improvement alone might not be enough indeed the structure of the internet graph and routing in the network might necessitate much faster improvements in the speeds of key links in the network in this paper using combination of analysis and extensive simulations we show that the worst congestion in the internet as level graph in fact scales poorly with the network size sup omega sup where is the number of nodes when shortest path routing is used to route traffic between ases we also show somewhat surprisingly that policy based routing does not exacerbate the maximum congestion when compared to shortest path routing our results show that it is crucial to identify ways to alleviate this congestion to avoid some links from being perpetually congested to this end we show that the congestion scaling properties of internet like graphs can be improved dramatically by introducing moderate amounts of redundancy in the graph in terms of parallel edges between pairs of adjacent nodes
challenge involved in applying density based clustering to categorical biomedical data is that the cube of attribute values has no ordering defined making the search for dense subspaces slow we propose the hierdenc algorithm for hierarchical density based clustering of categorical data and complementary index for searching for dense subspaces efficiently the hierdenc index is updated when new objects are introduced such that clustering does not need to be repeated on all objects the updating and cluster retrieval are efficient comparisons with several other clustering algorithms showed that on large datasets hierdenc achieved better runtime scalability on the number of objects as well as cluster quality by fast collapsing the bicliques in large networks we achieved an edge reduction of as much as hierdenc is suitable for large and quickly growing datasets since it is independent of object ordering does not require re clustering when new data emerges and requires no user specified input parameters
this paper surveys how the maximum adjacency ma ordering of the vertices in graph can be used to solve various graph problems we first explain that the minimum cut problem can be solved efficiently by utilizing the ma ordering the idea is then extended to fundamental operation of graph edge splitting based on this the edge connectivity augmentation problem for given and also for the entire range of can be solved efficiently by making use of the ma ordering where it is asked to add the smallest number of new edges to given graph so that its edge connectivity is increased to other related topics are also surveyed
this paper examines algorithmic aspects of searching for approximate functional dependencies in database relation the goal is to avoid exploration of large parts of the space of potential rules this is accomplished by leveraging found rules to make finding other rules more efficient the overall strategy is an attribute at time iteration which uses local breadth first searches on lattices that increase in width and height in each iteration the resulting algorithm provides many opportunities to apply heuristics to tune the search for particular data sets and or search objectives the search can be tuned at both the global iteration level and the local search level number of heuristics are developed and compared experimentally
current recommender systems attempt to identify appealing items for user by applying syntactic matching techniques which suffer from significant limitations that reduce the quality of the offered suggestions to overcome this drawback we have developed domain independent personalization strategy that borrows reasoning techniques from the semantic web elaborating recommendations based on the semantic relationships inferred between the user's preferences and the available items our reasoning based approach improves the quality of the suggestions offered by the current personalization approaches and greatly reduces their most severe limitations to validate these claims we have carried out case study in the digital tv field in which our strategy selects tv programs interesting for the viewers from among the myriad of contents available in the digital streams our experimental evaluation compares the traditional approaches with our proposal in terms of both the number of tv programs suggested and the users perception of the recommendations finally we discuss concerns related to computational feasibility and scalability of our approach
despite the effectiveness of search engines the persistently increasing amount of web data continuously obscures the search task efforts have thus concentrated on personalized search that takes account of user preferences new concept is introduced towards this direction search based on ranking of local set of categories that comprise user search profile new algorithms are presented that utilize web page categories to personalize search results series of user based experiments show that the proposed solutions are efficient finally we extend the application of our techniques in the design of topic focused crawlers which can be considered an alternative personalized search
we discuss some basic issues of interactive computations in the framework of rough granular computing among these issues are hierarchical modeling of granule structures and interactions between granules of different complexity interactions between granules on which computations are performed are among the fundamental concepts of wisdom technology wistech wistech is encompassing such areas as interactive computations multiagent systems cognitive computation natural computing complex adaptive and autonomous systems or knowledge representation and reasoning about knowledge
the tremendous growth of system memories has increased the capacities and capabilities of memory resident embedded databases yet current embedded databases need to be tuned in order to take advantage of new memory technologies in this paper we study the implications of hosting memory resident databases and propose hardware and software query driven techniques to improve their performance and energy consumption we exploit the structured organization of memories which enables selective mode of operation in which banks are accessed selectively unused banks are placed in lower power mode based on access pattern information we propose hardware techniques that dynamically control the memory by making the system adapt to the access patterns that arise from queries we also propose software query directed scheme that directly modifies the queries to reduce the energy consumption by ensuring uniform bank accesses our results show that these optimizations could lead to at the least reduction in memory energy we also show that query directed schemes better utilize the low power modes achieving up to improvement
modern database systems provide not only powerful data models but also complex query languages supporting powerful features such as the ability to create new database objects and invocation of arbitrary methods possibly written in third party programming language in this sense query languages have evolved into powerful programming languages surprisingly little work exists utilizing techniques from programming language research to specify and analyse these query languages this paper provides formal high level operational semantics for complex value oql like query language that can create fresh database objects and invoke external methods we define type system for our query language and prove an important soundness propertywe define simple effect typing discipline to delimit the computational effects within our queries we prove that this effect system is correct and show how it can be used to detect cases of non determinism and to define correct query optimizations
we present practical technique for pointing and selection using combination of eye gaze and keyboard triggers eyepoint uses two step progressive refinement process fluidly stitched together in look press look release action which makes it possible to compensate for the accuracy limitations of the current state of the art eye gaze trackers while research in gaze based pointing has traditionally focused on disabled users eyepoint makes gaze based pointing effective and simple enough for even able bodied users to use for their everyday computing tasks as the cost of eye gaze tracking devices decreases it will become possible for such gaze based techniques to be used as viable alternative for users who choose not to use mouse depending on their abilities tasks and preferences
there has been tremendous growth in the amount and range of information available on the internet the users requests for online information can be captured by long tail model few popular websites enjoy high number of visitations while the majority of the rest are less frequently requested in this study we use real world data to investigate this phenomenon and show that both users physical location and time of access affect the heterogeneity of website requests the effect can partially be explained by differences in demographic characteristics at locations and diverse user browsing behavior in weekdays and weekends these results can be used to design better online marketing strategies affiliate advertising models and internet caching algorithms with sensitivities to user location and time of access differences
leakage energy reduction for caches has been the target of many recent research efforts in this work we propose novel compiler directed approach to reduce the data cache leakage energy by exploiting the program behavior the proposed approach is based on the observation that only small portion of the data are active at runtime and the program spends lot of time in loops so large portion of data cache lines which are not accessed by the loop can be placed into the leakage control mode to reduce leakage energy consumption the compiler directed approach does not require hardware counters to monitor the access patterns of the cache lines and it is adaptive to the program behavior the experimental results show that the compiler directed approach is very competitive in terms of energy consumption and energy delay product compared to the recently proposed pure hardware based approach we also show that the utilization of loop transformations can increase the effectiveness of our strategy
with an increasing use of data mining tools and techniques we envision that knowledge discovery and data mining system kddms will have to support and optimize for the following scenarios sequence of queries user may analyze one or more datasets by issuing sequence of related complex mining queries and multiple simultaneous queries several users may be analyzing set of datasets concurrently and may issue related complex queriesthis paper presents systematic mechanism to optimize for the above cases targeting the class of mining queries involving frequent pattern mining on one or multiple datasets we present system architecture and propose new algorithms to simultaneously optimize multiple such queries and use knowledgeable cache to store and utilize the past query results we have implemented and evaluated our system with both real and synthetic datasets our experimental results show that our techniques can achieve speedup of up to factor of compared with the systems which do not support caching or optimize for multiple queries
collaborative brainstorming can be challenging but important part of creative group problem solving mind mapping has the potential to enhance the brainstorming process but has its own challenges when used in group we introduce groupmind collaborative mind mapping tool that addresses these challenges and opens new opportunities for creative teamwork including brainstorming we present semi controlled evaluation of groupmind and its impact on teamwork problem solving and collaboration for brainstorming activities groupmind performs better than using traditional whiteboard in both interaction group and nominal group settings for the task involving memory recall the hierarchical mind map structure also imposes important framing effects on group dynamics and idea organization during the brainstorming process we also present design ideas to assist in the development of future tools to support creative problem solving in groups
new paradigm for mobile service chain's competitive and collaborative mechanism is proposed in this study the main idea of the proposed approach is based on multi agent system with optimal profit of the pull push and collaborative models among the portal access service provider pasp the product service provider psp and the mobile service provider msp to address the running mechanism for the multi agent system an integrated system framework is proposed based on the agent evolution algorithm aea which could resolve all these modes to examine the feasibility of the framework prototype system based on java repast is implemented the simulation experiments show that this system can help decision makers take the appropriate strategies with higher profits by analyzing the expectations and variances or risks of each player's profit the interaction between and among entities in the chain is well understood it is found that in the situation where collaborative mechanism is applied the performance of players is better as compared to the other two situations where competitive mechanism is implemented if some constraints are applied the risk will be kept at low level
the common abstraction of xml schema by unranked regular tree languages is not entirely accurate to shed some light on the actual expressive power of xml schema intuitive semantical characterizations of the element declarations consistent edc rule are provided in particular it is obtained that schemas satisfying edc can only reason about regular properties of ancestors of nodes hence with respect to expressive power xml schema is closer to dtds than to tree automata these theoretical results are complemented with an investigation of the xml schema definitions xsds occurring in practice revealing that the extra expressiveness of xsds over dtds is only used to very limited extent as this might be due to the complexity of the xml schema specification and the difficulty of understanding the effect of constraints on typing and validation of schemas simpler formalism equivalent to xsds is proposed it is based on contextual patterns rather than on recursive types and it might serve as light weight front end for xml schema next the effect of edc on the way xml documents can be typed is discussed it is argued that cleaner more robust larger but equally feasible class is obtained by replacing edc with the notion of pass preorder typing ppt schemas that allow one to determine the type of an element of streaming document when its opening tag is met this notion can be defined in terms of grammars with restrained competition regular expressions and there is again an equivalent syntactical formalism based on contextual patterns finally algorithms for recognition simplification and inclusion of schemas for the various classes are given
we propose method to handle approximate searching by image content in medical image databases image content is represented by attributed relational graphs holding features of objects and relationships between objects the method relies on the assumption that fixed number of labeled or expected objects eg heart lungs etc are common in all images of given application domain in addition to variable number of unexpected or unlabeled objects eg tumor hematoma etc the method can answer queries by example such as find all rays that are similar to smith's ray the stored images are mapped to points in multidimensional space and are indexed using state of the art database methods trees the proposed method has several desirable propertiesdatabase search is approximate so that all images up to prespecified degree of similarity tolerance are retrievedit has no false dismissals ie all images qualifying query selection criteria are retrieved it is much faster than sequential scanning for searching in the main memory and on the disk ie by up to an order of magnitude thus scaling up well for large databases
this paper proposes novel method using constant inter frame motion for self calibration from an image sequence of an object rotating around single axis with varying camera internal parameters our approach makes use of the facts that in many commercial systems rotation angles are often controlled by an electromechanical system and that the inter frame essential matrices are invariant if the rotation angles are constant but not necessary known therefore recovering camera internal parameters is possible by making use of the equivalence of essential matrices which relate the unknown calibration matrices to the fundamental matrices computed from the point correspondences we also describe linear method that works under restrictive conditions on camera internal parameters the solution of which can be used as the starting point of the iterative non linear method with looser constraints the results are refined by enforcing the global constraints that the projected trajectory of any point should be conic after compensating for the focusing and zooming effects finally using the bundle adjustment method tailored to the special case ie static camera and constant object rotation the structure of the object is recovered and the camera parameters are further refined simultaneously to determine the accuracy and the robustness of the proposed algorithm we present the results on both synthetic and real sequences
discriminative reranking is one method for constructing high performance statistical parsers collins discriminative reranker requires source of candidate parses for each sentence this paper describes simple yet novel method for constructing sets of best parses based on coarse to fine generative parser charniak this method generates best lists that are of substantially higher quality than previously obtainable we used these parses as the input to maxent reranker johnson et al riezler et al that selects the best parse from the set of parses for each sentence obtaining an score of on sentences of length or less
we have proposed the extent system for automated photograph annotation using image content and context analysis key component of extent is landmark recognition system called landmarker in this paper we present the architecture of landmarker the content of query photograph is analyzed and compared against database of sample landmark images to recognize any landmarks it contains an algorithm is presented for comparing query image with sample image context information may be used to assist landmark recognition also we show how landmarker deals with scalability to allow recognition of large number of landmarks we have implemented prototype of the system and present empirical results on large dataset
this article describes our research on spoken language translation aimed toward the application of computer aids for second language acquisition the translation framework is incorporated into multilingual dialogue system in which student is able to engage in natural spoken interaction with the system in the foreign language while speaking query in their native tongue at any time to obtain spoken translation for language assistance thus the quality of the translation must be extremely high but the domain is restricted experiments were conducted in the weather information domain with the scenario of native english speaker learning mandarin chinese we were able to utilize large corpus of english weather domain queries to explore and compare variety of translation strategies formal example based and statistical translation quality was manually evaluated on test set of spontaneous utterances the best speech translation performance percnt correct percnt incorrect and percnt rejected is achieved by system which combines the formal and example based methods using parsability by domain specific chinese grammar as rejection criterion
preventive measures sometimes fail to defect malicious attacks with cyber attacks on data intensive applications becoming an ever more serious threat intrusion tolerant database systems are significant concern the main objective of intrusion tolerant database systems is to detect attacks and to assess and repair the damage caused by the attacks in timely manner such that the database will not be damaged to such degree that is unacceptable or useless this paper focuses on efficient damage assessment and repair in resilient distributed database systems the complexity of distributed database systems caused by data partition distributed transaction processing and failures makes damage assessment and repair much more challenging than in centralized database systems this paper identifies the key challenges and presents an efficient algorithm for distributed damage assessment and repair
this paper presents new algorithm that detects set of dominant points on the boundary of an eight connected shape to obtain polygonal approximation of the shape itself the set of dominant points is obtained from the original break points of the initial boundary where the integral square is zero for this goal most of the original break points are deleted by suppressing those whose perpendicular distance to an approximating straight line is lower than variable threshold value the proposed algorithm iteratively deletes redundant break points until the required approximation which relies on decrease in the length of the contour and the highest error is achieved comparative experiment with another commonly used algorithm showed that the proposed method produced efficient and effective polygonal approximations for digital planar curves with features of several sizes
multisource data flow problems involve information which may enter nodes independently through different classes of edges in some cases dissimilar meet operations appear to be used for different types of nodes these problems include bidirectional and flow sensitive problems as well as many static analyses of concurrent programs with synchronization tuple frameworks type of standard data flow framework provide natural encoding for multisource problems using single meet operator previously the solution of these problems has been described as the fixed point of set of data flow equations using our tuple representation we can access the general results of standard data flow frameworks concerning convergence time and solution precision for these problems we demonstrate this for the bidirectional component of partial redundancy suppression and two problems on the program summary graph an interesting subclass of tuple frameworks the join of meets frameworks is useful for reachability problems especially those stemming from analyses of explicitly parallel programs we give results on function space properties for join of meets frameworks that indicate precise solutions for most of them will be difficult to obtain
in this paper we analyze the node spatial distribution of mobile wireless ad hoc networks characterizing this distribution is of fundamental importance in the analysis of many relevant properties of mobile ad hoc networks such as connectivity average route length and network capacity in particular we have investigated under what conditions the node spatial distribution resulting after large number of mobility steps resembles the uniform distribution this is motivated by the fact that the existing theoretical results concerning mobile ad hoc networks are based on this assumption in order to test this hypothesis we performed extensive simulations using two well known mobility models the random waypoint model which resembles intentional movement and brownian like model which resembles nonintentional movement our analysis has shown that in brownian like motion the uniformity assumption does hold and that the intensity of the concentration of nodes in the center of the deployment region that occurs in the random waypoint model heavily depends on the choice of some mobility parameters for extreme values of these parameters the uniformity assumption is impaired
we present tool that predicts whether the software under development inside an ide has bug an ide plugin performs this prediction using the change classification technique to classify source code changes as buggy or clean during the editing session change classification uses support vector machines svm machine learning classifier algorithm to classify changes to projects mined from their configuration management repository this technique besides being language independent and relatively accurate can classify change immediately upon its completion and use features extracted solely from the change delta added deleted and the source code to predict buggy changes thus integrating change classification within an ide can predict potential bugs in the software as the developer edits the source code ideally reducing the amount of time spent on fixing bugs later to this end we have developed change classification plugin for eclipse based on client server architecture described in this paper
weak pseudorandom function wprf is cryptographic primitive similar to but weaker than pseudorandom function for wprfs one only requires that the output is pseudorandom when queried on random inputs we show that unlike normal prfs wprfs are seed incompressible in the sense that the output of wprf is pseudorandom even if bounded amount of information about the key is leakedas an application of this result we construct simple mode of operation which when instantiated with any wprf gives leakage resilient stream cipher the implementation of such cipher is secure against every side channel attack as long as the amount of information leaked per round is bounded but overall can be arbitrary large the construction is simpler than the previous one dziembowski pietrzak focs as it only uses single primitive wprf in straight forward manner
we describe framework for finding and tracking trails for autonomous outdoor robot navigation through combination of visual cues and ladar derived structural information the algorithm is able to follow paths which pass through multiple zones of terrain smoothness border vegetation tread material and illumination conditions our shape based visual trail tracker assumes that the approaching trail region is approximately triangular under perspective it generates region hypotheses from learned distribution of expected trail width and curvature variation and scores them using robust measure of color and brightness contrast with flanking regions the structural component analogously rewards hypotheses which correspond to empty or low density regions in groundstrike filtered ladar obstacle map our system's performance is analyzed on several long sequences with diverse appearance and structural characteristics ground truth segmentations are used to quantify performance where available and several alternative algorithms are compared on the same data
after reviewing number of internet tools and technologies originating in the field of logic programming and discussing promissing directions of ongoing research we describe logic programming based networking infrastructure which combines reasoning and knowledge processing with flexible coordination of dynamic state changes and computation mobility as well as and its use for the design of intelligent mobile agent programs lightweight logic programming language jinni implemented in java is introduced as flexible scripting tool for gluing together knowledge processing components and java objects in networked client server applications and thin client environments as well as through applets over the web mobile threads implemented by capturing first order continuations in compact data structure sent over the network allow jinni to interoperate with remote high performance binprolog servers for cpu intensive knowledge processing controlled natural language to prolog translator with support of third party speech recognition and text to speech translation allows interaction with users not familiar with logic programming
science projects of various disciplines face fundamental challenge thousands of users want to obtain new scientific results by application specific and dynamic correlation of data from globally distributed sources considering the involved enormous and exponentially growing data volumes centralized data management reaches its limits since scientific data are often highly skewed and exploration tasks exhibit large degree of spatial locality we propose the locality aware allocation of data objects onto distributed network of interoperating databases hisbase is an approach to data management in scientific federated data grids that addresses the scalability issue by combining established techniques of database research in the field of spatial data structures quadtrees histograms and parallel databases with the scalable resource sharing and load balancing capabilities of decentralized peer to peer pp networks the proposed combination constitutes complementary science infrastructure enabling load balancing and increased query throughput
the web and especially major web search engines are essential tools in the quest to locate online information for many people this paper reports results from research that examines characteristics and changes in web searching from nine studies of five web search engines based in the us and europe we compare interactions occurring between users and web search engines from the perspectives of session length query length query complexity and content viewed among the web search engines the results of our research shows users are viewing fewer result pages searchers on us based web search engines use more query operators than searchers on european based search engines there are statistically significant differences in the use of boolean operators and result pages viewed and one cannot necessary apply results from studies of one particular web search engine to another web search engine the wide spread use of web search engines employment of simple queries and decreased viewing of result pages may have resulted from algorithmic enhancements by web search engine companies we discuss the implications of the findings for the development of web search engines and design of online content
in this paper we formulate two classes of problems the colored range query problems and the colored point enclosure query problems to model multi dimensional range and point enclosure queries in the presence of categorical information many of these problems are difficult to solve using traditional data structural techniques based on new framework of combining sketching techniques and traditional data structures we obtain two sets of results in solving the problems approximately and efficiently in addition the framework can be employed to attack other related problems by finding the appropriate summary structures
in this paper we show how pattern matching can be seen to arise from proof term assignment for the focused sequent calculus this use of the curry howard correspondence allows us to give novel coverage checking algorithm and makes it possible to give rigorous correctness proof for the classical pattern compilation strategy of building decision trees via matrices of patterns
in this paper we propose complete model handling the physical simulation of deformable objects we formulate continuous expressions for stretching bending and twisting energies these expressions are mechanically rigorous and geometrically exact both elastic and plastic deformations are handled to simulate wide range of materials we validate the proposed model in several classical test configurations the use of geometrical exact energies with dynamic splines provides very accurate results as well as interactive simulation times which shows the suitability of the proposed model for constrained cad applications we illustrate the application potential of the proposed model by describing virtual system for cable positioning which can be used to test compatibility between planned fixing clip positions and mechanical cable properties
emerging applications in the area of emergency response and disaster management are increasingly demanding interactive capabilities to allow for the quick understanding of critical situation in particular in urban environments key component of these interactive simulations is how to recreate the behavior of crowd in real time while supporting individual behaviors crowds can often be unpredictable and present mixed behaviors such as panic or aggression that can very rapidly change based on unexpected new elements introduced into the environment we present preliminary research specifically oriented towards the simulation of large crowds for emergency response and rescue planning situations our approach uses highly scalable architecture integrated with an efficient rendering architecture and an immersive visualization environment for interaction in this environment users can specify complex scenarios plug in crowd behavior algorithms and interactively steer the simulation to analyze and evaluate multiple what if situations
we introduce an algorithm for space variant filtering of video based on spatio temporal laplacian pyramid and use this algorithm to render videos in order to visualize prerecorded eye movements spatio temporal contrast and colour saturation are reduced as function of distance to the nearest gaze point of regard ie non fixated distracting regions are filtered out whereas fixated image regions remain unchanged results of an experiment in which the eye movements of an expert on instructional videos are visualized with this algorithm so that the gaze of novices is guided to relevant image locations show that this visualization technique facilitates the novices perceptual learning
this paper presents an investigation into dynamic self adjustment of task deployment and other aspects of self management through the embedding of multiple policiesnon dedicated loosely coupled computing environments such as clusters and grids are increasingly popular platforms for parallel processing these abundant systems are highly dynamic environments in which many sources of variability affect the run time efficiency of tasks the dynamism is exacerbated by the incorporation of mobile devices and wireless communicationthis paper proposes an adaptive strategy for the flexible run time deployment of tasks to continuously maintain efficiency despite the environmental variability the strategy centres on policy based scheduling which is informed by contextual and environmental inputs such as variance in the round trip communication time between client and its workers and the effective processing performance of each workera self management framework has been implemented for evaluation purposes the framework integrates several policy controlled adaptive services with the application code enabling the run time behaviour to be adapted to contextual and environmental conditions using this framework an exemplar self managing parallel application is implemented and used to investigate the extent of the benefits of the strategy
in the area of health care and sports in recent years variety of mobile applications have been established mobile devices are of emerging interest due to their high availability and increasing computing power in many different health scenarios in this paper we present scalable secure sensor monitoring platform ssmp which collects vital data of users vital parameters can be collected by just one single sensor or in multi sensor configuration nowadays wide spectrum of sensors is available which provide wireless connectivity eg bluetooth vital data can then easily be transmitted to mobile device which subsequently transmits these data to an ehealth portal there are already solutions implementing these capabilities however privacy aspects of users are very often neglected since health data may enable people to draw potentially compromising conclusions eg insurance companies it is absolutely necessary to design an enhanced security concept in this context to complicate matters further the trustworthiness of providers which are operating with user's health data can not be determined by users priori this means that the security concept implemented by the provider may bear security flaws additionally there is no guarantee that the provider preserves the users privacy claims in this work we propose security concept incorporating privacy aspects using mobile devices for transferring and storing health data at portal in addition the concept guarantees anonymity in the transfer process as well as for stored data at service provider hence insider attacks based on stored data can be prevented
next generation system designs are challenged by multiple walls among them the inter related impediments offered by power dissipation limits and reliability are particularly difficult ones that all current chip system design teams are grappling with in this paper we first describe the attendant challenges in integrated multi dimensional pre silicon modeling and the solution approaches being pursued later we focus on leading edge solutions for power thermal and failure rate mitigation that have been proposed in our work over the past decade
the main goal of this paper is to provide routing table free online algorithms for wireless sensor networks wsns to select cost eg node residual energies and delay efficient paths as basic information to drive the routing process both node costs and hop count distances are considered particular emphasis is given to greedy routing schemes due to their suitability for resource constrained and highly dynamic networks for what concerns greedy forwarding we present the statistically assisted routing algorithm sara where forwarding decisions are driven by statistical information on the costs of the nodes within coverage and in the second order neighborhood by analysis we prove that an optimal online policy exists we derive its form and we exploit it as the core of sara besides greedy techniques sub optimal algorithms where node costs can be partially propagated through the network are also presented these techniques are based on real time learning lrta algorithms which through an initial exploratory phase converge to quasi globally optimal paths all the proposed schemes are then compared by simulation against globally optimal solutions discussing the involved trade offs and possible performance gains the results show that the exploitation of second order cost information in sara substantially increases the goodness of the selected paths with respect to fully localized greedy routing finally the path quality can be further increased by lrta schemes whose convergence can be considerably enhanced by properly setting real time search parameters however these solutions fail in highly dynamic scenarios as they are unable to adapt the search process to time varying costs
peer to peer pp networks are beginning to form the infrastructure of future applications computers are organized in pp overlay networks to facilitate search queries with reasonable cost so scalability is major aim in design of pp networks in this paper to obtain high factor of scalability we partition network search space using consistent static shared upper ontology we name our approach semantic partition tree spt all resources and queries are annotated using the upper ontology and queries are semantically routed in the overlay network also each node indexes addresses of other nodes that possess contents expressible by the concept it maintains so our approach can be conceived as an ontology based distributed hash table dht also we introduce lookup service for the network which is very scalable and independent of the network size and just depends on depth of the ontology tree further we introduce broadcast algorithm on the network we present worst case analysis of both lookup and broadcast algorithms and measure their performance using simulation the results show that our scheme is highly scalable and can be used in real pp applications
stacked wafer integration has the potential to improve multiprocessor system on chip mpsoc integration density performance and power efficiency however the power density of mpsocs increases with the number of active layers resulting in high chip temperatures this can reduce system reliability reduce performance and increase cooling cost thermal optimization for mpsocs imposes numerous challenges it is difficult to manage assignment and scheduling of heterogeneous workloads to maintain thermal safety in addition the thermal characteristics of mpsocs differ from those of mpsocs because each stacked layer has different thermal resistance to the ambient and vertically adjacent processors have strong temperature correlation we propose mpsoc thermal optimization algorithm that conducts task assignment scheduling and voltage scaling power balancing algorithm is initially used to distribute tasks among cores and active layers detailed thermal analysis is used to guide hotspot mitigation algorithm that incrementally reduces the peak mpsoc temperature by appropriately adjusting task execution times and voltage levels the proposed algorithm considers leakage power consumption and adapts to inter layer thermal heterogeneity performance evaluation on set of multiprogrammed and multithreaded benchmarks indicates that the proposed techniques can optimize dmpsoc power consumption power profile and chip peak temperature
the test scheduling problem is one of the major issues in the test integration of system on chip soc and test schedule is usually influenced by the test access mechanism tam in this paper we propose graph based approach to power constrained test scheduling with tam assignment and test conflicts also considered by mapping test schedule to subgraph of the test compatibility graph an interval graph recognition method can be used to determine the order of the core tests we then present heuristic algorithm that can effectively assign tam wires to the cores given the test order with the help of the tabu search method and the test compatibility graph the proposed algorithm allows rapid exploration of the solution space experimental results for the itc benchmarks show that short test length is achieved within reasonable computation time
commercial workloads form an important class of applications and have performance characteristics that are distinct from scientific and technical benchmarks such as spec cpu however due to the prohibitive simulation time of commercial workloads it is extremely difficult to use them in computer architecture research in this paper we study the efficacy of using statistical sampling based simulation methodology for two classes of commercial workloads java server benchmark specjbb and an online transaction processing oltp benchmark dbt our results show that although specjbb shows distinct garbage collection phases there are no large scale phases in the oltp benchmark we take advantage of this stationary behavior in steady phase and propose statistical sampling based simulation technique dynasim with two dynamic stopping rules in this approach the simulation terminates once the target accuracy has been met we apply dynasim to simulate commercial workloads and show that with the simulation of only few million total instructions the error can be within at confidence level of dynasim compares favorably with random sampling and representative sampling in terms of the total number of instructions simulated time cost and with representative sampling in terms of the number of checkpoints storage cost dynasim increases the usability of sampling based simulation approach for commercial workloads and will encourage the use of commercial workloads in computer architecture research
we describe new data format for storing triangular symmetric and hermitian matrices called rectangular full packed format rfpf the standard two dimensional arrays of fortran and also known as full format that are used to represent triangular and symmetric matrices waste nearly half of the storage space but provide high performance via the use of level blas standard packed format arrays fully utilize storage array space but provide low performance as there is no level packed blas we combine the good features of packed and full storage using rfpf to obtain high performance via using level blas as rfpf is standard full format representation also rfpf requires exactly the same minimal storage as packed the format each lapack full and or packed triangular symmetric and hermitian routine becomes single new rfpf routine based on eight possible data layouts of rfpf this new rfpf routine usually consists of two calls to the corresponding lapack full format routine and two calls to level blas routines this means no new software is required as examples we present lapack routines for cholesky factorization cholesky solution and cholesky inverse computation in rfpf to illustrate this new work and to describe its performance on several commonly used computer platforms performance of lapack full routines using rfpf versus lapack full routines using the standard format for both serial and smp parallel processing is about the same while using half the storage performance gains are roughly one to factor of for serial and one to factor of for smp parallel times faster using vendor lapack full routines with rfpf than with using vendor and or reference packed routines
application resource usage models can be used in the decision making process for ensuring quality of service as well as for capacity planning apart from their general use in performance modeling optimization and systems management current solutions for modeling application resource usage tend to address parts of the problem by either focusing on specific application or specific platform or on small subset of system resources we propose simple and flexible approach for modeling application resource usage in platform independent manner that enables the prediction of application resource usage on unseen platforms the technique proposed is application agnostic requiring no modification to the application binary or source and no knowledge of application semantics we implement linux based prototype and evaluate it using four different workloads including real world applications and benchmarks our experiments reveal prediction errors that are bound within of the observed for these workloads when using the proposed approach
similarity search is core module of many data analysis tasks including search by example classification and clustering for time series data dynamic time warping dtw has been proven very effective similarity measure since it minimizes the effects of shifting and distortion in time however the quadratic cost of dtw computation to the length of the matched sequences makes its direct application on databases of long time series very expensive we propose technique that decomposes the sequences into number of segments and uses cheap approximations thereof to compute fast lower bounds for their warping distances we present several progressively tighter bounds relying on the existence or not of warping constraints finally we develop an index and multi step technique that uses the proposed bounds and performs two levels of filtering to efficiently process similarity queries thorough experimental study suggests that our method consistently outperforms state of the art methods for dtw similarity search
various known models of probabilistic xml can be represented as instantiations of the abstract notion of documents in addition to ordinary nodes documents have distributional nodes that specify the possible worlds and their probabilistic distribution particular families of documents are determined by the types of distributional nodes that can be used as well as by the structural constraints on the placement of those nodes in document some of the resulting families provide natural extensions and combinations of previously studied probabilistic xml models the focus of the paper is on the expressive power of families of documents in particular two main issues are studied the first is the ability to efficiently translate given document of one family into another family the second is closure under updates namely the ability to efficiently represent the result of updating the instances of document of given family as another document of that family for both issues we distinguish two variants corresponding to value based and object based semantics of documents
we discuss the parallelization of algorithms for solving poly nomial systems symbolically by way of triangular decompositions we introduce component level parallelism for which the number of processors in use depends on the geometry of the solution set of the input system our long term goal is to achieve an efficient multi level parallelism coarse grained component level for tasks computing geometric objects in the solution sets and medium fine grained level for polynomial arithmetic such as gcd resultant computation within each task component level parallelization of triangular decompositions belongs to the class of dynamic irregular parallel applications which leads us to address the following question how to exploit geometrical information at an early stage of the solving process that would be favorable to parallelization we report on the effectiveness of the approaches that we have applied including modular methods solving by decreasing order of dimension task pool with dimension and rank guided scheduling we have extended the aldor programming language to support multiprocessed parallelism on smps and realized preliminary implementation our experimentation shows promising speedups for some well known problems and proves that our component level parallelization is practically efficient we expect that this speedup would add multiplicative factor to the speedup of medium fine grained level parallelization as parallel gcd and resultant computations
transactional coherence and consistency tcc offers way to simplify parallel programming by executing all code within transactions in tcc systems transactions serve as the fundamental unit of parallel work communication and coherence as each transaction completes it writes all of its newly produced state to shared memory atomically while restarting other processors that have speculatively read stale data with this mechanism tcc based system automatically handles data synchronization correctly without programmer intervention to gain the benefits of tcc programs must be decomposed into transactions we describe two basic programming language constructs for decomposing programs into transactions loop conversion syntax and general transaction forking mechanism with these constructs writing correct parallel programs requires only small incremental changes to correct sequential programs the performance of these programs may then easily be optimized based on feedback from real program execution using few simple techniques
modern stream applications such as sensor monitoring systems and publish subscription services necessitate the handling of large numbers of continuous queries specified over high volume data streams efficient sharing of computations among multiple continuous queries especially for the memory and cpu intensive window based operations is critical novel challenge in this scenario is to allow resource sharing among similar queries even if they employ windows of different lengths this paper first reviews the existing sharing methods in the literature and then illustrates the significant performance shortcomings of these methodsthis paper then presents novel paradigm for the sharing of window join queries namely we slice window states of join operator into fine grained window slices and form chain of sliced window joins by using an elaborate pipelining methodology the number of joins after state slicing is reduced from quadratic to linear this novel sharing paradigm enables us to push selections down into the chain and flexibly select subsequences of such sliced window joins for computation sharing among queries with different window sizes based on the state slice sharing paradigm two algorithms are proposed for the chain buildup one minimizes the memory consumption while the other minimizes the cpu usage the algorithms are proven to find the optimal chain with respect to memory or cpu usage for given query workload we have implemented the slice share paradigm within the data stream management system cape the experimental results show that our strategy provides the best performance over diverse range of workload settings among all alternate solutions in the literature
in ad hoc networks the performance is significantly degraded as the size of the network grows the network clustering by which the nodes are hierarchically organized on the basis of the proximity relieves this performance degradation finding the weakly connected dominating set wcds is promising approach for clustering the wireless ad hoc networks finding the minimum wcds in the unit disk graph is an np hard problem and host of approximation algorithms has been proposed in this article we first proposed centralized approximation algorithm called dla cc based on distributed learning automata dla for finding near optimal solution to the minimum wcds problem then we propose dla based clustering algorithm called dla dc for clustering the wireless ad hoc networks the proposed cluster formation algorithm is distributed implementation of dla cc in which the dominator nodes and their closed neighbors assume the role of the cluster heads and cluster members respectively in this article we compute the worst case running time and message complexity of the clustering algorithm for finding near optimal cluster head set we argue that by proper choice of the learning rate of the clustering algorithm trade off between the running time and message complexity of algorithm with the cluster head set size clustering optimality can be made the simulation results show the superiority of the proposed algorithms over the existing methods
traditional compilers compile and optimize files separately making worst case assumptions about the program context in which file is to be linked more aggressive compilation architectures perform cross file interprocedural or whole program analyses potentially producing much faster programs but substantially increasing the cost of compilation even more radical are systems that perform all compilation and optimization at run time such systems can optimize programs based on run time program and system properties as well as static whole program properties however run time compilers also called dynamic compilers or just in time compilers suffer under severe constraints on allowable compilation time since any time spent compiling steals from time spent running the program none of these compilation models dominates the others each has unique strengths and weaknesses not present in the other modelswe are developing new staged compilation model which strives to combine high run time code quality with low compilation overhead compilation is organized as series of stages with stages corresponding to for example separate compilation library linking program linking and run time execution any given optimization can be performed at any of these stages to reduce compilation time while maintaining high effectiveness an optimization should be performed at the earliest stage that provides the necessary program context information to carry out the optimization effectively moreover single optimization can itself be spread across multiple stages with earlier stages performing preplanning work that enables the final stage to complete the optimization quickly in this way we hope to produce highly optimized programs nearly as good as what could be done with purely run time compiler that had an unconstrained compilation time budget but at much more practical compile time costwe are building the whirlwind optimizing compiler as the concrete embodiment of this staged compilation model initially targeting object oriented languages key component of whirlwind is set of techniques for automatically constructing staged compilers from traditional unstaged compilers including aggressive applications of specialization and other partial evaluation technology
many cache management schemes designed for mobile environments are based on invalidation reports irs however ir based approach suffers from long query latency and it cannot efficiently utilize the broadcast bandwidth in this paper we propose techniques to address these problems first by replicating small fraction of the essential information related to cache invalidation the query latency can be reduced then we propose techniques to efficiently utilize the broadcast bandwidth based on counters associated with each data item novel techniques are designed to maintain the accuracy of the counter in case of server failures client failures and disconnections extensive simulations are provided and used to evaluate the proposed methodology compared to previous ir based algorithms the proposed solution can significantly reduce the query latency improve the bandwidth utilization and effectively deal with disconnections and failures
recently many natural language processing nlp applications have improved the quality of their output by using various machine learning techniques to mine information extraction ie patterns for capturing information from the input text currently to mine ie patterns one should know in advance the type of the information that should be captured by these patterns in this work we propose novel methodology for corpus analysis based on cross examination of several document collections representing different instances of the same domain we show that this methodology can be used for automatic domain template creation as the problem of automatic domain template creation is rather new there is no well defined procedure for the evaluation of the domain template quality thus we propose methodology for identifying what information should be present in the template using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates
model transformations provide powerful capability to automate model refinements however the use of model transformation languages may present challenges to those who are unfamiliar with specific transformation language this paper presents an approach called model transformation by demonstration mtbd which allows an end user to demonstrate the exact transformation desired by actually editing source model and demonstrating the changes that evolve to target model an inference engine built into the underlying modeling tool records all editing operations and infers transformation pattern which can be reused in other models the paper motivates the need for the approach and discusses the technical contributions of mtbd case study with several sample inferred transformations serves as concrete example of the benefits of mtbd
spectral clustering refers to flexible class of clustering procedures that can produce high quality clusterings on small data sets but which has limited applicability to large scale problems due to its computational complexity of in general with the number of data points we extend the range of spectral clustering by developing general framework for fast approximate spectral clustering in which distortion minimizing local transformation is first applied to the data this framework is based on theoretical analysis that provides statistical characterization of the effect of local distortion on the mis clustering rate we develop two concrete instances of our general framework one based on local means clustering kasp and one based on random projection trees rasp extensive experiments show that these algorithms can achieve significant speedups with little degradation in clustering accuracy specifically our algorithms outperform means by large margin in terms of accuracy and run several times faster than approximate spectral clustering based on the nystrom method with comparable accuracy and significantly smaller memory footprint remarkably our algorithms make it possible for single machine to spectral cluster data sets with million observations within several minutes
many software systems suffer from missing support for behavioral runtime composition and configuration of software components the concern behavioral composition and configuration is not treated as first class entity but instead it is hard coded in different programming styles leading to tangled composition and configuration code that is hard to understand and maintain we propose to embed dynamic language with tailorable object and class concept into the host language in which the components are written and use the tailorable language for behavioral composition and configuration tasks using this approach we can separate the concerns behavioral composition and configuration from the rest of the software system leading to more reusable understandable and maintainable composition and configuration of software components
deep packet inspection dpi has been widely adopted in detecting network threats such as intrusion viruses and spam it is challenging however to achieve high speed dpi due to the expanding rule sets and ever increasing line rates key issue is that the size of the finite automata falls beyond the capacity of on chip memory thus incurring expensive off chip accesses in this paper we present dpico hardware based dpi engine that utilizes novel techniques to minimize the storage requirements for finite automata the techniques proposed are modified content addressable memory mcam interleaved memory banks and data packing the experiment results show the scalable performance of dpico can achieve up to gbps throughput using contemporary fpga chip experiment data also show that dpico based accelerator can improve the pattern matching performance of dpi server by up to times
an accurate tractable analytic cache model for time shared systems is presented which estimates the overall cache miss rate of multiprocessing system with any cache size and time quanta the input to the model consists of the isolated miss rate curves for each process the time quanta for each of the executing processes and the total cache size the output is the overall miss rate trace driven simulations demonstrate that the estimated miss rate is very accurate since the model provides fast and accurate way to estimate the effect of context switching it is useful for both understanding the effect of context switching on caches and optimizing cache performance for time shared systems cache partitioning mechanism is also presented and is shown to improve the cache miss rate up to over the normal lru replacement policy
in this paper we investigate technique for fusing approximate knowledge obtained from distributed heterogeneous information sources this issue is substantial eg in modeling multiagent systems where group of loosely coupled heterogeneous agents cooperate in achieving common goal information exchange leading ultimately to knowledge fusion is natural and vital ingredient of this process we use generalization of rough sets and relations which depends on allowing arbitrary similarity relations the starting point of this research is where framework for knowledge fusion in multiagent systems is introduced agents individual perceptual capabilities are represented by similarity relations further aggregated to express joint capabilities of teams this aggregation expressing shift from individual to social level of agents activity has been formalized by means of dynamic logic the approach of doherty et al uses the full propositional dynamic logic which does not guarantee tractability of reasoning our idea is to adapt the techniques of nguyen to provide an engine for tractable approximate database querying restricted to horn fragment of serial dynamic logic we also show that the obtained formalism is quite powerful in applications
the problem of modeling memory locality of applications to guide compiler optimizations in systematic manner is an important unsolved problem made even more significant with the advent of multi core and many core architectures we describe an approach based on novel source level metric called static reuse distance to model the memory behavior of applications written in matlab we use matlab as representative language that lets end users express their algorithms precisely but at relatively high level matlab's high level characteristics allow the static analysis to focus on large objects such as arrays without losing accuracy due to processor specific layout of scalar values in memory we present an efficient algorithm to compute static reuse distances using an extended version of dependence graphs our approach differs from earlier similar attempts in three important aspects it targets high level programming systems characterized by heavy use of libraries it works on full programs instead of being confined to loops and it integrates practical mechanisms to handle separately compiled procedures as well as pre compiled library procedures that are only available in binary form we study matlab code taken from real programs to demonstrate the effectiveness of our model finally we present some applications of our approach to program transformations that are known to be important in matlab but are expected to be relevant to other similar high level languages as well
the problem of frequent item discovery in streaming data has attracted lot of attention lately while the above problem has been studied extensively and several techniques have been proposed for its solution these approaches treat all the values of the data stream equally nevertheless not all values are of equal importance in several situations we are interested more in the new values that have appeared in the stream rather than in the older onesin this paper we address the problem of finding recent frequent items in data stream given small bounded memory and present novel algorithms to this direction we propose basic algorithm that extends the functionality of existing approaches by monitoring item frequencies in recent windows subsequently we present an improved version of the algorithm with significantly improved performance in terms of accuracy at no extra memory cost finally we perform an extensive experimental evaluation and show that the proposed algorithms can efficiently identify the frequent items in ad hoc recent windows of data stream
the past two decades have presented significant technological developments of mobile information and communication technology ict such as portable technologies eg mobile phones notebook computers personal digital assistants and associated wireless infrastructures eg wireless local area networks mobile telecommunications infrastructures bluetooth personal area networks mobile ict offers range of technical opportunities for organisations and their members to implement enterprise mobility however the challenges of unlocking the opportunities of enterprise mobility are not well understood one of the key issues is to establish systems and associated working practices that are deemed usable by both individuals and the organisation the aim of this paper is to show that the concept of organisational usability can enrich the understanding of mobile ict in organisations as an addition to the traditional understanding of individual usability organisational usability emphasises the role of mobile ict beyond individual support large scale study of four different ways of organising foreign exchange trading in middle eastern bank serves as the concrete foundation for the discussion the empirical study showed how the final of the four attempts at establishing trading deployed mobile ict to enable mobile trading and by providing solution which was deemed usable for both the organisation and the traders the paper contributes to the understanding of how usability of mobile ict critically depends on carefully balancing individual and organisational requirements it also demonstrates the need for research in enterprise mobility to embrace both individual and organisational concerns in order to grasp the complexity of the phenomena
high performance clusters have been growing rapidly in scale most of these clusters deploy high speed interconnect such as infini band to achieve higher performance most scientific applications executing on these clusters use the message passing interface mpi as the parallel programming model thus the mpi library has key role in achieving application performance by consuming as few resources as possible and enabling scalable performance state of the art mpi implementations over infiniband primarily use the reliable connection rc transport due to its good performance and attractive features however the rc transport requires connection between every pair of communicating processes with each requiring several kb of memory as clusters continue to scale memory requirements in rc based implementations increase the connection less unreliable datagram ud transport is an attractive alternative which eliminates the need to dedicate memory for each pair of processes in this paper we present high performance ud based mpi design we implement our design and compare the performance and resource usage with the rc based mvapich we evaluate npb smg sweepd and sppm up to processes on an core infiniband cluster for smg our prototype shows speedup and seven fold reduction in memory for processes additionally based on our model our design has an estimated times reduction in memory over mvapich at processes when all connections are created to the best of our knowledge this is the first research work that presents high performance mpi design over infiniband that is completely based on ud and can achieve near identical or better application performance than rc
outlier detection has been used for centuries to detect and where appropriate remove anomalous observations from data outliers arise due to mechanical faults changes in system behaviour fraudulent behaviour human error instrument error or simply through natural deviations in populations their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences it can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing the original outlier detection methods were arbitrary but now principled and systematic techniques are used drawn from the full gamut of computer science and statistics in this paper we introduce survey of contemporary techniques for outlier detection we identify their respective motivations and distinguish their advantages and disadvantages in comparative review
this paper presents novel image feature representation method called multi texton histogram mth for image retrieval mth integrates the advantages of co occurrence matrix and histogram by representing the attribute of co occurrence matrix using histogram it can be considered as generalized visual attribute descriptor but without any image segmentation or model training the proposed mth method is based on julesz's textons theory and it works directly on natural images as shape descriptor meanwhile it can be used as color texture descriptor and leads to good performance the proposed mth method is extensively tested on the corel dataset with natural images the results demonstrate that it is much more efficient than representative image feature descriptors such as the edge orientation auto correlogram and the texton co occurrence matrix it has good discrimination power of color texture and shape features
it is widely believed that distributed software development is riskier and more challenging than collocated development prior literature on distributed development in software engineering and other fields discuss various challenges including cultural barriers expertise transfer difficulties and communication and coordination overhead we evaluate this conventional belief by examining the overall development of windows vista and comparing the post release failures of components that were developed in distributed fashion with those that were developed by collocated teams we found negligible difference in failures this difference becomes even less significant when controlling for the number of developers working on binary we also examine component characteristics such as code churn complexity dependency information and test code coverage and find very little difference between distributed and collocated components to investigate if less complex components are more distributed further we examine the software process and phenomena that occurred during the vista development cycle and present ways in which the development process utilized may be insensitive to geography by mitigating the difficulties introduced in prior work in this area
seed based framework for textual information extraction allows for weakly supervised extraction of named entities from anonymized web search queries the extraction is guided by small set of seed named entities without any need for handcrafted extraction patterns or domain specific knowledge allowing for the acquisition of named entities pertaining to various classes of interest to web search users inherently noisy search queries are shown to be highly valuable albeit little explored resource for web based named entity discovery
making the structure of software visible during system development helps build shared understanding of the context for each piece of work ii identify progress with implementation and iii highlight any conflict between individual development activities finding an adequate representation for such information is not straightforward especially for large applications this paper describes an implementation of such visualization system designed to explore some of the issues involved the approach is based on war room command console metaphor and uses bank of eight linked consoles to present information the tool was applied to several industrial software systems written in mixture of java and one of which was over million lines of code in size
this paper is concerned with the parallel evaluation of datalog rule programs mainly by processors that are interconnected by communication network we introduce paradigm called data reduction for the parallel evaluation of general datalog program several parallelization strategies discussed previously in cw gst ws are special cases of this paradigm the paradigm parallelizes the evaluation by partitioning among the processors the instantiations of the rules after presenting the paradigm we discuss the following issues that we see fundamental for parallelization strategies derived from the paradigm properties of the strategies that enable reduction in the communication overhead decomposability load balancing and application to programs with negation we prove that decomposability concept introduced previously in ws cw is undecidable
dynamic load balancing is key factor in achieving high performance for large scale distributed simulations on grid infrastructures in grid environment the available resources and the simulation's computation and communication behavior may experience run time critical imbalances consequently an initial static partitioning should be combined with dynamic load balancing scheme to ensure the high performance of the distributed simulation many improved or novel dynamic load balancing designs have been proposed in recent years which aim to improve the distributed simulation performance such designs are in general non formalized and the realizations of the designs are highly time consuming and error prone practices in this paper we propose formal dynamic load balancing design approach using discrete event system specification devs we discuss the feasibility of using devs and as an additional step we consider studying recently proposed design through formalized devs model system our focus is how devs component based formalized design approach can predict some of the key design factors before the design is realized or can further validate and consolidate realized dynamic load balancing designs
the glasgow haskell compiler ghc has quite sophisticated support for concurrency in its runtime system which is written in low level code as ghc evolves the runtime system becomes increasingly complex error prone difficult to maintain and difficult to add new concurrency features this paper presents an alternative approach to implement concurrency in ghc rather than hard wiring all kinds of concurrency features the runtime system is thin substrate providing only small set of concurrency primitives and the remaining concurrency features are implemented in software libraries written in haskell this design improves the safety of concurrency support it also provides more customizability of concurrency features which can be developed as haskell library packages and deployed modularly
for large scale and residual software like network service reliability is critical requirement recent research has shown that most of network software still contains number of bugs methods for automated detection of bugs in software can be classified into static analysis based on formal verification and runtime checking based on fault injection in this paper framework for checking software security vulnerability is proposed the framework is based on automated bug detection technologies ie static analysis and fault injection which are complementary each other the proposed framework provides new direction in which various kinds of software can be checked its vulnerability by making use of static analysis and fault injection technology in experiment on proposed framework we find unknown vulnerability as well as known vulnerability in windows network module
we present the design implementation and evaluation of promise novel peer to peer media streaming system encompassing the key functions of peer lookup peer based aggregated streaming and dynamic adaptations to network and peer conditions particularly promise is based on new application level pp service called collectcast collectcast performs three main functions inferring and leveraging the underlying network topology and performance information for the selection of senders monitoring the status of peers and connections and reacting to peer connection failure or degradation with low overhead dynamically switching active senders and standby senders so that the collective network performance out of the active senders remains satisfactory based on both real world measurement and simulation we evaluate the performance of promise and discuss lessons learned from our experience with respect to the practicality and further optimization of promise
several mesh like coarse grained reconfigurable architectures have been devised in the last few years accompanied with their corresponding mapping flows one of the major bottlenecks in mapping algorithms on these architectures is the limited memory access bandwidth only few mapping methodologies encountered the problem of the limited bandwidth while none has explored how the performance improvements are affected from the architectural characteristics we study in this paper the impact that the architectural parameters have on performance speedups achieved when the pes local rams are used for storing the variables with data reuse opportunities the data reuse values are transferred in the internal interconnection network instead of being fetched from external memories in order to reduce the data transfer burden on the bus network novel mapping algorithm is also proposed that uses list scheduling technique the experimental results quantified the trade offs that exist between the performance improvements and the memory access latency the interconnection network and the processing element's local ram size for this reason our mapping methodology targets on flexible architecture template which permits such an exploration more specifically the experiments showed that the improvements increase with the memory access latency while richer interconnection topology can improve the operation parallelism by factor of on average finally for the considered set of benchmarks the operation parallelism has been improved from to from the application of our methodology and by having each pe's local ram size of words
operational transformation ot is technique originally invented for supporting consistency maintenance in collaborative text editors word processors have much richer data types and more comprehensive operations than plain text editors among others the capability of updating attributes of any types of object is an essential feature of all word processors in this paper we report an extension of ot for supporting generic update operation in addition to insert and delete operations for collaborative word processing we focus on technical issues and solutions involved in transforming updates for both consistency maintenance and group undo novel technique called multi version single display mvsd has been devised to resolve conflict between concurrent updates and integrated into the framework of ot this work has been motivated by and conducted in the coword project which aims to convert ms word into real time collaborative word processor without changing its source code this ot extension is relevant not only to word processors but also to range of interactive applications that can be modelled as editors
we present fully automatic method for content selection evaluation in summarization that does not require the creation of human model summaries our work capitalizes on the assumption that the distribution of words in the input and an informative summary of that input should be similar to each other results on large scale evaluation from the text analysis conference show that input summary comparisons are very effective for the evaluation of content selection our automatic methods rank participating systems similarly to manual model based pyramid evaluation and to manual human judgments of responsiveness the best feature jensen shannon divergence leads to correlation as high as with manual pyramid and with responsiveness evaluations
software comprehension understanding software structure and behavior is essential for developing maintaining and improving software this is particularly true of agent based systems in which the actions of autonomous agents are affected by numerous factors such as events in dynamic environment local uncertain beliefs and intentions of other agents existing comprehension tools are not suited to such large concurrent software and do not leverage concepts of the agent oriented paradigm to aid the user in understanding the software's behavior to address the software comprehension of agent based systems this research proposes method and accompanying tool that automates some of the manual tasks performed by the human user during software comprehension such as explanation generation and knowledge verification
this paper focuses on the realizability problem of framework for modeling and specifying the global behavior of reactive electronic services services in this framework web accessible programs peers communicate by asynchronous message passing and virtual global watcher listens silently to the network the global behavior is characterized by conversation which is the infinite sequence of messages observed by the watcher we show that given büchi automaton specifying the desired set of conversations called conversation protocol it is possible to implement it using set of finite state peers if three realizability conditions are satisfied in particular the synthesized peers will conform to the protocol by generating only those conversations specified by the protocol our results enable top down verification strategy where conversation protocol is specified by realizable büchi automaton the properties of the protocol are verified on the büchi automaton specification the peer implementations are synthesized from the protocol via projection
traditional duplicate elimination techniques are not applicable to many data stream applications in general precisely eliminating duplicates in an unbounded data stream is not feasible in many streaming scenarios therefore we target at approximately eliminating duplicates in streaming environments given limited space based on well known bitmap sketch we introduce data structure stable bloom filter and novel and simple algorithm the basic idea is as follows since there is no way to store the whole history of the stream sbf continuously evicts the stale information so that sbf has room for those more recent elements after finding some properties of sbf analytically we show that tight upper bound of false positive rates is guaranteed in our empirical study we compare sbf to alternative methods the results show that our method is superior in terms of both accuracy and time effciency when fixed small space and an acceptable false positive rate are given
modern techniques for distributed information retrieval use set of documents sampled from each server but these samples have been underutilised in server selection we describe new server selection algorithm sushi which unlike earlier algorithms can make full use of the text of each sampled document and which does not need training data sushi can directly optimise for many common cases including high precision retrieval and by including simple stopping condition can do so while reducing network traffic our experiments compare sushi with alternatives and show it achieves the same effectiveness as the best current methods while being substantially more efficient selecting as few as as many servers
design and control of vector fields is critical for many visualization and graphics tasks such as vector field visualization fluid simulation and texture synthesis the fundamental qualitative structures associated with vector fields are fixed points periodic orbits and separatrices in this paper we provide new technique that allows for the systematic creation and cancellation of fixed points and periodic orbits this technique enables vector field design and editing on the plane and surfaces with desired qualitative properties the technique is based on conley theory which provides unified framework that supports the cancellation of fixed points and periodic orbits we also introduce novel periodic orbit extraction and visualization algorithm that detects for the first time periodic orbits on surfaces furthermore we describe the application of our periodic orbit detection and vector field simplification algorithms to engine simulation data demonstrating the utility of the approach we apply our design system to vector field visualization by creating data sets containing periodic orbits this helps us understand the effectiveness of existing visualization techniques finally we propose new streamline based technique that allows vector field topology to be easily identified
sensor networks are very specific type of wireless networks where both security and performance issues need to be solved efficiently in order to avoid manipulations of the sensed data and at the same time minimize the battery energy consumption this paper proposes an efficient way to perform data collection by grouping the sensors in aggregation zones allowing the aggregators to process the sensed data inside the aggregation zone in order to minimize the amount of transmissions to the sink moreover the paper provides security mechanism based on hash chains to secure data transmissions in networks with low capability sensors and without the requirements of an instantaneous source authentication
grid resources are non dedicated and thus grid users are forced to compete with resource owners for idle cpu cycles as result the turnaround times of both the grid jobs and the owners jobs are invariably delayed to resolve this problem the current study proposes progressive multi layer resource reconfiguration framework designated as pmr in which intra and inter site reconfiguration strategies are employed to adapt grid users jobs dynamically to changes in the available cpu resources at each node the experimental results show that pmr enables the idle cpu cycles of resource to be fully exploited by grid users with minimum interference to the resource owner's jobs
there exist emerging applications of data streams that require association rule mining such as network traffic monitoring and web click streams analysis different from data in traditional static databases data streams typically arrive continuously in high speed with huge amount and changing data distribution this raises new issues that need to be considered when developing association rule mining techniques for stream data this paper discusses those issues and how they are addressed in the existing literature
classification is an important problem in data mining given an example and class classifier usually works by estimating the probability of being member of ie membership probability well calibrated classifiers are those able to provide accurate estimates of class membership probabilities that is the estimated probability is close to which is the true empirical probability of being member of given that the probability estimated by the classifier is calibration is not necessary property for producing accurate classifiers and thus most of the research has focused on direct accuracy maximization strategies ie maximum margin rather than on calibration however non calibrated classifiers are problematic in applications where the reliability associated with prediction must be taken into account ie cost sensitive classification cautious classification etc in these applications sensible use of the classifier must be based on the reliability of its predictions and thus the classifier must be well calibrated in this paper we show that lazy associative classifiers lac are accurate and well calibrated using well known sound entropy minimization method we explore important applications where such characteristics ie accuracy and calibration are relevant and we demonstrate empirically that lac drastically outperforms other classifiers such as svms naive bayes and decision trees even after these classifiers are calibrated by specific methods additional highlights of lac include the ability to incorporate reliable predictions for improving training and the ability to refrain from doubtful predictions
in recent years wireless sensor networking has shown great promise in applications ranging from industrial control environmental monitoring and inventory tracking given the resource constrained nature of sensor devices and the dynamic wireless channel used for communication sensor networking protocol needs to be compact energy efficient and highly adaptable in this paper we present sampl simple aggregation and message passing layer aimed at flexible aggregation of sensor information over long period of time and supporting sporadic messages from mobile devices sampl is compact network layer that operates on top of low power csma ca based mac protocol the protocol has been designed with extensibility in mind to support new transducer devices and unforeseen applications without requiring reprogramming of the entire network sampl uses highly adaptive tree based routing scheme to achieve highly robust operation in time varying environment the protocol supports peer to peer data transactions local storage of data similar to what many rfid systems provide as well as secure gateway to infrastructure communication sampl is built on top of the nano rk operating system that runs on the firefly sensor networking platform nano rk's resource management primitives are used to create virtual energy budgets within sampl that enforce application lifetimes as of october sampl has been operating as part of the sensor andrew project at carnegie mellon university with battery powered sensor nodes for over seven months and continues to be actively used as research testbed we describe our deployment tools and network health monitoring strategies necessary for configuring and maintaining long term operation of sensor network our approach has led to sustainable average packet success rate of across the entire network
to execute mpi applications reliably fault tolerance mechanisms are needed message logging is well known solution to provide fault tolerance for mpi applications it as been proved that it can tolerate higher failure rate than coordinated checkpointing however pessimistic and causal message logging can induce high overhead on failure free execution in this paper we present op new optimistic message logging protocol based on active optimistic message logging contrary to existing optimistic message logging protocols that saves dependency information on reliable storage periodically op logs dependency information as soon as possible to reduce the amount of data piggybacked on application messages thus it reduces the overhead of the protocol on failure free execution making it more scalable and simplifying recovery op is implemented as module of the open mpi library experiments show that active message logging is promising to improve scalability and performance of optimistic message logging
we introduce sensordcsp naturally distributed benchmark based on real world application that arises in the context of networked distributed systems in order to study the performance of distributed csp discsp algorithms in truly distributed setting we use discrete event network simulator which allows us to model the impact of different network traffic conditions on the performance of the algorithms we consider two complete discsp algorithms asynchronous backtracking abt and asynchronous weak commitment search awc and perform performance comparison for these algorithms on both satisfiable and unsatisfiable instances of sensordcsp we found that random delays due to network traffic or in some cases actively introduced by the agents combined with dynamic decentralized restart strategy can improve the performance of discsp algorithms in addition we introduce gsensordcsp plain embedded version of sensordcsp that is closely related to various real life dynamic tracking systems we perform both analytical and empirical study of this benchmark domain in particular this benchmark allows us to study the attractiveness of solution repairing for solving sequence of discsps that represent the dynamic tracking of set of moving objects
we analyze theoretically the subspace best approximating images of convex lambertian object taken from the same viewpoint but under different distant illumination conditions since the lighting is an arbitrary function the space of all possible images is formally infinite dimensional however previous empirical work has shown that images of largely diffuse objects actually lie very close to five dimensional subspace in this paper we analytically construct the principal component analysis for images of convex lambertian object explicitly taking attached shadows into account and find the principal eigenmodes and eigenvalues with respect to lighting variability our analysis makes use of an analytic formula for the irradiance in terms of spherical harmonic coefficients of the illumination and shows under appropriate assumptions that the principal components or eigenvectors are identical to the spherical harmonic basis functions evaluated at the surface normal vectors our main contribution is in extending these results to the single viewpoint case showing how the principal eigenmodes and eigenvalues are affected when only limited subset the upper hemisphere of normals is available and the spherical harmonics are no longer orthonormal over the restricted domain our results are very close both qualitatively and quantitatively to previous empirical observations and represent the first essentially complete theoretical explanation of these observations our analysis is also likely to be of interest in other areas of computer vision and image based rendering in particular our results indicate that using complex illumination for photometric problems in computer vision is not significantly more difficult than using directional sources
various programming languages allow the construction of structure shy programs such programs are defined generically for many different datatypes and only specify specific behavior for few relevant subtypes typical examples are xml query languages that allow selection of subdocuments without exhaustively specifying intermediate element tags other examples are languages and libraries for polytypic or strategic functional programming and for adaptive object oriented programming in this paper we present an algebraic approach to transformation of declarative structure shy programs in particular for strategic functions and xml queries we formulate rich set of algebraic laws not just for transformation of structure shy programs but also for their conversion into structure sensitive programs and vice versa we show how subsets of these laws can be used to construct effective rewrite systems for specialization generalization and optimization of structure shy programs we present type safe encoding of these rewrite systems in haskell which itself uses strategic functional programming techniques
programs which manipulate pointers are hard to debug pointer analysis algorithms originally aimed at optimizing compilers may provide some remedy by identifying potential errors such as dereferencing null pointers by statically analyzing the behavior of programs on all their input dataour goal is to identify the core program analysis techniques that can be used when developing realistic tools which detect memory errors at compile time without generating too many false alarms our preliminary experience indicates that the following techniques are necessary finding aliases between pointers ii flow sensitive techniques that account for the program control flow constructs iii partial interpretation of conditional statements iv analysis of the relationships between pointers and sometimes analysis of the underlying data structures manipulated by the programwe show that combination of these techniques can yield better results than those achieved by state of the art tools yet it is not clear to us whether our ideas are applicable to large programs
in this paper we introduce new approach for the embedding of linear elastic deformable models our technique results in significant improvements in the efficient physically based simulation of highly detailed objects first our embedding takes into account topological details that is disconnected parts that fall into the same coarse element are simulated independently second we account for the varying material properties by computing stiffness and interpolation functions for coarse elements which accurately approximate the behaviour of the embedded material finally we also take into account empty space in the coarse embeddings which provides better simulation of the boundary the result is straightforward approach to simulating complex deformable models with the ease and speed associated with coarse regular embedding and with quality of detail that would only be possible at much finer resolution
recently the increasing use of time series data has initiated various research and development attempts in the field of data and knowledge management time series data is characterized as large in data size high dimensionality and update continuously moreover the time series data is always considered as whole instead of individual numerical fields indeed large set of time series data is from stock market stock time series has its own characteristics over other time series moreover dimensionality reduction is an essential step before many time series analysis and mining tasks for these reasons research is prompted to augment existing technologies and build new representation to manage financial time series data in this paper financial time series is represented according to the importance of the data points with the concept of data point importance tree data structure which supports incremental updating is proposed to represent the time series and an access method for retrieving the time series data point from the tree which is according to their order of importance is introduced this technique is capable to present the time series in different levels of detail and facilitate multi resolution dimensionality reduction of the time series data in this paper different data point importance evaluation methods new updating method and two dimensionality reduction approaches are proposed and evaluated by series of experiments finally the application of the proposed representation on mobile environment is demonstrated
through analysis and experiments this paper investigates two phase waiting algorithms to minimize the cost of waiting for synchronization in large scale multiprocessors in two phase algorithm thread first waits by polling synchronization variable if the cost of polling reaches limit lpoll and further waiting is necessary the thread is blocked incurring an additional fixed cost the choice of lpoll is critical determinant of the performance of two phase algorithms we focus on methods for statically determining lpoll because the run time overhead of dynamically determining lpoll can be comparable to the cost of blocking in large scale multiprocessor systems with lightweight threads our experiments show that always block lpoll is good waiting algorithm with performance that is usually close to the best of the algorithms compared we show that even better performance can be achieved with static choice of lpoll based on knowledge of likely wait time distributions motivated by the observation that different synchronization types exhibit different wait time distributions we prove that static choice of lpoll can yield close to optimal on line performance against an adversary that is restricted to choosing wait times from fixed family of probability distributions this result allows us to make an optimal static choice of lpoll based on synchronization type for exponentially distributed wait times we prove that setting lpoll results in waiting cost that is no more than times the cost of an optimal off line algorithm for uniformly distributed wait times we prove that setting lpoll square root of results in waiting cost that is no more than square root of the golden ratio times the cost of an optimal off line algorithm experimental measurements of several parallel applications on the alewife multiprocessor simulator corroborate our theoretical findings
code sandboxing is useful for many purposes but most sandboxing techniques require kernel modifications do not completely isolate guest code or incur substantial performance costs vx is multipurpose user level sandbox that enables any application to load and safely execute one or more guest plug ins confining each guest to system call api controlled by the host application and to restricted memory region within the host's address space vx runs guest code efficiently on several widespread operating systems without kernel extensions or special privileges it protects the host program from both reads and writes by its guests and it allows the host to restrict the instruction set available to guests the key to vx combination of portability flexibility and efficiency is its use of segmentation hardware to sandbox the guest's data accesses along with lightweight instruction translator to sandbox guest instructions we evaluate vx using microbenchmarks and whole system benchmarks and we examine four applications based on vx an archival storage system an extensible public key infrastructure an experimental user level operating system running atop another host os and linux system call jail the first three applications export custom apis independent of the host os to their guests making their plug ins binary portable across host systems compute intensive workloads for the first two applications exhibit between slowdown and speedup on vx relative to native execution speedups result from vx instruction translator improving the cache locality of guest code the experimental user level operating system allows the use of the guest os's applications alongside the host's native applications and runs faster than whole system virtual machine monitors such as vmware and qemu the linux system call jail incurs up to overhead but requires no kernel modifications and is delegation based avoiding concurrency vulnerabilities present in other interposition mechanisms
little is known about the strategies end user programmers use in debugging their programs and even less is known about gender differences that may exist in these strategies without this type of information designers of end user programming systems cannot know the target at which to aim if they are to support male and female end user programmers we present study investigating this issue we asked end user programmers to debug spreadsheets and to describe their debugging strategies using mixed methods we analyzed their strategies and looked for relationships among participants strategy choices gender and debugging success our results indicate that males and females debug in quite different ways that opportunities for improving support for end user debugging strategies for both genders are abundant and that tools currently available to end user debuggers may be especially deficient in supporting debugging strategies used by females
visual complexity is an apparent feature in website design yet its effects on cognitive and emotional processing are not well understood the current study examined website complexity within the framework of aesthetic theory and psychophysiological research on cognition and emotion we hypothesized that increasing the complexity of websites would have detrimental cognitive and emotional impact on users in passive viewing task pvt website screenshots differing in their degree of complexity operationalized by jpeg file size correlation with complexity ratings in preliminary study were presented to participants in randomized order additionally standardized visual search task vst assessing reaction times and one week delayed recognition task on these websites were conducted and participants rated all websites for arousal and valence psychophysiological responses were assessed during the pvt and vst visual complexity was related to increased experienced arousal more negative valence appraisal decreased heart rate and increased facial muscle tension musculus corrugator visual complexity resulted in increased reaction times in the vst and decreased recognition rates reaction times in the vst were related to increases in heart rate and electrodermal activity these findings demonstrate that visual complexity of websites has multiple effects on human cognition and emotion including experienced pleasure and arousal facial expression autonomic nervous system activation task performance and memory it should thus be considered an important factor in website design
the vast expansion of interconnectivity with the internet and the rapid evolution of highly capable but largely insecure mobile devices threatens cellular networks in this paper we characterize the impact of the large scale compromise and coordination of mobile phones in attacks against the core of these networks through combination of measurement simulation and analysis we demonstrate the ability of botnet composed of as few as compromised mobile phones to degrade service to area code sized regions by as such attacks are accomplished through the execution of network service requests and not constant stream of phone calls users are unlikely to be aware of their occurrence we then investigate number of significant network bottlenecks their impact on the density of compromised nodes per base station and how they can be avoided we conclude by discussing number of countermeasures that may help to partially mitigate the threats posed by such attacks
many enterprise applications require the use of object oriented middleware and message oriented middleware in combination middleware mediated transacdons have been proposed as transaction model to address reliability of such applications they extend distributed object transactions to include message oriented transactions in this paper we present three message queuing patterns that we have found useful for implementing middleware mediated transactions we discuss and show how the patterns can be applied to support guaranteed compensation in the engineering of transactional enterprise applications
join is fundamental operator in data stream management system dsms it is more efficient to share execution of multiple windowed joins than separate execution of everyone because the former saves part of cost in common windows therefore shared window join is adopted widely in multi queries dsms when all tasks of queries exceed maximum system capacity the overloaded dsms fails to process all of its input data and keep up with the rates of data arrival especially in time critical environment queries should be completed not just timely but within certain deadlines in this paper we address load shedding approach for shared window join over real time data streams load shedding algorithm ls sjrt cw is proposed to handle queries shared window join in overloaded real time system effectively it would reduce load shedding overhead by adjusting sliding window size experiment results show that our algorithm would decrease average deadline miss ratio over some ranges of workloads
as interactive multimedia communications are developing rapidly on the internet they present stringent challenges on end to end ee performance on the other hand however the internet's architecture ipv remains almost the same as it was originally designed for only data transmission purpose and has experienced big hurdle to actualize qos universally this paper designs cooperatively overlay routing service cors aiming to overcome the performance limit inherent in the internet's ip layer routing service the key idea of cors is to efficiently compose number of eligible application layer paths with suitable relays in the overlay network besides the direct ip path cors can transfer data simultaneously through one or more application layer paths to adaptively satisfy the data's application specific requirements on ee performance simulation results indicate the proposed schemes are scalable and effective practical experiments based on prototype implemented on planetlab show that cors is feasible to enhance the transmission reliability and the quality of multimedia communications
sensors have been increasingly used for many ubiquitous computing applications such as asset location monitoring visual surveillance and human motion tracking in such applications it is important to place sensors such that every point of the target area can be sensed by more than one sensor especially many practical applications require coverage for triangulation hull building and etc also in order to extract meaningful information from the data sensed by multiple sensors those sensors need to be placed not too close to each other minimum separation requirement to address the coverage problem with the minimum separation requirement our recent work kim et al proposes two heuristic methods so called overlaying method and tre based method which complement each other depending on the minimum separation requirement for these two methods we also provide mathematical analysis that can clearly guide us when to use the tre based method and when to use the overlaying method and also how many sensors are required to make it self contained in this paper we first revisit the two heuristic methods then as an extension we present an ilp based optimal solution targeting for grid coverage with this ilp based optimal solution we investigate how much close the two heuristic methods are to the optimal solution finally this paper discusses the impacts of the proposed methods on real deployed systems using two example sensor systems to the best of our knowledge this is the first work that systematically addresses the coverage problem with the minimum separation requirement
mobile location aware applications have become quite popular across range of new areas such as pervasive games and mobile edutainment applications however it is only recently that approaches have been presented which combine gaming and education with mobile augmented reality systems however they typically lack close crossmedia integration of the surroundings and often annotate or extend the environment rather than modifying and altering it in this paper we present mobile outdoor mixed reality game for exploring the history of city in the spatial and the temporal dimension we introduce the design and concept of the game and present universal mechanism to define and setup multi modal user interfaces for the game challenges finally we discuss the results of the user tests
surrogate is an object that stands for document and enables navigation to that document hypermedia is often represented with textual surrogates even though studies have shown that image and text surrogates facilitate the formation of mental models and overall understanding surrogates may be formed by breaking document down into set of smaller elements each of which is surrogate candidate while processing these surrogate candidates from an html document relevant information may appear together with less useful junk material such as navigation bars and advertisements this paper develops pattern recognition based approach for eliminating junk while building the set of surrogate candidates the approach defines features on candidate elements and uses classification algorithms to make selection decisions based on these features for the purpose of defining features in surrogate candidates we introduce the document surrogate model dsm streamlined document object model dom like representation of semantic structure using quadratic classifier we were able to eliminate junk surrogate candidates with an average classification rate of by using this technique semiautonomous agents can be developed to more effectively generate surrogate collections for users we end by describing new approach for hypermedia and the semantic web which uses the dsm to define value added surrogates for document
we consider generic garbled circuit gc based techniques for secure function evaluation sfe in the semi honest modelwe describe efficient gc constructions for addition subtraction multiplication and comparison functions our circuits for subtraction and comparison are approximately two times smaller in terms of garbled tables than previous constructions this implies corresponding computation and communication improvements in sfe of functions using our efficient building blocks the techniques rely on recently proposed free xor gc techniquefurther we present concrete and detailed improved gc protocols for the problem of secure integer comparison and related problems of auctions minimum selection and minimal distance performance improvement comes both from building on our efficient basic blocks and several problem specific gc optimizations we provide precise cost evaluation of our constructions which serves as baseline for future protocols
the restricted correspondence problem is the task of solving the classical stereo correspondence problem when the surface being observed is known to belong to family of surfaces that vary in known way with one or more parameters under this constraint the surface can be extracted far more robustly than by classical stereo applied to an arbitrary surface since the problem is solved semi globally rather than locally for each epipolar line here the restricted correspondence problem is solved for two examples the first being the extraction of the parameters of an ellipsoid from calibrated stereo pair the second example is the estimation of the osculating paraboloid at the frontier points of convex object
recently research on text mining has attracted lots of attention from both industrial and academic fields text mining concerns of discovering unknown patterns or knowledge from large text repository the problem is not easy to tackle due to the semi structured or even unstructured nature of those texts under consideration many approaches have been devised for mining various kinds of knowledge from texts one important aspect of text mining is on automatic text categorization which assigns text document to some predefined category if the document falls into the theme of the category traditionally the categories are arranged in hierarchical manner to achieve effective searching and indexing as well as easy comprehension for human beings the determination of category themes and their hierarchical structures were most done by human experts in this work we developed an approach to automatically generate category themes and reveal the hierarchical structure among them we also used the generated structure to categorize text documents the document collection was trained by self organizing map to form two feature maps these maps were then analyzed to obtain the category themes and their structure although the test corpus contains documents written in chinese the proposed approach can be applied to documents written in any language and such documents can be transformed into list of separated terms
unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints ie pairs of instances labeled as belonging to same or different clusters in recent years number of algorithms have been proposed for enhancing clustering quality by employing such supervision such methods use the constraints to either modify the objective function or to learn the distance measure we propose probabilistic model for semi supervised clustering based on hidden markov random fields hmrfs that provides principled framework for incorporating supervision into prototype based clustering the model generalizes previous approach that combines constraints and euclidean distance learning and allows the use of broad range of clustering distortion measures including bregman divergences eg euclidean distance and divergence and directional similarity measures eg cosine similarity we present an algorithm that performs partitional semi supervised clustering of data by minimizing an objective function derived from the posterior energy of the hmrf model experimental results on several text data sets demonstrate the advantages of the proposed framework
evaluative texts on the web have become valuable source of opinions on products services events individuals etc recently many researchers have studied such opinion sources as product reviews forum posts and blogs however existing research has been focused on classification and summarization of opinions using natural language processing and data mining techniques an important issue that has been neglected so far is opinion spam or trustworthiness of online opinions in this paper we study this issue in the context of product reviews which are opinion rich and are widely used by consumers and product manufacturers in the past two years several startup companies also appeared which aggregate opinions from product reviews it is thus high time to study spam in reviews to the best of our knowledge there is still no published study on this topic although web spam and email spam have been investigated extensively we will see that opinion spam is quite different from web spam and email spam and thus requires different detection techniques based on the analysis of million reviews and million reviewers from amazoncom we show that opinion spam in reviews is widespread this paper analyzes such spam activities and presents some novel techniques to detect them
given string and language the hamming distance of to is the minimum hamming distance of to any string in the edit distance of string to language is analogously definedfirst we prove that there is language in ac such that both hamming and edit distance to this language are hard to approximate they cannot be approximated with factor for any unless np denotes the length of the input string second we show the parameterized intractability of computing the hamming distance we prove that for every there exists language in ac for which computing the hamming distance is hard moreover there is language in for which computing the hamming distance is wp hardthen we show that the problems of computing the hamming distance and of computing the edit distance are in some sense equivalent by presenting approximation ratio preserving reductions from the former to the latter and vice versafinally we define hamp to be the class of languages to which the hamming distance can efficiently ie in polynomial time be computed we show some properties of the class hamp on the other hand we give evidence that characterization in terms of automata or formal languages might be difficult
in this paper new color space called the rgb color ratio space is proposed and defined according to reference color such that an image can be transformed from conventional color space to the rgb color ratio space because color in the rgb color ratio space is represented as three color ratios and intensity the chrominance can be completely reserved three color ratios and the luminance can be de correlated with the chrominance different from traditional distance measurement road color model is determined by an ellipse area in the rgb ratio space enclosed by the estimated boundaries proposed adaptive fuzzy logic in which fuzzy membership functions are defined according to estimated boundaries is introduced to implement clustering rules therefore each pixel will have its own fuzzy membership function corresponding to its intensity basic neural network is trained and used to achieve parameters optimization the low computation cost of the proposed segmentation method shows the feasibility for real time application experimental results for road detection demonstrate the robustness to intensity variation of the proposed approach
we show the decidability of model checking pa processes against several first order logics based upon the reachability predicate the main tool for this result is the recognizability by tree automata of the reachability relation the tree automata approach and the transition logics we use allow smooth and general treatment of parameterized model checking for pa this approach is extended to handle quite general notion of costs of pa steps in particular when costs are parikh images of traces we show decidability of transition logic extended by some form of first order reasoning over costs
hierarchical agent framework is proposed to construct monitoring layer towards self aware parallel systems on chip socs with monitoring services as new design dimension systems are capable of observing and reconfiguring themselves dynamically at all levels of granularity based on application requirements and platform conditions agents with hierarchical priorities work adaptively and cooperatively to maintain and improve system performance in the presence of variations and faults function partitioning of agents and hierarchical monitoring operations on parallel socs are analyzed applying the design approach on the network on chip noc platform demonstrates the design process and benefits using the novel approach
proliferation of portable wireless enabled laptop computers and pdas cost effective deployment of access points and availability of the license exempt bands and appropriate networking standards contribute to the conspicuous success of ieee wlans in the article we provide comprehensive overview of techniques for capacity improvement and qos provisioning in the ieee protocol family these techniques represent the efforts both in the research community and the ieee working groups specifically we summarize the operations of ieee legacy as well as its extension introduce several protocol modeling techniques and categorize the various approaches to improve protocol capacity to provide qos by either devising new mac protocol components or fine tuning protocol parameters in ieee and to judiciously arbitrate radio resources eg transmission rate and power to demonstrate how to adapt qos provisioning in newly emerging areas we use the wireless mesh network as an example discuss the role ieee plays in such network and outline research issues that arise
digital audio video data have become an integral part of multimedia information systems to reduce storage and bandwidth requirements they are commonly stored in compressed format such as mpeg increasing amounts of mpeg encoded audio and video documents are available online and in proprietary collections in order to effectively utilise them we need tools and techniques to automatically analyse segment and classify mpeg video content several techniques have been developed both in the audio and visual domain to analyse videos this paper presents survey of audio and visual analysis techniques on mpeg encoded media that are useful in supporting variety of video applications although audio and visual feature analyses have been carried out extensively they become useful to applications only when they convey semantic meaning of the video content therefore we also present survey of works that provide semantic analysis on mpeg encoded videos
bottom sketch is summary of set of items with nonnegative weights each such summary allows us to compute approximate aggregates over the set of items bottom sketches are obtained by associating with each item in ground set an independent random rank drawn from probability distribution that depends on the weight of the item for each subset of interest the bottom sketch is the set of the minimum ranked items and their ranks bottom sketches have numerous applications we develop and analyze data structures and estimators for bottom sketches to facilitate their deployment we develop novel estimators and algorithms that show that they are superior alternative to other sketching methods in both efficiency of obtaining the sketches and the accuracy of the estimates derived from the sketches
novel touch based interaction method by use of orientation information of touch region is proposed to capture higher dimensional information of touch including position and an orientation as well we develop robust algorithms to detect contact shape and to estimate its orientation angle also we suggest practical guidelines to use our method through experiments considering various conditions and show possible service scenarios of aligning documents and controlling media player
this paper offers theoretical study of constraint simplification fundamental issue for the designer of practical type inference system with subtyping in the simpler case where constraints are equations simple isomorphism between constrained type schemes and finite state automata yields complete constraint simplification method using it as guide for the intuition we move on to the case of subtyping and describe several simplification algorithms although no longer complete they are conceptually simple efficient and very effective in practice overall this paper gives concise theoretical account of the techniques found at the core of our type inference system our study is restricted to the case where constraints are interpreted in non structural lattice of regular terms nevertheless we highlight small number of general ideas which explain our algorithms at high level and may be applicable to variety of other systems copyright academic press
personalization of learning has become prominent issue in the educational field at various levels this article elaborates different view on personalisation than what usually occurs in this area its baseline is that personalisation occurs when learning turns out to become personal in the learner's mind through literature survey we analyze constitutive dimensions of this inner sense of personalisation here we devote special attention to confronting learners with tracked information making their personal interaction footprints visible contrasts with the back office usage of this data by researchers instructors or adaptive systems we contribute prototype designed for the moodle platform according to the conceptual approach presented here
the fluid documents project has developed various research prototypes that show that powerful annotation techniques based on animated typographical changes can help readers utilize annotations more effectively our recently developed fluid open hypermedia prototype supports the authoring and browsing of fluid annotations on third party web pages this prototype is an extension of the arakne environment an open hypermedia application that can augment web pages with externally stored hypermedia structures this paper describes how various web standards including dom css xlink xpointer and rdf can be used and extended to support fluid annotations
wireless sensor networks have created new opportunities for data collection in variety of scenarios such as environmental and industrial where we expect data to be temporally and spatially correlated researchers may want to continuously collect all sensor data from the network for later analysis suppression both temporal and spatial provides opportunities for reducing the energy cost of sensor data collection we demonstrate how both types can be combined for maximal benefit we frame the problem as one of monitoring node and edge constraints monitored node triggers report if its value changes monitored edge triggers report if the difference between its nodes values changes the set of reports collected at the base station is used to derive all node values we fully exploit the potential of this global inference in our algorithm conch short for constraint chaining constraint chaining builds network of constraints that are maintained locally but allow global view of values to be maintained with minimal cost network failure complicates the use of suppression since either causes an absence of reports we add enhancements to conch to build in redundant constraints and provide method to interpret the resulting reports in case of uncertainty using simulation we experimentally evaluate conch's effectiveness against competing schemes in number of interesting scenarios
abstract researchers have recently discovered several interesting self organized regularities from the world wide web ranging from the structure and growth of the web to the access patterns in web surfing what remains to be great challenge in web log mining is how to explain user behavior underlying observed web usage regularities in this paper we will address the issue of how to characterize the strong regularities in web surfing in terms of user navigation strategies and present an information foraging agent based approach to describing user behavior by experimenting with the agent based decision models of web surfing we aim to explain how some web design factors as well as user cognitive factors may affect the overall behavioral patterns in web usage
most video retrieval systems are multimodal commonly relying on textual information low and high level semantic features extracted from query visual examples in this work we study the impact of exploiting different knowledge sources in order to automatically retrieve query visual examples relevant to video retrieval task our hypothesis is that the exploitation of external knowledge sources can help on the identification of query semantics as well as on improving the understanding of video contents we propose set of techniques to automatically obtain additional query visual examples from different external knowledge sources such as dbpedia flickr and google images which have different coverage and structure characteristics the proposed strategies attempt to exploit the semantics underlying the above knowledge sources to reduce the ambiguity of the query and to focus the scope of the image searches in the repositories we assess and compare the quality of the images obtained from the different external knowledge sources when used as input of number of video retrieval tasks we also study how much they complement manually provided sets of examples such as those given by trecvid tasks based on our experimental results we report which external knowledge source is more likely to be suitable for the evaluated retrieval tasks results also demonstrate that the use of external knowledge can be good complement to manually provided examples and when lacking of visual examples provided by user our proposed approaches can retrieve visual examples to improve the user's query
we introduce light weight scalable truthful routing protocol lstop for selfish nodes problem in mobile ad hoc networks where node may use different cost to send packets to different neighbours lstop encourages nodes cooperation by rewarding nodes for their forwarding service according to their cost it incurs low overhead of in the worst case and only on the average we show the truthfulness of lstop and present the result of an extensive simulation study to show that lstop approaches optimal cost routing and achieves significant better network performance compared to ad hoc vcg
dynamic binary translators dbts provide powerful platforms for building dynamic program monitoring and adaptation tools dbts however have high memory demands because they cache translated code and auxiliary code to software code cache and must also maintain data structures to support the code cache the high memory demands make it difficult for memory constrained embedded systems to take advantage of dbt based tools previous research on dbt memory management focused on the translated code and auxiliary code only however we found that data structures are comparable to the code cache in size we show that the translated code size auxiliary code size and the data structure size interact in complex manner depending on the path selection trace selection and link formation strategy therefore holistic memory efficiency comprising translated code auxiliary code and data structures cannot be improved by focusing on the code cache only in this paper we use path selection for improving holistic memory efficiency which in turn impacts performance in memory constrained environments although there has been previous research on path selection such research only considered performance in memory unconstrained environments the challenge for holistic memory efficiency is that the path selection strategy results in complex interactions between the memory demand components also individual aspects of path selection and the holistic memory efficiency may impact performance in complex ways we explore these interactions to motivate path selection targeting holistic memory demand we enumerate all the aspects involved in path selection design and evaluate comprehensive set of approaches for each aspect finally we propose path selection strategy that reduces memory demands by and at the same time improves performance by compared to an industrial strength dbt
effective identification of coexpressed genes and coherent patterns in gene expression data is an important task in bioinformatics research and biomedical applications several clustering methods have recently been proposed to identify coexpressed genes that share similar coherent patterns however there is no objective standard for groups of coexpressed genes the interpretation of co expression heavily depends on domain knowledge furthermore groups of coexpressed genes in gene expression data are often highly connected through large number of intermediate genes there may be no clear boundaries to separate clusters clustering gene expression data also faces the challenges of satisfying biological domain requirements and addressing the high connectivity of the data sets in this paper we propose an interactive framework for exploring coherent patterns in gene expression data novel coherent pattern index is proposed to give users highly confident indications of the existence of coherent patterns to derive coherent pattern index and facilitate clustering we devise an attraction tree structure that summarizes the coherence information among genes in the data set we present efficient and scalable algorithms for constructing attraction trees and coherent pattern indices from gene expression data sets our experimental results show that our approach is effective in mining gene expression data and is scalable for mining large data sets
we address specific enterprise document search scenario where the information need is expressed in an elaborate manner in our scenario information needs are expressed using short query of few keywords together with examples of key reference pages given this setup we investigate how the examples can be utilized to improve the end to end performance on the document retrieval task our approach is based on language modeling framework where the query model is modified to resemble the example pages we compare several methods for sampling expansion terms from the example pages to support query dependent and query independent query expansion the latter is motivated by the wish to increase aspect recall and attempts to uncover aspects of the information need not captured by the query for evaluation purposes we use the csiro data set created for the trec enterprise track the best performance is achieved by query models based on query independent sampling of expansion terms from the example documents
the view update problem is concerned with indirectly modifying those tuples that satisfy view or derived table by an appropriate update against the corresponding base tables the notion of deduction tree is defined and the relationship between such trees and the view update problem for indefinite deductive databases is considered it is shown that traversal of an appropriate deduction tree yields sufficient information to perform view updates at the propositional level to obtain similar result at the first order level it is necessary for theoretical and computational reasons to impose some weak stratification and definiteness constraints on the database
this paper describes new method for contouring signed grid whose edges are tagged by hermite data ie exact intersection points and normals this method avoids the need to explicitly identify and process features as required in previous hermite contouring methods using new numerically stable representation for quadratic error functions we develop an octree based method for simplifying contours produced by this method we next extend our contouring method to these simpli pound ed octrees this new method imposes no constraints on the octree such as being restricted octree and requires no crack patching we conclude with simple test for preserving the topology of the contour during simplification
benchmarking is critical when evaluating performance but is especially difficult for file and storage systems complex interactions between devices caches kernel daemons and other os components result in behavior that is rather difficult to analyze moreover systems have different features and optimizations so no single benchmark is always suitable the large variety of workloads that these systems experience in the real world also adds to this difficulty in this article we survey file system and storage benchmarks from recent papers we found that most popular benchmarks are flawed and many research papers do not provide clear indication of true performance we provide guidelines that we hope will improve future performance evaluations to show how some widely used benchmarks can conceal or overemphasize overheads we conducted set of experiments as specific example slowing down read operations on ext by factor of resulted in only percnt wall clock slowdown in popular compile benchmark finally we discuss future work to improve file system and storage benchmarking
vector and matrix clocks are extensively used in asynchronous distributed systems this paper asks how does the clock abstraction generalize to address this problem the paper motivates and proposes logical clocks of arbitrary dimensions it then identifies and explores the conceptual link between such clocks and knowledge it establishes the necessary and sufficient conditions on the size and dimension of clocks required to attain any specified level of knowledge about the timestamp of the most recent system state for which this is possible without using any messages in the clock protocol the paper then gives algorithms to determine the time stamp of the latest system state about which specified level of knowledge is attainable in given system state and to compute the timestamp of the earliest system state in which specified level of knowledge about given system state is attainable the results are applicable to applications that deal with certain class of properties identified as monotonic properties
check if we can apply woodruff's method to our protocol we show an efficient secure two party protocol based on yao's construction which provides security against malicious adversaries yao's original protocol is only secure in the presence of semi honest adversaries security against malicious adversaries can be obtained by applying the compiler of goldreich micali and wigderson the gmw compiler however this approach does not seem to be very practical as it requires using generic zero knowledge proofsour construction is based on applying cut and choose techniques to the original circuit and inputs security is proved according to the ideal real simulation paradigm and the proof is in the standard model with no random oracle model or common reference string assumptions the resulting protocol is computationally efficient the only usage of asymmetric cryptography is for running oblivious transfers for each input bit or for each bit of statistical security parameter whichever is larger our protocol combines techniques from folklore like cut and choose along with new techniques for efficiently proving consistency of inputs we remark that naive implementation of the cut and choose technique with yao's protocol does not yield secure protocol this is the first paper to show how to properly implement these techniques and to provide full proof of securityour protocol can also be interpreted as constant round black box reduction of secure two party computation to oblivious transfer and perfectly hiding commitments or black box reduction of secure two party computation to oblivious transfer alone with number of rounds which is linear in statistical security parameter these two reductions are comparable to kilian's reduction which uses ot alone but incurs number of rounds which is linear in the depth of the circuit
reuse signature or reuse distance pattern is an accurate model for program memory accessing behaviors it has been studied and shown to be effective in program analysis and optimizations by many recent works however the high overhead associated with reuse distance measurement restricts the scope of its application this paper explores applying sampling in reuse signature collection to reduce the time overhead we compare different sampling strategies and show that an enhanced systematic sampling with uniform coverage of all distance ranges can be used to extrapolate the reuse distance distribution based on that analysis we present novel sampling method with measurement accuracy of more than our average speedup of reuse signature collection is while the best improvement observed is this is the first attempt to utilize sampling in measuring reuse signatures experiments with varied programs and instrumentation tools show that sampling has great potential in promoting the practical uses of reuse signatures and enabling more optimization opportunities
embedded system designers face unique set of challenges in making their systems more secure as these systems often have stringent resource constraints or must operate in harsh or physically insecure environments one of the security issues that have recently drawn attention is software integrity which ensures that the programs in the system have not been changed either by an accident or an attack in this paper we propose an efficient hardware mechanism for runtime verification of software integrity using encrypted instruction block signatures we introduce several variations of the basic mechanism and give details of three techniques that are most suitable for embedded systems performance evaluation using selected mibench mediabench and basicrypt benchmarks indicates that the considered techniques impose relatively small performance overhead the best overall technique has performance overhead in the range when protecting byte instruction blocks with byte signatures with byte instruction blocks the overhead is in the range the average overhead with kb cache is with additional investment in signature cache this overhead can be almost completely eliminated
clustered microarchitectures are an effective organization to deal with the problem of wire delays and complexity by partitioning some of the processor resources the organization of the data cache is key factor in these processors due to its effect on cache miss rate and inter cluster communications this paper investigates alternative designs of the data cache centralized distributed replicated and physically distributed cache architectures are analyzed results show similar average performance but significant performance variations depending on the application features specially cache miss ratio and communications in addition we also propose novel instruction steering scheme in order to reduce communications this scheme conditionally stalls the dispatch of instructions depending on the occupancy of the clusters whenever the current instruction cannot be steered to the cluster holding most of the inputs this new steering outperforms traditional schemes results show an average speedup of and up to for some applications
age specific human computer interaction ashci has vast potential applications in daily life however automatic age estimation technique is still underdeveloped one of the main reasons is that the aging effects on human faces present several unique characteristics which make age estimation challenging task that requires non standard classification approaches according to the speciality of the facial aging effects this paper proposes the ages aging pattern subspace method for automatic age estimation the basic idea is to model the aging pattern which is defined as sequence of personal aging face images by learning representative subspace the proper aging pattern for an unseen face image is then determined by the projection in the subspace that can best reconstruct the face image while the position of the face image in that aging pattern will indicate its age the ages method has shown encouraging performance in the comparative experiments either as an age estimator or as an age range estimator
this paper presents an empirical study that evaluates oo method function points oomfp functional size measurement procedure for object oriented systems that are specified using the oo method approach laboratory experiment with students was conducted to compare oomfp with the ifpug function point analysis fpa procedure on range of variables including efficiency reproducibility accuracy perceived ease of use perceived usefulness and intention to use the results show that oomfp is more time consuming than fpa but the measurement results are more reproducible and accurate the results also indicate that oomfp is perceived to be more useful and more likely to be adopted in practice than fpa in the context of oo method systems development we also report lessons learned and suggest improvements to the experimental procedure employed and replications of this study using samples of industry practitioners
as an approach that applies not only to support user navigation on the web recommender systems have been built to assist and augment the natural social process of asking for recommendations from other people in typical recommender system people provide suggestions as inputs which the system aggregates and directs to appropriate recipients in some cases the primary computation is in the aggregation in others the value of the system lies in its ability to make good matches between the recommenders and those seeking recommendationsin this paper we discuss the architectural and design features of webmemex system that provides recommended information based on the captured history of navigation from list of people well known to the users including the users themselves allows users to have access from any networked machine demands user authentication to access the repository of recommendations and allows users to specify when the capture of their history should be performed
as the world uses more digital video that requires greater storage space grid computing is becoming indispensable for urgent problems in multimedia content analysis parallel horus support tool for applications in multimedia grid computing lets users implement multimedia applications as sequential programs for efficient execution on clusters and grids based on wide area multimedia services
traditionally software pipelining is applied either to theinnermost loop of given loop nest or from the innermostloop to outer loops in this paper we propose three stepapproach called single dimension software pipelining ssp to software pipeline loop nest at an arbitraryloop levelthe first step identifies the most profitable loop level forsoftware pipelining in terms of initiation rate or data reusepotential the second step simplifies the multi dimensionaldata dependence graph ddg into dimensional ddgand constructs dimensional schedule for the selectedloop level the third step derives simple mapping functionwhich specifies the schedule time for the operations of themulti dimensional loop based on the dimensional schedulewe prove that the ssp method is correct and at least asefficient as other modulo scheduling methodswe establish the feasibility and correctness of our approachby implementing it on the ia architecture experimentalresults on small number of loops show significantperformance improvements over existing modulo schedulingmethods that software pipeline loop nest from the innermostloop
the problem of performing tasks on asynchronous or undependable processors is basic problem in distributed computing this paper considers an abstraction of this problem called write all using processors write into all locations of an array of size in this problem writing abstracts the notion of performing simple task despite substantial research there is dearth of efficient deterministic asynchronous algorithms for write all efficiency of algorithms is measured in terms of work that accounts for all local steps performed by the processors in solving the problem thus an optimal algorithm would have work however it is known that optimality cannot be achieved when the quest then is to obtain work optimal solutions for this problem using non trivial compared to number of processors recently it was shown that optimality can be achieved using non trivial number of processors where log the new result in this paper significantly extends the range of processors for which optimality is achieved the result shows that optimality can be achieved using close to processors more precisely using log processors for any additionally the new result uses only the atomic read write memory without resorting to using the test and set primitive that was necessary in the previous solution this paper presents the algorithm and gives its analysis showing that the work complexity of the algorithm is which is optimal when while all prior deterministic algorithms require super linear work when
given sequence of symbols over some alphabet sigma of size sigma we develop new compression methods that are very simple to implement ii provide time random access to any symbol or short substring of the original sequence our simplest solution uses at most bits of space where and is the zeroth order empirical entropy of we discuss number of improvements and trade offs over the basic method for example we can achieve bits of space for log sigma several applications are discussed including text compression compressed full text indexing and string matching
most search engines display some document metadata such as title snippet and url in conjunction with the returned hits to aid users in determining documents however metadata is usually fragmented pieces of information that even when combined does not provide an overview of returned document in this paper we propose mechanism of enriching metadata of the returned results by incorporating automatically extracted document keyphrases with each returned hit we hypothesize that keyphrases of document can better represent the major theme in that document therefore by examining the keyphrases in each returned hit users can better predict the content of documents and the time spent on downloading and examining the irrelevant documents will be reduced substantially
when the mobile environment consists of light weight devices the energy consumption of location based services lbss and the limited bandwidth of the wireless network become important issues motivated by this we propose new spatial query processing algorithms to support mobile continuous nearest neighbor query mcnnq in wireless broadcast environments our solution provides general client server architecture for answering mcnnq on objects with unknown and possibly variable movement types our solution enables the application of spatio temporal access methods specifically designed for particular type to arbitrary movements without any false misses our algorithm does not require any conventional spatial index for mcnnq processing it can be adapted to static or moving objects and does not require additional knowledge eg direction of moving objects beyond the maximum speed and the location of each object extensive experiments demonstrate that our location based data dissemination algorithm significantly outperforms index based solutions
creating maintaining or using digital library requires the manipulation of digital documents information workspaces provide visual representation allowing users to collect organize annotate and author information the visual knowledge builder vkb helps users access collect annotate and combine materials from digital libraries and other sources into personal information workspace vkb has been enhanced to include direct search interfaces for nsdl and google users create visualization of search results while selecting and organizing materials for their current activity additionally metadata applicators have been added to vkb this interface allows the rapid addition of metadata to documents and aids the user in the extraction of existing metadata for application to other documents study was performed to compare the selection and organization of documents in vkb to the commonly used tools ofa web browser and word processor this study shows the value of visual workspaces for such effort but points to the need for subdocument level objects ephemeral visualizations and support for moving from visual representations to metadata
growing attention is being paid to application security at requirements engineering time confidentiality is particular subclass of security concerns that requires sensitive information to never be disclosed to unauthorized agents disclosure refers to undesired knowledge states of such agents in previous work we have extended our requirements specification framework with epistemic constructs for capturing what agents may or may not know about the application roughly an agent knows some property if the latter is found in the agent's memorythis paper makes the semantics of such constructs further precise through formal model of how sensitive information may appear or disappear in an agent's memory based on this extended framework catalog of specification patterns is proposed to codify families of confidentiality requirements proof of concept tool is presented for early checking of requirements models against such confidentiality patterns in case of violation the counterexample scenarios generated by the tool show how an unauthorized agent may acquire confidential knowledge counter measures should then be devised to produce further confidentiality requirements
constructing multipath routes in manets is important for providing reliable delivery load balancing and bandwidth aggregation however popular multipath routing approaches fail to produce spatially disjoint routes in simple and cost effective manner and existing single path approaches cannot be easily modified to produce multiple disjoint routes in this paper we propose electric field based routing efr as reliable framework for routing in manets by applying the concept of electric field lines our location based protocol naturally provides spatially disjoint routes based on the shapes of these lines the computation is highly localized and requires no explicit coordination among routes efr can also be easily extended to offer load balancing bandwidth aggregation and power management through simulation efr shows higher delivery ratio and lower overhead under high mobility high network loads and network failures compared to popular multipath and location based schemes efr also demonstrates high resiliency to dos attacks
simulation is widely used for developing evaluating and analyzing sensor network applications especially when deploying large scale sensor network remains expensive and labor intensive however due to its computation intensive nature existent simulation tools have to make trade offs between fidelity and scalability and thus offer limited capabilities as design and analysis tools in this paper we introduce disens distributed sensor network simulation highly scalable distributed simulation system for sensor networks disens does not only faithfully emulates an extensive set of sensor hardware and supports extensible radio power models so that sensor network applications can be simulated transparently with high fidelity but also employs distributed memory parallel cluster system to attack the complex simulation problem combining an efficient distributed synchronization protocol and sophisticated node partitioning algorithm based on existent research disens achieves greater scalability than even many discrete event simulators on small to medium size cluster nodes disens is able to simulate hundreds of motes in realtime speed and scale to thousands in sub realtime speed to our knowledge disens is the first full system sensor network simulator with such scalability
this paper provides an extensive survey of the different methods of addressing security issues in the grid computing environment and specifically contributes to the research environment by developing comprehensive framework for classification of these research endeavors the framework presented classifies security literature into system solutions behavioral solutions hybrid solutions and related technologies each one of these categories is explained in detail in the paper to provide insight as to their unique methods of accomplishing grid security the types of grid and security situations they apply best to and the pros and cons for each type of solution further several areas of research were identified in the course of the literature survey where more study is warranted these avenues for future research are also discussed in this paper several types of grid systems exist currently and the security needs and solutions to address those needs for each type vary as much as the types of systems themselves this research framework will aid in future research efforts to define analyze and address grid security problems for the many varied types of grid setups as well as the many security situations that each grid may face
in this paper we extend the scope of mining association rules from traditional single dimensional intratransaction associations to multidimensional intertransaction associations intratransaction associations are the associations among items with the same transaction where the notion of the transaction could be the items bought by the same customer the events happened on the same day and so on however an intertransaction association describes the association relationships among different transactions such as ldquo if company a's stock goes up on day b's stock will go down on day but go up on day rdquo in this case whether we treat company or day as the unit of transaction the associated items belong to different transactions moreover such an intertransaction association can be extended to associate multiple contextual properties in the same rule so that multidimensional intertransaction associations can be defined and discovered two dimensional intertransaction association rule example is ldquo after mcdonald and burger king open branches kfc will open branch two months later and one mile away rdquo which involves two dimensions time and space mining intertransaction associations poses more challenges on efficient processing than mining intratransaction associations interestingly intratransaction association can be treated as special case of intertransaction association from both conceptual and algorithmic point of view in this study we introduce the notion of multidimensional intertransaction association rules study their measurements mdash support and confidence mdash and develop algorithms for mining intertransaction associations by extension of apriori we overview our experience using the algorithms on both real life and synthetic data sets further extensions of multidimensional intertransaction association rules and potential applications are also discussed
modern data mining settings involve combination of attribute valued descriptors over entities as well as specified relationships between these entities we present an approach to cluster such non homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints unlike prior work that views constraints as boolean criteria we present formulation that allows constraints to be satisfied or violated in smooth manner this enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function we present results on both synthetic data as well as several real world datasets
one of the main challenges faced by content based publish subscribe systems is handling large amount of dynamic subscriptions and publications in multidimensional content space to reduce subscription forwarding load and speed up content matching subscription covering subsumption and merging techniques have been proposed in this paper we propose mics multidimensional indexing for content space that provides an efficient representation and processing model for large number of subscriptions and publications mics creates one dimensional representation for publications and subscriptions using hilbert space filling curve based on this representation we propose novel content matching and subscription management covering subsumption and merging algorithms our experimental evaluation indicates that the proposed approach significantly speeds up subscription management operations compared to the naive linear approach
an overall sensornet architecture would help tame the increasingly complex structure of wireless sensornet software and help foster greater interoperability between different codebases previous step in this direction is the sensornet protocol sp unifying link abstraction layer this paper takes the natural next step by proposing modular network layer for sensornets that sits atop sp this modularity eases implementation of new protocols by increasing code reuse and enables co existing protocols to share and reduce code and resources consumed at run time we demonstrate how current protocols can be decomposed into this modular structure and show that the costs in performance and code footprint are minimal relative to their monolithic counterparts
die stacking is an exciting new technology that increases transistor density by vertically integrating two or more die with dense high speed interface the result of die stacking is significant reduction of interconnect both within die and across dies in system for instance blocks within microprocessor can be placed vertically on multiple die to reduce block to block wire distance latency and power disparate si technologies can also be combined in die stack such as dram stacked on cpu resulting in lower power higher bw and lower latency interfaces without concern for technology integration into single process flow has the potential to change processor design constraints by providing substantial power and performance benefits despite the promising advantages of there is significant concern for thermal impact in this research we study the performance advantages and thermal challenges of two forms of die stacking stacking large dram or sram cache on microprocessor and dividing traditional microarchitecture between two die in stack results it is shown that mb stacked dram cache can reduce the cycles per memory access of twothreaded rms benchmark on average by and as much as while increasing the peak temperature by negligible ºc off die bw and power are also reduced by on average it is also shown that floorplan of high performance microprocessor can simultaneously reduce power and increase performance with small ºc increase in peak temperature voltage scaling can reach neutral thermals with simultaneous power reduction and performance improvement
as gml is becoming the de facto standard for geographic data storage transmission and exchange more and more geographic data exists in gml format in applications gml documents are usually very large in size because they contain large number of verbose markup tags and large amount of spatial coordinate data in order to speedup data transmission and reduce network cost it is essential to develop effective and efficient gml compression tools although gml is special case of xml current xml compressors are not effective if directly applied to gml because these compressors have been designed for general xml data in this paper we propose gpress compressor for effectively compressing gml documents to the best of our knowledge gpress is the first compressor specifically for gml documents compression gpress exploits the unique characteristics of gml documents to achieve good performance extensive experiments over real world gml documents show that gpress evidently outperforms xmill one of the best existing xml compressors in compression ratio while its compression efficiency is comparable to the existing xml compressors
aggregation and cube are important operations for online analytical processing olap many efficient algorithms to compute aggregation and cube for relational olap have been developed some work has been done on efficiently computing cube for multidimensional data warehouses that store data sets in multidimensional arrays rather than in tables however to our knowledge there is nothing to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional olap this paper presents set of aggregation algorithms on compressed data warehouses for multidimensional olap these algorithms operate directly on compressed data sets which are compressed by the mapping complete compression methods without the need to first decompress them the algorithms have different performance behaviors as function of the data set parameters sizes of outputs and main memory availability the algorithms are described and the and cpu cost functions are presented in this paper decision procedure to select the most efficient algorithm for given aggregation request is also proposed the analysis and experimental results show that the algorithms have better performance on sparse data than the previous aggregation algorithms
emerging grid computing infrastructures such as cloud computing can only become viable alternatives for the enterprise if they can provide stable service levels for business processes and sla based costing in this paper we describe and apply three step approach to map sla and qos requirements of business processes to such infrastructures we start with formalization of service capabilities and business process requirements we compare them and if we detect performance or reliability gap we dynamically improve performance of individual services deployed in grid and cloud computing environments here we employ translucent replication of services an experimental evaluation in amazon ec verified our approach
in this paper we present new end to end protocol namely scalable streaming video protocol ssvp which operates on top of udp and is optimized for unicast video streaming applications ssvp employs additive increase multiplicative decrease aimd based congestion control and adapts the sending rate by properly adjusting the inter packet gap ipg the smoothness oriented modulation of aimd parameters and ipg adjustments reduce the magnitude of aimd oscillation and allow for smooth transmission patterns while tcp friendliness is maintained our experimental results demonstrate that ssvp eventually adapts to the vagaries of the network and achieves remarkable performance on real time video delivery in the event where awkward network conditions impair the perceptual video quality we investigate the potential improvement via layered adaptation mechanism that utilizes receiver buffering and adapts video quality along with long term variations in the available bandwidth the adaptation mechanism sends new layer based on explicit criteria that consider both the available bandwidth and the amount of buffering at the receiver preventing wasteful layer changes that have an adverse effect on user perceived quality quantifying the interactions of ssvp with the specific adaptation scheme we identify notable gains in terms of video delivery especially in the presence of limited bandwidth
this work examines how awareness systems class of technologies that support sustained and effortless communication between individuals and groups can support family communication going beyond the evaluation of specific design concepts this paper reports on three studies that aimed to answer the following research questions do families want to be aware of each other through the day or would they perhaps rather not know more about each other's activities and whereabouts than they already do if they do wish to have some awareness what should they be aware of the research involved in depth interviews with participants field trial of an awareness system connecting five busy parents with their children and survey of participants conducted over the web triangulation of the results of the three studies leads to the following conclusions some busy parents want to automatically exchange awareness information during the day while others do not availability of partner for coordinating family activities daily activities in new family situations activity and location information of dependent children are salient awareness information needs for this group awareness information needs to vary with contexts suggesting the need for flexible mechanisms to manage the sharing of such information
we present novel framework based on continuous fluid simulator for general simulation of realistic bubbles with which we can handle as many significant dynamic bubble effects as possible to capture very thin liquid film of bubbles we have developed regional level set method allowing multi manifold interface tracking based on the definitions of regional distance and its five operators the implementation of the regional level set method is very easy an implicit surface of liquid film with arbitrary thickness can be reconstructed from the regional level set function to overcome the numerical instability problem we exploit new semi implicit surface tension model which is unconditionally stable and makes the simulation of surface tension dominated phenomena much more efficient an approximated film thickness evolution model is proposed to control the bubble's lifecycle all these new techniques combine into general framework that can produce various realistic dynamic effects of bubbles
shortest path queries spq are essential in many graph analysis and mining tasks however answering shortest path queries on the fly on large graphs is costly to online answer shortest path queries we may materialize and index shortest paths however straightforward index of all shortest paths in graph of vertices takes space in this paper we tackle the problem of indexing shortest paths and online answering shortest path queries as many large real graphs are shown richly symmetric the central idea of our approach is to use graph symmetry to reduce the index size while retaining the correctness and the efficiency of shortest path query answering technically we develop framework to index large graph at the orbit level instead of the vertex level so that the number of breadth first search trees materialized is reduced from to delta where delta le is the number of orbits in the graph we explore orbit adjacency and local symmetry to obtain compact breadth first search trees compact bfs trees an extensive empirical study using both synthetic data and real data shows that compact bfs trees can be built efficiently and the space cost can be reduced substantially moreover online shortest path query answering can be achieved using compact bfs trees
many large organizations have multiple large databases as they transact from multiple branches most of the previous pieces of work are based on single database thus it is necessary to study data mining on multiple databases in this paper we propose two measures of similarity between pair of databases also we propose an algorithm for clustering set of databases efficiency of the clustering process has been improved using the following strategies reducing execution time of clustering algorithm using more appropriate similarity measure and storing frequent itemsets space efficiently
knowing that two numerical variables always hold different values at some point of program can be very useful especially for analyzing aliases if then and are not aliased and this knowledge is of great help for many other program analyses surprisingly disequalities are seldom considered in abstract interpretation most of the proposed numerical domains being restricted to convex sets in this paper we propose to combine simple ordering properties with disequalities difference bound matrices or dbms is domain proposed by david dill for expressing relations of the form or we define ddbms disequalities dbms as conjunctions of dbms with simple disequalities of the form or we give algorithms on ddbms for deciding the emptiness computing normal form and performing the usual operations of an abstract domain these algorithms have the same complexity where is the number of variables than those for classical dbms if the variables are considered to be valued in dense set or in the arithmetic case the emptiness decision is np complete and other operations run in
as organizations implement information strategies that call for sharing access to resources in the networked environment mechanisms must be provided to protect the resources from adversaries the proposed delegation framework addresses the issue of how to advocate selective information sharing in role based systems while minimizing the risks of unauthorized access we introduce systematic approach to specify delegation and revocation policies using set of rules we demonstrate the feasibility of our framework through policy specification enforcement and proof of concept implementation on specific domains eg the healthcare environment we believe that our work can be applied to organizations that rely heavily on collaborative tasks
as new communications media foster international collaborations we would be remiss in overlooking cultural differences when assessing them in this study pairs in three cultural groupings american american aa chinese chinese cc and american chinese ac worked on two decision making tasks one face to face and the other via im drawing upon prior research we predicted differences in conversational efficiency conversational content interaction quality persuasion and performance the quantitative results combined with conversation analysis suggest that the groups viewed the task differently aa pairs as an exercise in situation specific compromise cc as consensus reaching cultural differences were reduced but not eliminated in the im condition
automatic annotation of medical images is an increasingly important tool for physicians in their daily activity hospitals nowadays produce an increasing amount of data manual annotation is very costly and prone to human mistakes this paper proposes multi cue approach to automatic medical image annotation we represent images using global and local features these cues are then combined using three alternative approaches all based on the support vector machine algorithm we tested our methods on the irma database and with two of the three approaches proposed here we participated in the imageclefmed benchmark evaluation in the medical image annotation track these algorithms ranked first and fifth respectively among all submission experiments using the third approach also confirm the power of cue integration for this task
we evaluate various heuristics for hierarchical spectral clustering in large telephone call and web graphs spectral clustering without additional heuristics often produces very uneven cluster sizes or low quality clusters that may consist of several disconnected components fact that appears to be common for several data sources but to our knowledge no general solution provided so far divide and merge recently described postfiltering procedure may be used to eliminate bad quality branches in binary tree hierarchy we propose an alternate solution that enables way cuts in each step by immediately filtering unbalanced or low quality clusters before splitting them furtherour experiments are performed on graphs with various weight and normalization built based on call detail records and web crawls we measure clustering quality both by modularity as well as by the geographic and topical homogeneity of the clusters compared to divide and merge we give more homogeneous clusters with more desirable distribution of the cluster sizes
enforcing strong replica consistency among set of replicas of service deployed across an asynchronous distributed system in the presence of crash failures is real practical challenge if each replica runs the consistency protocol bundled with the actual service implementation this target cannot be achieved as replicas need to be located over partially synchronous distributed system to solve the distributed agreement problems underlying strong replica consistencya three tier architecture for software replication enables the separation of the replication logic ie protocols and mechanisms necessary for managing software replication from both clients and server replicas the replication logic is embedded in middle tier that confines the need of partial synchrony and thus frees replica deploymentin this paper we first introduce the basic concepts underlying three tier replication then we present the interoperable replication logic irl architecture fault tolerant corba compliant infrastructure irl exploits three tier approach to replicate stateful deterministic corba objects and allows object replicas to run on object request brokers from different vendors description of an irl prototype developed in our department is proposed along with an extensive performance analysis
this paper presents cognitive vision system capable of autonomously learning protocols from perceptual observations of dynamic scenes the work is motivated by the aim of creating synthetic agent that can observe scene containing interactions between unknown objects and agents and learn models of these sufficient to act in accordance with the implicit protocols present in the scene discrete concepts utterances and object properties and temporal protocols involving these concepts are learned in an unsupervised manner from continuous sensor input alone crucial to this learning process are methods for spatio temporal attention applied to the audio and visual sensor data these identify subsets of the sensor data relating to discrete concepts clustering within continuous feature spaces is used to learn object property and utterance models from processed sensor data forming symbolic description the progol inductive logic programming system is subsequently used to learn symbolic models of the temporal protocols presented in the presence of noise and over representation in the symbolic data input to it the models learned are used to drive synthetic agent that can interact with the world in semi natural way the system has been evaluated in the domain of table top game playing and has been shown to be successful at learning protocol behaviours in such real world audio visual environments
the modeling analysis and design of systems is generally based on many formalisms to describe discrete and or continuous behaviors and to map these descriptions into specific platform in this context the article proposes the concept of functional metamodeling to capture then to integrate modeling languages the concept offers an alternative to standard model driven engineering mde and is well adapted to mathematical descriptions such as the ones found in system modeling as an application set of functional metamodels is proposed for dataflows usable to model continuous behaviors state transition systems usable to model discrete behaviors and metamodel for actions to model interactions with target platform and concurrent execution model of control architecture for legged robot is proposed as an application of these modeling languages
we propose simple obstacle model to be used while simulating wireless sensor networks to the best of our knowledge this is the first time such an integrated and systematic obstacle model for these networks has been proposed we define several types of obstacles that can be found inside the deployment area of wireless sensor network and provide categorization of these obstacles based on their nature physical and communication obstacles ie obstacles that are formed out of node distribution patterns or have physical presence respectively their shape and their change of nature over time we make an extension to custom made sensor network simulator simdust and conduct number of simulations in order to study the effect of obstacles on the performance of some representative in terms of their logic data propagation protocols for wireless sensor networks our findings confirm that obstacle presence has significant impact on protocol performance and also that different obstacle shapes and sizes may affect each protocol in different ways this provides an insight into how routing protocol will perform in the presence of obstacles and highlights possible protocol shortcomings moreover our results show that the effect of obstacles is not directly related to the density of sensor network and cannot be emulated only by changing the network density
technological achievements have made it possible to fabricate cmos circuits with over billion transistors implement boolean operations using quantum devices and or the spin of an electron implement transformations using bio and molecular based cells problems with many of these technologies are due to such factors as process variations defects and impurities in materials and solutions and noise consequently many systems built from these technologies operate imperfectly luckily there are many complex and large market systems applications that tolerate acceptable though not always correct results in addition there is emerging body of mathematical analysis related to imperfect computation in this paper we first introduce the concepts of acceptable error tolerance and acceptable performance degradation and demonstrate how important attributes of these concepts can be quantified we interlace this discussion with several examples of systems that can effectively employ these two concepts next we mention several immerging technologies that motivate the need to study these concepts as well as related mathematical paradigms finally we will list few cad issues that are needed to support this new form of technological revolution
in mainstream oo languages inheritance can be used to add new methods or to override existing methods virtual classes and feature oriented programming are techniques which extend the mechanism of inheritance so that it is possible to refine nested classes as well these techniques are attractive for programming in the large because inheritance becomes tool for manipulating whole class hierarchies rather than individual classes nevertheless it has proved difficult to design static type systems for virtual classes because virtual classes introduce dependent types the compile time type of an expression may depend on the run time values of objects in that expressionwe present formal object calculus which implements virtual classes in type safe manner our type system uses novel technique based on prototypes which blur the distinction between compile time and run time at run time prototypes act as objects and they can be used in ordinary computations at compile time they act as types prototypes are similar in power to dependent types and subtyping is shown to be form of partial evaluation we prove that prototypes are type safe but undecidable and briefly outline decidable semi algorithm for dealing with them
distributed sparing is method to improve the performance of raid disk arrays with respect to dedicated sparing system with disks including the spare disk since it utilizes the bandwidth of all disks we analyze the performance of raid with distributed sparing in normal mode degraded mode and rebuild mode in an oltp environment which implies small reads and writes the analysis in normal mode uses an queuing model which takes into account the components of disk service time in degraded mode low cost approximate method is developed to estimate the mean response time of fork join requests resulting from accesses to recreate lost data on the failed disk rebuild mode performance is analyzed by considering an vacationing server model with multiple vacations of different types to take into account differences in processing requirements for reading the first and subsequent tracks an iterative solution method is used to estimate the mean response time of disk requests as well as the time to read each disk which is shown to be quite accurate through validation against simulation results we next compare raid performance in system without cache with cache and with nonvolatile storage nvs cache the last configuration in addition to improved read response time due to cache hits provides fast write capability such that dirty blocks can be destaged asynchronously and at lower priority than read requests resulting in an improvement in read response time the small write penalty is also reduced due to the possibility of repeated writes to dirty blocks in the cache and by taking advantage of disk geometry to efficiently destage multiple blocks at time
this work investigates the problem of privacy preserving mining of frequent itemsets we propose procedure to protect the privacy of data by adding noisy items to each transaction then an algorithm is proposed to reconstruct frequent itemsets from these noise added transactions the experimental results indicate that this method can achieve rather high level of accuracy our method utilizes existing algorithms for frequent itemset mining and thereby takes full advantage of their progress to mine frequent itemset efficiently
in mobile computing environments as result of the reduced capacity of local storage it is commonly not feasible to replicate entire datasets on each mobile unit in addition reliable secure and economical access to central servers is not always possible moreover since mobile computers are designed to be portable they are also physically small and thus often unable to hold or process the large amounts of data held in centralised databases as many systems are only as useful as the data they can process the support provided by database and system management middleware for applications in mobile environments is an important driver for the uptake of this technology by application providers and thus also for the wider use of the technologyone of the approaches to maximize the available storage is through the use of database summarisation to date most strategies for reducing data volumes have used compression techniques that ignore the semantics of the data those that do not use data compression techniques adopt structural ie data and use independent methods in this paper we outline the special constraints imposed on storing information in mobile databases and provide flexible data summarisation policy the method works by assigning level of priority to each data item through the setting of number of parameters the paper discusses some policies for setting these parameters and some implementation strategies
with concurrent and garbage collected languages like java and becoming popular the need for suitable non intrusive efficient and concurrent multiprocessor garbage collector has become acute we propose novel mark and sweep on the fly algorithm based on the sliding views mechanism of levanoni and petrank we have implemented our collector on the jikes java virtual machine running on netfinity multiprocessor and compared it to the concurrent algorithm and to the stop the world collector supplied with jikes jvm the maximum pause time that we measured with our benchmarks over all runs was ms in all runs the pause times were smaller than those of the stop the world collector by two orders of magnitude and they were also always shorter than the pauses of the jikes concurrent collector throughput measurements of the new garbage collector show that it outperforms the jikes concurrent collector by up to as expected the stop the world does better than the on the fly collectors with results showing about differenceon top of being an effective mark and sweep on the fly collector standing on its own our collector may also be used as backup collector collecting cyclic data structures for the levanoni petrank reference counting collector these two algorithms perfectly fit sharing the same allocator similar data structure and similar jvm interface
dynamic energy performance scaling deps framework is proposed to save energy in fixed priority hard real time embedded systems in this generalized framework two existing technologies ie dynamic hardware resource configuration dhrc and dynamic voltage frequency scaling dvfs can be combined for energy performance tradeoff the problem of selecting the optimal hardware configuration and voltage frequency parameters is formulated to achieve maximal energy savings and meet the deadline constraint simultaneously through case study the effectiveness of deps has been validated
phrase based statistical machine translation approach mdash the alignment template approach mdash is described this translation approach allows for general many to many relations between words thereby the context of words is taken into account in the translation model and local changes in word order from source to target language can be learned explicitly the model is described using log linear modeling approach which is generalization of the often used source ndash channel approach thereby the model is easier to extend than classical statistical machine translation systems we describe in detail the process for learning phrasal translations the feature functions used and the search algorithm the evaluation of this approach is performed on three different tasks for the german ndash english speech verbmobil task we analyze the effect of various system components on the french ndash english canadian hansards task the alignment template system obtains significantly better results than single word based translation model in the chinese ndash english national institute of standards and technology nist machine translation evaluation it yields statistically significantly better nist scores than all competing research and commercial translation systems
we introduce the concept of administrative scope in role hierarchy and demonstrate that it can be used as basis for role based administration we then develop family of models for role hierarchy administration rha employing administrative scope as the central concept we then extend rha the most complex model in the family to complete decentralized model for role based administration we show that sarbac the resulting role based administrative model has significant practical and theoretical advantages over arbac we also discuss how administrative scope might be applied to the administration of general hierarchical structures how our model can be used to reduce inheritance in the role hierarchy and how it can be configured to support discretionary access control features
broadcast data dissemination is well suited for mobile wireless environments where bandwidth is scarce and mutual interference must be minimised however broadcasting monopolises the medium precluding clients from performing any other communication we address this problem in two ways firstly we segment the server broadcast with intervening periods of silence during which the wireless devices may communicate secondly we reduce the average access delay for clients using novel cooperative caching scheme our scheme is fully decentralised and uses information available locally at the client our results show that our model prevents the server from monopolising the medium and that our caching strategy reduces client access delays significantly
recently planning based on answer set programming has been proposed as an approach towards realizing declarative planning systems in this paper we present the language κc which extends the declarative planning language by action costs κc provides the notion of admissible and optimal plans which are plans whose overall action costs are within given limit resp minimum over all plans ie cheapest plans as we demonstrate this novel language allows for expressing some nontrivial planning tasks in declarative way furthermore it can be utilized for representing planning problems under other optimality criteria such as computing shortest plans with the least number of steps and refinement combinations of cheapest and fastest plans we study complexity aspects of the language κc and provide transformation to logic programs such that planning problems are solved via answer set programming furthermore we report experimental results on selected problems our experience is encouraging that answer set planning may be valuable approach to expressive planning systems in which intricate planning problems can be naturally specified and solved
in this contribution we present new paradigm and methodology for the network on chip noc based design of complex hardware software systems while classical industrial design platforms represent dedicated fixed architectures for specific applications flexible noc architectures open new degrees of system reconfigurability after giving an overview on required demands for noc hyper platforms we describe the realisation of these prerequisites within the hinoc platform we introduce new dynamic hardware software co design methodology for pre and post manufacturing design finally we will summarize the concept combined with an outlook on further investigations
mobile services operate on hosts with diverse capabilities in heterogeneous networks where the usage of resources such as processor memory and network is constantly changing in order to maintain efficiency in terms of performance and resource utilization such services should be able to adapt to changes in their environmentthis paper proposes and empirically evaluates an application transparent adaptation strategy for service oriented systems the strategy is based upon the solution of an optimization model derived from an existing suite of metrics for services which maps system services to network nodesthe strategy is evaluated empirically using number of distinct scenarios involving runtime changes in processor memory and network utilization in order to maintain execution efficiency in response to these changing operating conditions the strategy rearranges the service topology of the system dynamically by moving services between network nodes the results show that the negative impact of environmental changes on runtime efficiency can be reduced after adaptation from to depending on the selected parameters
we present an optimization framework for exploring gradient domain solutions for image and video processing the proposed framework unifies many of the key ideas in the gradient domain literature under single optimization formulation our hope is that this generalized framework will allow the reader to quickly gain general understanding of the field and contribute new ideas of their own we propose novel metric for measuring local gradient saliency that identifies salient gradients that give rise to long coherent edges even when the individual gradients are faint we present general weighting scheme for gradient constraints that improves the visual appearance of results we also provide solution for applying gradient domain filters to videos and video streams in coherent manner finally we demonstrate the utility of our formulation in creating effective yet simple to implement solutions for various image processing tasks to exercise our formulation we have created new saliency based sharpen filter and pseudo image relighting application we also revisit and improve upon previously defined filters such as nonphotorealistic rendering image deblocking and sparse data interpolation over images eg colorization using optimization
upcoming multi media compression applications will require high memory bandwidth in this paper we estimate that software reference implementation of an mpeg video decoder typically requires mtransfers to memory to decode cif times video object plane vop at frames this imposes high penalty in terms of power but also performancehowever we also show that we can heavily improve on the memory transfers without sacrificing speed even gaining about on cache misses and cycles for dec alpha by aggressive code transformations for this purpose we have manually applied an extended version of our data transfer and storage exploration dtse methodology which was originally developed for custom hardware implementations
although hierarchical pp systems have been found to outperform flat systems in many respects current pp research does not focus on strategies to build and maintain such systems available solutions assume either no or little coordination between peers that could lead the system toward satisfying globally defined goal eg minimizing traffic in this paper we focus on hierarchical dhts and provide full set of algorithms to build and maintain such systems that mitigate this problem in particular given the goal state of minimizing the total traffic without overloading any peer our algorithms dynamically adjust the system state as to keep the goal met at any time the algorithms are fully decentralized and probabilistic all decisions taken by the peers are based on their partial view on set of system wide parameters thus they demonstrate the main principle of self organization the system behavior emerges from local interactions our simulations run in range of realistic settings confirm good performance of the algorithms
programmers have traditionally used locks to synchronize concurrent access to shared data lock based synchronization however has well known pitfalls using locks for fine grain synchronization and composing code that already uses locks are both difficult and prone to deadlock transactional memory provides an alternate concurrency control mechanism that avoids these pitfalls and significantly eases concurrent programming transactional memory language constructs have recently been proposed as extensions to existing languages or included in new concurrent language specifications opening the door for new compiler optimizations that target the overheads of transactional memorythis paper presents compiler and runtime optimizations for transactional memory language constructs we present high performance software transactional memory system stm integrated into managed runtime environment our system efficiently implements nested transactions that support both composition of transactions and partial roll back our jit compiler is the first to optimize the overheads of stm and we show novel techniques for enabling jit optimizations on stm operations we measure the performance of our optimizations on way smp running multi threaded transactional workloads our results show that these techniques enable transactional memory's performance to compete with that of well tuned synchronization
this paper proposes neural network based approach for solving the resource discovery problem in peer to peer pp networks and an adaptive global local memetic algorithm aglma for performing the training of the neural network this training is very challenging due to the large number of weights and noise caused by the dynamic neural network testing the aglma is memetic algorithm consisting of an evolutionary framework which adaptively employs two local searchers having different exploration logic and pivot rules furthermore the aglma makes an adaptive noise compensation by means of explicit averaging on the fitness values and dynamic population sizing which aims to follow the necessity of the optimization process the numerical results demonstrate that the proposed computational intelligence approach leads to an efficient resource discovery strategy and that the aglma outperforms two classical resource discovery strategies as well as popular neural network training algorithm
this paper introduces the prophet critic hybrid conditionalbranch predictor which has two component predictorsthat play the role of either prophet or critictheprophet is conventional predictor that uses branch historyto predict the direction of the current branchfurther accessesof the prophet yield predictions for the branches followingthe current onepredictions for the current branchand the ones that follow are collectively known as thebranch's futurethey are actually prophecy or predictedbranch futurethe critic uses both the branch's history andfuture to give critique of the prophet's prediction fo thecurrent branchthe critique either agree or disagree isused to generate the final prediction for the branchour results show an byte prophet critic hybridhas fewer mispredicts than byte bc gskewpredictor predictor similar to that of the proposed compaq alpha ev processor across wide range of applicationsthe distance between pipeline flushes due to mispredictsincreases from one flush per micro operations uops to one per uopsfor gcc the percentage of mispredictedbranches drops from to on machinebased on the intel pentium processor this improvesupc uops per cycle by for gcc andreduces the number of uops fetched along both correct andincorrect paths by
in this paper we propose new dynamic and efficient bounding volume hierarchy for breakable objects undergoing structured and or unstructured motion our object space method is based on different ways to incrementally update the hierarchy during simulation by exploiting temporal coherence and lazy evaluation techniques this leads to significant advantages in terms of execution speed furthermore we also show how our method lends itself naturally for an adaptive low memory cost implementation which may be of critical importance in some applications finally we propose two different techniques for detecting self intersections one using our hierarchical data structure and the other is an improved sorting based method
in this article we present an experimental study of the properties of webgraphs we study large crawl from of pages and about billion edges made available by the webbase project at stanford as well as several synthetic ones generated according to various models proposed recently we investigate several topological properties of such graphs including the number of bipartite cores and strongly connected components the distribution of degrees and pagerank values and some correlations we present comparison study of the models against these measuresour findings are that the webbase sample differs slightly from the older samples studied in the literature and ii despite the fact that these models do not catch all of its properties they do exhibit some peculiar behaviors not found for example in the models from classical random graph theorymoreover we developed software library able to generate and measure massive graphs in secondary memory this library is publicy available under the gpl licence we discuss its implementation and some computational issues related to secondary memory graph algorithms
process algebraic techniques for distributed systems are increasingly being targeted at identifying abstractions that are adequate for both high level programming and specification and security analysis and verification drawing on our earlier work in bugliesi and focardi we investigate the expressive power of core set of security and network abstractions that provide high level primitives for specifying the honest principals in network while at the same time enabling an analysis of the network level adversarial attacks that may be mounted by an intruder we analyse various bisimulation equivalences for security that arise from endowing the intruder with label label different adversarial capabilities and label ii label increasingly powerful control over the interaction among the distributed principals of network by comparing the relative strength of the bisimulation equivalences we obtain direct measure of the intruder's discriminating power and hence of the expressiveness of the corresponding intruder model
we present aspect oriented programming in jiazzi jiazzi enhances java with separately compiled externally linked code modules called units units can act as effective aspect constructs with the ability to separate crosscutting concern code in non invasive and safe way unit linking provides convenient way for programmers to explicitly control the inclusion and configuration of code that implements concern while separate compilation of units enhances the independent development and deployment of the concern the expressiveness of concern separation is enhanced by units in two ways first classes can be made open to the addition of new behavior fields and methods after they are initially defined which enables the direct modularization of concerns whose code crosscut object boundaries second the signatures of methods and classes can also be made open to refinement which permits more aggressive modularization by isolating the naming and calling requirements of concern implementation
end user programming has become ubiquitous so much so that there are more end user programmers today than there are professional programmers end user programming empowers but to do what make really bad decisions based on really bad programs enter software engineering's focus on quality considering software quality is necessary because there is ample evidence that the programs end users create are filled with expensive errors in this paper consider what happens when we add to end user programming environments considerations of software quality going beyond the create program aspect of end user programming describe philosophy to software engineering for end users and then survey several projects in this area basic premise is that end user software engineering can only succeed to the extent that it respects the fact that the user probably has little expertise or even interest in software engineering
motivated by the optimality of shortest remaining processing time srpt for mean response time in recent years many computer systems have used the heuristic of favoring small jobs in order to dramatically reduce user response times however rarely do computer systems have knowledge of exact remaining sizes in this paper we introduce the class of smart policies which formalizes the heuristic of favoring small jobs in way that includes wide range of policies that schedule using inexact job size information examples of smart policies include policies that use exact size information eg srpt and psjf ii policies that use job size estimates and iii policies that use finite number of size based priority levels for many smart policies eg srpt with inexact job size information there are no analytic results available in the literature in this work we prove four main results we derive upper and lower bounds on the mean response time the mean slowdown the response time tail and the conditional response time of smart policies in each case the results explicitly characterize the tradeoff between the accuracy of the job size information used to prioritize and the performance of the resulting policy thus the results provide designers insight into how accurate job size information must be in order to achieve desired performance guarantees
in this paper we show how to augment object oriented application interfaces with enhanced specifications that include sequencing constraints called protocols protocols make explicit the relationship between messages methods supported by the application these relationships are usually only given implicitly either in the code or in textual comments we define notions of interface compatibility based upon protocols and show how compatibility can be checked discovering class of errors that cannot be discovered via the type system alone we then define software adaptors that can be used to bridge the difference between object oriented applications that have functionally compatible but type incompatible interfaces we discuss what it means for an adaptor to be well formed leveraging the information provided by protocols we show how adaptors can be automatically generated from high level description called an interface mapping
location based and personalized services are the key factors for promoting user satisfaction however most service providers did not consider the needs of mobile user in terms of their location and event participation consequently the service provider may lose the chance for better service and profit in this paper we present multi stage collaborative filtering mscf process to provide event recommendation based on mobile user's location to achieve this purpose the collaborative filtering cf technique is employed and the adaptive resonance theory art network is applied to cluster mobile users according to their personal profile sequential pattern mining is then used to discover the correlations between events for recommendation the mscf is designed not only to recommend for the old registered mobile user ormu but also to handle the cold start problem for new registered mobile user nrmu this research is designed to achieve the followings to present personalized event recommendation system for mobile users to discover mobile users moving patterns to provide recommendations based on mobile users preferences to overcome the cold start problem for new registered mobile user the experimental results of this research show that the mscf is able to accomplish the above purposes and shows better outcome for cold start problem when comparing with user based cf and item based cf
in this research we aim to identify factors that significantly affect the clickthrough of web searchers our underlying goal is determine more efficient methods to optimize the clickthrough rate we devise clickthrough metric for measuring customer satisfaction of search engine results using the number of links visited number of queries user submits and rank of clicked links we use neural network to detect the significant influence of searching characteristics on future user clickthrough our results show that high occurrences of query reformulation lengthy searching duration longer query length and the higher ranking of prior clicked links correlate positively with future clickthrough we provide recommendations for leveraging these findings for improving the performance of search engine retrieval and result ranking along with implications for search engine marketing copy wiley periodicals inc
various code certification systems allow the certification and static verification of important safety properties such as memory and control flow safety these systems are valuable tools for verifying that untrusted and potentially malicious code is safe before execution however one important safety property that is not usually included is that programs adhere to specific bounds on resource consumption such as running time we present decidable type system capable of specifying and certifying bounds on resource consumption our system makes two advances over previous resource bound certification systems both of which are necessary for practical system we allow the execution time of programs and their subroutines to vary depending on their arguments and we provide fully automatic compiler generating certified executables from source level programs the principal device in our approach is strategy for simulating dependent types using sum and inductive kinds
assessing mobility in thorough fashion is crucial step toward more efficient mobile network design recent research on mobility has focused on two main points analyzing models and studying their impact on data transport these works investigate the consequences of mobility in this paper instead we focus on the causes of mobility starting from established research in sociology we propose simps mobility model of human crowds with pedestrian motion this model defines process called sociostation rendered by two complimentary behaviors namely socialize and isolate that regulate an individual with regard to her his own sociability level simps leads to results that agree with scaling laws observed both in small scale and large scale human motion although our model defines only two simple individual behaviors we observe many emerging collective behaviors group formation splitting path formation and evolution
data domain description techniques aim at deriving concise descriptions of objects belonging to category of interest for instance the support vector domain description svdd learns hypersphere enclosing the bulk of provided unlabeled data such that points lying outside of the ball are considered anomalous however relevant information such as expert and background knowledge remain unused in the unsupervised setting in this paper we rephrase data domain description as semi supervised learning task that is we propose semi supervised generalization of data domain description sssvdd to process unlabeled and labeled examples the corresponding optimization problem is non convex we translate it into an unconstraint continuous problem that can be optimized accurately by gradient based techniques furthermore we devise an effective active learning strategy to query low confidence observations our empirical evaluation on network intrusion detection and object recognition tasks shows that our sssvdds consistently outperform baseline methods in relevant learning settings
hair simulation remains one of the most challenging aspects of creating virtual characters most research focuses on handling the massive geometric complexity of hundreds of thousands of interacting hairs this is accomplished either by using brute force simulation or by reducing degrees of freedom with guide hairs this paper presents hybrid eulerian lagrangian approach to handling both self and body collisions with hair efficiently while still maintaining detail bulk interactions and hair volume preservation is handled efficiently and effectively with flip based fluid solver while intricate hair hair interaction is handled with lagrangian self collisions thus the method has the efficiency of continuum guide based hair models with the high detail of lagrangian self collision approaches
we present an adaptive work stealing thread scheduler steal for fork join multithreaded jobs like those written using the cilk multithreaded language or the hood work stealing library the steal algorithm is appropriate for large parallel servers where many jobs share common multiprocessor resource and in which the number of processors available to particular job may vary during the job's execution steal provides continual parallelism feedback to job scheduler in the form of processor requests and the job must adaptits execution to the processors allotted to it assuming that the job scheduler never allots any job more processors than requested by thejob's thread scheduler steal guarantees that the job completes in near optimal time while utilizing at least constant fraction of the allotted processors our analysis models the job scheduler as the thread scheduler's adversary challenging the thread scheduler to be robust to the system environment and the job scheduler's administrative policies we analyze the performance of steal using trim analysis which allows us to prove that our thread scheduler performs poorly on at most small number of time steps while exhibiting near optimal behavior on the vast majority to be precise suppose that job has work and span critical path length on machine with processors steal completes the job in expected lg time steps where is the length of scheduling quantum and denotes the lg trimmed availability this quantity is the average of the processor availability over all but the lg time steps having the highest processor availability when the job's parallelism dominates the trimmed availability that is the job achieves nearly perfect linear speedup conversely when the trimmed mean dominates the parallelism the asymptotic running time of the job is nearly its span
mobile storage devices such as usb flash drives offer flexible solution for the transport and exchange of data nevertheless in order to prevent unauthorized access to sensitive data many enterprises require strict security policies for the use of such devices with the effect of rendering their advantages rather unfruitful trusted virtual domains tvds provide secure it infrastructure offering homogeneous and transparent enforcement of access control policies on data and network resources however the current model does not specifically deal with mobile storage devices in this paper we present an extension of the tvd architecture to incorporate the usage of mobile storage devices our proposal addresses three major issues coherent extension of tvd policy enforcement by introducing architectural components that feature identification and management of transitory devices transparent mandatory encryption of sensitive data stored on mobile devices and highly dynamic centralized key management service in particular we address offline scenarios allowing users to access and modify data while being temporarily disconnected from the domain we also present prototype implementation based on the turaya security kernel
we consider the problem of clustering web image search results generally the image search results returned by an image search engine contain multiple topics organizing the results into different semantic clusters facilitates users browsing in this paper we propose hierarchical clustering method using visual textual and link analysis by using vision based page segmentation algorithm web page is partitioned into blocks and the textual and link information of an image can be accurately extracted from the block containing that image by using block level link analysis techniques an image graph can be constructed we then apply spectral techniques to find euclidean embedding of the images which respects the graph structure thus for each image we have three kinds of representations ie visual feature based representation textual feature based representation and graph based representation using spectral clustering techniques we can cluster the search results into different semantic clusters an image search example illustrates the potential of these techniques
several supervised learning algorithms are suited to classify instances into multiclass value space multinomial logit mnl is recognized as robust classifier and is commonly applied within the crm customer relationship management domain unfortunately to date it is unable to handle huge feature spaces typical of crm applications hence the analyst is forced to immerse himself into feature selection surprisingly in sharp contrast with binary logit current software packages lack any feature selection algorithm for multinomial logit conversely random forests another algorithm learning multiclass problems is just like mnl robust but unlike mnl it easily handles high dimensional feature spaces this paper investigates the potential of applying the random forests principles to the mnl framework we propose the random multinomial logit rmnl ie random forest of mnls and compare its predictive performance to that of mnl with expert feature selection random forests of classification trees we illustrate the random multinomial logit on cross sell crm problem within the home appliances industry the results indicate substantial increase in model accuracy of the rmnl model to that of the mnl model with expert feature selection
dynamic information flow tracking dift is an important tool for detecting common security attacks and memory bugs dift tool tracks the flow of information through monitored program's registers and memory locations as the program executes detecting and containing fixing problems on the fly unfortunately sequential dift tools are quite slow and dift is quite challenging to parallelize in this paper we present new approach to parallelizing dift like functionality extending our recent work on accelerating sequential dift we consider variant of dift that tracks the information flow only through unary operations relaxed dift and yet makes sense for detecting security attacks and memory bugs we present parallel algorithm for relaxed dift based on symbolic inheritance tracking which achieves linear speed up asymptotically moreover we describe techniques for reducing the constant factors so that speed ups can be obtained even with just few processors we implemented the algorithm in the context of log based architectures lba system which provides hardware support for logging program trace and delivering it to other monitoring processors our simulation results on spec benchmarks and video player show that our parallel relaxed dift reduces the overhead to as low as using monitoring cores on core chip multiprocessor
this article presents resolution matched shadow maps rmsm modified adaptive shadow map asm algorithm that is practical for interactive rendering of dynamic scenes adaptive shadow maps which build quadtree of shadow samples to match the projected resolution of each shadow texel in eye space offer robust solution to projective and perspective aliasing in shadow maps however their use for interactive dynamic scenes is plagued by an expensive iterative edge finding algorithm that takes highly variable amount of time per frame and is not guaranteed to converge to correct solution this article introduces simplified algorithm that is up to ten times faster than asms has more predictable performance and delivers more accurate shadows our main contribution is the observation that it is more efficient to forgo the iterative refinement analysis in favor of generating all shadow texels requested by the pixels in the eye space image the practicality of this approach is based on the insight that for surfaces continuously visible from the eye adjacent eye space pixels map to adjacent shadow texels in quadtree shadow space this means that the number of contiguous regions of shadow texels which can be efficiently generated with rasterizer is proportional to the number of continuously visible surfaces in the scene moreover these regions can be coalesced to further reduce the number of render passes required to shadow an image the secondary contribution of this paper is demonstrating the design and use of data parallel algorithms inseparably mixed with traditional graphics programming to implement novel interactive rendering algorithm for the scenes described in this paper we achieve frames per second on static scenes and frames per second on dynamic scenes for and images with maximum effective shadow resolution of texels
escape analysis is static analysis that determines whether the lifetime of data may exceed its static scopethis paper first presents the design and correctness proof of an escape analysis for javatm this analysis is interprocedural context sensitive and as flow sensitive as the static single assignment form so assignments to object fields are analyzed in flow insensitive manner since java is an imperative language the effect of assignments must be precisely determined this goal is achieved thanks to our technique using two interdependent analyses one forward one backward we introduce new method to prove the correctness of this analysis using aliases as an intermediate step we use integers to represent the escaping parts of values which leads to fast and precise analysisour implementation blanchet which applies to the whole java language is then presented escape analysis is applied to stack allocation and synchronization elimination in our benchmarks we stack allocate percnt to percnt of data eliminate more than percnt of synchronizations on most programs percnt and percnt on two examples and get up to percnt runtime decrease percnt on average our detailed experimental study on large programs shows that the improvement comes more from the decrease of the garbage collection and allocation times than from improvements on data locality contrary to what happened for ml this comes from the difference in the garbage collectors
regular path queries are way of declaratively expressing queries on graphs as regular expression like patterns that are matched against paths in the graph there are two kinds of queries existential queries which specify properties about individual paths and universal queries which specify properties about all paths they provide simple and convenient framework for expressing program analyses as queries on graph representations of programs for expressing verification model checking problems as queries on transition systems for querying semi structured data etc parametric regular path queries extend the patterns with variables called parameters which significantly increase the expressiveness by allowing additional information along single or multiple paths to be captured and relatethis paper shows how variety of program analysis and model checking problems can be expressed easily and succinctly using parametric regular path queries the paper describes the specification design analysis and implementation of algorithms and data structures for efficiently solving existential and universal parametric regular path queries major contributions include the first complete algorithms and data structures for directly and efficiently solving existential and universal parametric regular path queries detailed complexity analysis of the algorithms detailed analytical and experimental performance comparison of variations of the algorithms and data structures and investigation of efficiency tradeoffs between different formulations of queries
mobile agent technology has emerged as promising programming paradigm for developing highly dynamic and large scale service oriented computing middlewares due to its desirable features for this purpose first of all scalable location transparent agent communication issue should be addressed in mobile agent systems despite agent mobility although there were proposed several directory service and message delivery mechanisms their disadvantages force them not to be appropriate to both low overhead location management and fast delivery of messages to agents migrating frequently to mitigate their limitations this paper presents scalable distributed directory service and message delivery mechanism the proposed mechanism enables each mobile agent to autonomously leave tails of forwarding pointers on some few of its visiting nodes depending on its preferences this feature results in low message forwarding overhead and low storage and maintenance cost of increasing chains of pointers per host also keeping mobile agent location information in the effective binding cache of each sending agent the sending agent can communicate with mobile agents much faster compared with the existing ones
idle resources can be exploited not only to run important local tasks such as data replication and virus checking but also to make contributions to society by participating in open computing projects like seti home when executing background processes to utilize such valuable idle resources we need to explicitly control them so that the user will not be discouraged from exploiting idle resources by foreground performance degradation unfortunately common priority based schedulers lack such explicit execution control in addition to encourage active use of idle resources mechanism for controlling background processes should not require modifications to the underlying operating system or user applications if such modifications are required the user may be reluctant to employ the mechanism in this paper we argue that we can reasonably detect resource contention between foreground and background processes and properly control background process execution at the user level we infer the existence of resource contention from the approximated resource shares of background processes our approach takes advantage of dynamically instrumented probes which are becoming increasingly popular in estimating the resource shares also it considers different resource types in combination and can handle varied workloads including multiple background processes we show that our system effectively avoids the performance degradation of foreground activities by suspending background processes in an appropriate fashion our system keeps the increase in foreground execution time due to background processes below or much lower in most of our experiments also we extend our approach to address undesirable resource allocations to cpu intensive processes that can occur in multiprocessor environments
we practices lack an impact on industry partly due to we field that is not quality aware in fact it is difficult to find we methodologies that pay explicit attention to quality aspects however the use of systematic process that includes quality concerns from the earliest stages of development can contribute to easing the building up of quality guaranteed web applications without drastically increasing development costs and time to market in this kind of process quality issues should be taken into account while developing each outgoing artifact from the requirements model to the final application also quality models should be defined to evaluate the quality of intermediate we artifacts and how it contributes to improving the quality of the deployed application in order to tackle its construction while avoiding some of the most common problems that existing quality models suffer from in this paper we propose number of we quality models to address the idiosyncrasies of the different stakeholders and we software artifacts involved additionally we propose that these we quality models are supported by an ontology based we measurement meta model that provides set of concepts with clear semantics and relationships this we quality metamodel is one of the main contributions of this paper furthermore we provide an example that illustrates how such metamodel may drive the definition of particular we quality model
in this work we present rendering method with guaranteed interactive frame rates in complex scenes the algorithm is based on an new data structure determined in preprocessing to avoid frozen displays in large simulative visualizations like industrial plants typically described as cad models within preprocessing polygons are grouped by size and within these groups core clusters are calculated based on similarity and locality the clusters and polygons are building up hierarchy including weights ascertained within repetitive stages of re grouping and re clustering this additional information allows to choose subset over all primitives to reduce scene complexity depending on the viewer's position sight and the determined weights within the hierarchy to guarantee specific frame rate the number of rendered primitives is limited by constant and typically constrained by hardware this reduction is controlled by the pre calculated weights and the viewer's position and is not done arbitrarily at least the rendered section is suitable scene approximation that includes the viewer's interests combining all this constant frame rate including million polygons at fps is obtainable practical results indicate that our approach leads to good scene approximations and realtime rendering of very large environments at the same time
because of the high volume and unpredictable arrival rate stream processing systems may not always be able to keep up with the input data streams resulting in buffer overflow and uncontrolled loss of data load shedding the prevalent strategy for solving this overflow problem has so far only been considered for relational stream processing but not for xml shedding applied to xml stream processing brings new opportunities and challenges due to complex nested nature of xml structures in this paper we tackle this unsolved xml shedding problem using three pronged approach first we develop an xquery preference model that enables users to specify the relative importance of preserving different subpatterns in the xml result structure this transforms shedding into the problem of rewriting the user query into shed queries that return approximate query answers with utility as measured by the given user preference model second we develop cost model to compare the performance of alternate shed queries third we develop two shedding algorithms optshed and fastshed optshed guarantees to find an optimal solution however at the cost of exponential complexity fastshed as confirmed by our experiments achieves close to optimal result in wide range of test cases finally we describe the in automaton shedding mechanism for xquery stream engines the experiments show that our proposed utility driven shedding solutions consistently achieve higher utility results compared to the existing relational shedding techniques
in the recent years the web has been rapidly deepened with the prevalence of databases online on this deep web many sources are structured by providing structured query interfaces and results organizing such structured sources into domain hierarchy is one of the critical steps toward the integration of heterogeneous web sources we observe that for structured web sources query schemas ie attributes in query interfaces are discriminative representatives of the sources and thus can be exploited for source characterization in particular by viewing query schemas as type of categorical data we abstract the problem of source organization into the clustering of categorical data our approach hypothesizes that homogeneous sources are characterized by the same hidden generative models for their schemas to find clusters governed by such statistical distributions we propose new objective function model differentiation which employs principled hypothesis testing to maximize statistical heterogeneity among clusters our evaluation over hundreds of real sources indicates that the schema based clustering accurately organizes sources by object domains eg books movies and on clustering web query schemas the model differentiation function outperforms existing ones such as likelihood entropy and context linkages with the hierarchical agglomerative clustering algorithm
nearest neighbor search is an important and widely used technique in number of important application domains in many of these domains the dimensionality of the data representation is often very high recent theoretical results have shown that the concept of proximity or nearest neighbors may not be very meaningful for the high dimensional case therefore it is often complex problem to find good quality nearest neighbors in such data sets furthermore it is also difficult to judge the value and relevance of the returned results in fact it is hard for any fully automated system to satisfy user about the quality of the nearest neighbors found unless he is directly involved in the process this is especially the case for high dimensional data in which the meaningfulness of the nearest neighbors found is questionable in this paper we address the complex problem of high dimensional nearest neighbor search from the user perspective by designing system which uses effective cooperation between the human and the computer the system provides the user with visual representations of carefully chosen subspaces of the data in order to repeatedly elicit his preferences about the data patterns which are most closely related to the query point these preferences are used in order to determine and quantify the meaningfulness of the nearest neighbors our system is not only able to find and quantify the meaningfulness of the nearest neighbors but is also able to diagnose situations in which the nearest neighbors found are truly not meaningful
we propose finite element simulation method that addresses the full range of material behavior from purely elastic to highly plastic for physical domains that are substantially reshaped by plastic flow fracture or large elastic deformations to mitigate artificial plasticity we maintain simulation mesh in both the current state and the rest shape and store plastic offsets only to represent the non embeddable portion of the plastic deformation to maintain high element quality in tetrahedral mesh undergoing gross changes we use dynamic meshing algorithm that attempts to replace as few tetrahedra as possible and thereby limits the visual artifacts and artificial diffusion that would otherwise be introduced by repeatedly remeshing the domain from scratch our dynamic mesher also locally refines and coarsens mesh and even creates anisotropic tetrahedra wherever simulation requests it we illustrate these features with animations of elastic and plastic behavior extreme deformations and fracture
shopping lists play central role in grocery shopping among other things shopping lists serve as memory aids and as tool for budgeting more interestingly shopping lists serve as an expression and indication of customer needs and interests accordingly shopping lists can be used as an input for recommendation techniques in this paper we describe methodology for making recommendations about additional products to purchase using items on the user's shopping list as shopping list entries seldom correspond to products we first use information retrieval techniques to map the shopping list entries into candidate products association rules are used to generate recommendations based on the candidate products we evaluate the usefulness and interestingness of the recommendations in user study
the emergence of wireless and mobile networks has made possible the introduction of new research area commerce or mobile commerce mobile payment is natural successor to web centric payments which has emerged as one of the sub domains of mobile commerce applications study reveals that there are wide ranges of mobile payment solutions and models which are available with the aid of various services such as short message service sms but there is no specific mobile payment system for educational institutions to collect the fees as well as for student community to pay the fees without huge investment this paper proposes secured framework for mobile payment consortia system mpcs to carry out the transactions from the bank to the academic institutions for the payment of fees by students through mobile phone mobile payment consortia system provides an end to end security using public key infrastructure pki through mobile information device profile midp enabled mobile device this framework provides an efficient reliable and secured system to perform mobile payment transactions and reduces transactional cost for both students and educational institutions mobile payment consortia system is designed with strong authentication and non repudiation by employing digital signatures confidentiality and message integrity are also provided by encrypting the messages at application level and by using public key certificates and digital signature envelops
the advance of multi core architectures provides significant benefits for parallel and throughput oriented computing but the performance of individual computation threads does not improve and may even suffer penalty because of the increased contention for shared resources this paper explores the idea of using available general purpose cores in cmp as helper engines for individual threads running on the active cores we propose lightweight architectural framework for efficient event driven software emulation of complex hardware accelerators and describe how this framework can be applied to implement variety of prefetching techniques we demonstrate the viability and effectiveness of our framework on wide range of applications from the spec cpu and olden benchmark suites on average our mechanism provides performance benefits within of pure hardware implementations furthermore we demonstrate that running event driven prefetching threads on top of baseline with hardware stride prefetcher yields significant speedups for many programs finally we show that our approach provides competitive performance improvements over other hardware approaches for multi core execution while executing fewer instructions and requiring considerably less hardware support
texture is an essential component of computer generated models for texture mapping procedure to be effective it has to generate continuous textures and cause only small mapping distortion the angle based flattening abf parameterization method is guaranteed to provide continuous no foldovers mapping it also minimizes the angular distortion of the parameterization including locating the optimal planar domain boundary however since it concentrates on minimizing the angular distortion of the mapping it can introduce relatively large linear distortionin this paper we introduce new procedure for reducing length distortion of an existing parameterization and apply it to abf results the linear distortion reduction is added as second step in texture mapping computation the new method is based on computing mapping from the plane to itself which has length distortion very similar to that of the abf parameterization by applying the inverse mapping to the result of the initial parameterization we obtain new parameterization with low length distortion we notice that the procedure for computing the inverse mapping can be applied to any other convenient mapping from the three dimensional surface to the plane in order to improve itthe mapping in the plane is computed by applying weighted laplacian smoothing to cartesian grid covering the planar domain of the initial mapping both the mapping and its inverse are provably continuous since angle preserving conformal mappings such as abf locally preserve distances as well the planar mapping has small local deformation as result the inverse mapping does not significantly increase the angular distortionthe combined texture mapping procedure provides mapping with low distance and angular distortion which is guaranteed to be continuous
retrieval speed and precision ultimately determine the success of any database system this article outlines the challenges posed by distributed and heterogeneous database systems including those that store unstructured data and surveys recent work much work remains to help users retrieve information with ease and efficiency from heterogeneous environment in which relational object oriented textual and pictorial databases coexist the article outlines the progress that has been made in query processing in distributed relational database systems and heterogeneous and multidatabase systems
the ultimate challenges of system modeling concern designing accurate yet highly transparent and user centric models we have witnessed plethora of neurofuzzy architectures which are aimed at addressing these two highly conflicting requirements this study is concerned with the design and the development of transparent logic networks realized with the aid of fuzzy neurons and fuzzy unineurons the construction of networks of this form requires formation of efficient interfaces that constitute conceptually appealing bridge between the model and the real world experimental environment in which the model is to be used in general the interfaces are constructed by invoking some form of granulation of information and binary boolean discretization in particular we introduce new discretization environment that is realized by means of particle swarm optimization pso and data clustering implemented by the means algorithm the underlying structure of the network is optimized by invoking combination of the pso and the mechanisms of conventional gradient based learning we discuss various optimization strategies by considering boolean as well as fuzzy data coming as the result of discretization of original experimental data and then involving several learning strategies we elaborate on the interpretation aspects of the network and show how those could be strengthened through efficient pruning we also show how the interpreted network leads to simpler and more accurate logic description of the experimental data number of experimental studies are included
this paper shows that even very small data caches when split to serve data streams exhibiting temporal and spatial localities can improve performance of embedded applications without consuming excessive silicon real estate or power it also shows that large block sizes or higher set associativities are unnecessary with split cache organizations we use benchmark programs from mibench to show that our cache organization outperforms other organizations in terms of miss rates access times energy consumption and silicon area
in this paper we address the task of crosslingual semantic relatedness we introduce method that relies on the information extracted from wikipedia by exploiting the interlanguage links available between wikipedia versions in multiple languages through experiments performed on several language pairs we show that the method performs well with performance comparable to monolingual measures of relatedness
the fair evaluation and comparison of side channel attacks and countermeasures has been long standing open question limiting further developments in the field motivated by this challenge this work makes step in this direction and proposes framework for the analysis of cryptographic implementations that includes theoretical model and an application methodology the model is based on commonly accepted hypotheses about side channels that computations give rise to it allows quantifying the effect of practically relevant leakage functions with combination of information theoretic and security metrics measuring the quality of an implementation and the strength of an adversary respectively from theoretical point of view we demonstrate formal connections between these metrics and discuss their intuitive meaning from practical point of view the model implies unified methodology for the analysis of side channel key recovery attacks the proposed solution allows getting rid of most of the subjective parameters that were limiting previous specialized and often ad hoc approaches in the evaluation of physically observable devices it typically determines the extent to which basic but practically essential questions such as how to compare two implementations or how to compare two side channel adversaries can be answered in sound fashion
with the significant advances in mobile computing technology there is an increasing demand for various mobile applications to process transactions in real time fashion when remote data access is considered in mobile environment data access delay becomes one of the most serious problems in meeting transaction deadlines in this paper we propose multi version data model and adopt the relative consistency as the correctness criterion for processing of real time transactions in mobile environment the purpose is to reduce the impacts of unpredictable and unreliable mobile network on processing of the real time transactions under the proposed model the overheads for concurrency control can be significantly reduced and the data availability is much enhanced even under network failures real time transaction may access stale data provided that they are relatively consistent with the data accessed by the transaction and the staleness of the data is within the requirements an image transaction model which pre fetches multiple data versions at fixed hosts is proposed to reduce the data access delay and to simplify the management of the real time transactions in mobile environment the image transaction model also helps in reducing the transaction restart overheads and minimizing the impacts of the unpredictable performance of mobile network on transaction executions
bisimulation semantics are very pleasant way to define the semantics of systems mainly because the simplicity of their definitions and their nice coalgebraic properties however they also have some disadvantages they are based on sequential operational semantics defined by means of an ordinary transition system and in order to be bisimilar two systems have to be too similar in this work we will present several natural proposals to define weaker bisimulation semantics that we think properly capture the desired behaviour of distributed systems the main virtue of all these semantics is that they are real bisimulation semantics thus inheriting most of the good properties of bisimulation semantics this is so because they can be defined as particular instances of jacobs and hughes categorical definition of simulation which they have already proved to satisfy all those properties
new ldquo range space rdquo approach is described for synergistic resolution of both stereovision and reflectance visual modeling problems simultaneously this synergistic approach can be applied to arbitrary camera arrangements with different intrinsic and extrinsic parameters image types image resolutions and image number these images are analyzed in step wise manner to extract range measurements and also to render customized perspective view the entire process is fully automatic an extensive and detailed experimental validation phase supports the basic feasibility and generality of the range space approach
category level object recognition segmentation and tracking in videos becomes highly challenging when applied to sequences from hand held camera that features extensive motion and zooming an additional challenge is then to develop fully automatic video analysis system that works without manual initialization of tracker or other human intervention both during training and during recognition despite background clutter and other distracting objects moreover our working hypothesis states that category level recognition is possible based only on an erratic flickering pattern of interest point locations without extracting additional features compositions of these points are then tracked individually by estimating parametric motion model groups of compositions segment video frame into the various objects that are present and into background clutter objects can then be recognized and tracked based on the motion of their compositions and on the shape they form finally the combination of this flow based representation with an appearance based one is investigated besides evaluating the approach on challenging video categorization database with significant camera motion and clutter we also demonstrate that it generalizes to action recognition in natural way
development of new embedded systems requires tuning of the software applications to specific hardware blocks and platforms as well as to the relevant input data instances the behaviour of these applications heavily relies on the nature of the input data samples thus making them strongly data dependent for this reason it is necessary to extensively profile them with representative samples of the actual input data an important aspect of this profiling is done at the dynamic data type level which actually steers the designers choice of implementation of these data types the behaviour of the applications is then characterized through an analysis phase as collection of software metadata that can be used to optimize the system as whole in this paper we propose to represent the behaviour of data dependent applications to enable optimizations rather than to analyze their structure or to define the engineering process behind them moreover we specifically limit ourselves to the scope of applications dominated by dynamically allocated data types running on embedded systems we characterize the software metadata that these optimizations require and we present methodology as well as appropriate techniques to obtain this information from the original application the optimizations performed on complete case study utilizing the extracted software metadata achieve overall improvements of up to in the number of cycles spent accessing memory when compared to code optimized only with the static techniques applied by gnu
this work addresses the issue of answering spatio temporal range queries when there is uncertainty associated with the model of the moving objects uncertainty is inherent in moving objects database mod applications and capturing it in the data model has twofold impact the number of updates when the actual trajectory deviates from its mod representation the linguistic constructs and the processing algorithms for querying the mod the paper presents both spatial and temporal uncertainty aspects which are combined into one model of uncertain trajectories given the model the methodology is presented which enables processing of queries such as what is the probability that given moving object was will be inside given region sometimes always during given time interval where the regions are bounded by arbitrary polygons
in recent years grid systems and peer to peer networks are the most commonly used solutions to achieve the same goal the sharing of resources and services in heterogeneous dynamic distributed environments many studies have proposed hybrid approaches that try to conjugate the advantages of the two models this paper proposes an architecture that integrates the pp interaction model in grid environments so as to build an open cooperative model wherein grid entities are composed in decentralized way in particular this paper focuses on qos aware discovery algorithm for pp grid systems analyzing protocol and explaining techniques used to improve its performance
technological success has ushered in massive amounts of data for scientific analysis to enable effective utilization of these data sets for all classes of users supporting intuitive data access and manipulation interfaces is crucial this paper describes an autonomous scientific workflow system that enables high level natural language based queries over low level data sets our technique involves combination of natural language processing metadata indexing and semantically aware workflow composition engine which dynamically constructs workflows for answering queries based on service and data availability specific contribution of this work is metadata registration scheme that allows for unified index of heterogeneous metadata formats and service annotations our approach thus avoids standardized format for storing all data sets or the implementation of federated mediator based querying framework we have evaluated our system using case study from the geospatial domain to show functional results our evaluation supports the potential benefits which our approach can offer to scientific workflow systems and other domain specific data intensive applications
this paper evaluates pointer tainting an incarnation of dynamic information flow tracking dift which has recently become an important technique in system security pointer tainting has been used for two main purposes detection of privacy breaching malware eg trojan keyloggers obtaining the characters typed by user and detection of memory corruption attacks against non control data eg buffer overflow that modifies user's privilege level in both of these cases the attacker does not modify control data such as stored branch targets so the control flow of the target program does not change phrased differently in terms of instructions executed the program behaves normally as result these attacks are exceedingly difficult to detect pointer tainting is considered one of the onlymethods for detecting them in unmodified binaries unfortunately almost all of the incarnations of pointer tainting are flawed in particular we demonstrate that the application of pointer tainting to the detection of keyloggers and other privacybreaching malware is problematic we also discuss whether pointer tainting is able to reliably detect memory corruption attacks against non control data pointer tainting generates itself the conditions for false positives we analyse the problems in detail and investigate various ways to improve the technique most have serious drawbacks in that they are either impractical and incur many false positives still and or cripple the technique's ability to detect attacks in conclusion we argue that depending on architecture and operating system pointer tainting may have some value in detecting memory orruption attacks albeit with false negatives and not on the popular architecture but it is fundamentally not suitable for automated detecting of privacy breaching malware such as keyloggers
common belief in the scientific community is that traffic classifiers based on deep packet inspection dpi are far more expensive in terms of computational complexity compared to statistical classifiers in this paper we counter this notion by defining accurate models for deep packet inspection classifier and statistical one based on support vector machines and by evaluating their actual processing costs through experimental analysis the results suggest that contrary to the common belief dpi classifier and an svm based one can have comparable computational costs although much work is left to prove that our results apply in more general cases this preliminary analysis is first indication of how dpi classifiers might not be as computationally complex compared to other approaches as we previously thought
in recent years both performance and power have become key factors in efficient memory design in this paper we propose systematic approach to reduce the energy consumption of the entire memory hierarchy we first evaluate an existing power aware memory system where memory modules can exist in different power modes and then propose on chip memory module buffers called energy saver buffers esb which reside in between the cache and main memory esbs reduce the additional overhead incurred due to frequent resynchronization of the memory modules in low power state an additional improvement is attained by using model that dynamically resizes the active cache based on the varying needs of program our experimental results demonstrate that an integrated approach can reduce the energy delay product by as much as when compared to traditional non power aware memory hierarchy
we show how properties of an interesting class of imperative programs can be calculated by means of relational modeling and symbolic computation the ideas of are implemented using symbolic computations based on maple
focus on organizing and implementing workflows in government from standpoint of data awareness or even data centricity might provide opportunities to address several of the challenges facing governments efforts to improve government effectiveness and efficiency as well as interoperation between government entities the notion of data aware as opposed to data unaware or just process centric workflows is based on taking into account and using the particular enactments and instance specific data in the workflow itself and beyond its single instance in other words on the one hand workflows process data however through their enactments and instances on the other hand workflows are data ready as inputs for other workflows including cross process enactment mining and analyses to be useful as strategy in government we need to explore how data centric approaches can tackle the specific challenges of government such as the ill structured or semi structured workflows found in emergency and disaster response management and we need to better understand the specific constraints under which intra and cross agency data centric workflows can be designed and implemented this paper lays out research agenda that will inform questions about the issues with and the potential of the data centric approach in the context of government
many electronic cash systems have been proposed with the proliferation of the internet and the activation of electronic commerce cash enables the exchange of digital coins with value assured by the bank's signature and with concealed user identity in an electronic cash system user can withdraw coins from the bank and then spends each coin anonymously and unlinkably in this paper we design an efficient anonymous mobile payment system based on bilinear pairings in which the anonymity of coins is revocable by trustee in case of dispute the message transfer from the customer to the merchant occurs only once during the payment protocol also the amount of communication between customer and merchant is about bits therefore our mobile payment system can be used in the wireless networks with the limited bandwidth the security of the new system is under the computational diffie hellman problem in the random oracle model
in this paper we present formal model named pobsam policy based self adaptive model for developing and modeling self adaptive systems in this model policies are used as mechanism to direct and adapt the behavior of self adaptive systems pobsam model consists of set of self managed modules smm an smm is collection of autonomous managers and managed actors managed actors are dedicated to functional behavior while autonomous managers govern the behavior of managed actors by enforcing suitable policies to adapt smm behavior in response to changes policies governing an smm are adjusted ie dynamic policies are used to govern and adapt system behavior we employ the combination of an algebraic formalism and an actor based model to specify this model formally managers are modeled as meta actors whose policies are described using an algebra managed actors are expressed by an actor model furthermore we provide an operational semantics for pobsam described using labeled transition systems
embedded systems designers are free to choose the most suitable configuration of cache in modern processor based socs choosing the appropriate cache configuration necessitates the simulation of long memory access traces to accurately obtain hit miss rates the long execution time taken to simulate these traces particularly separate simulation for each configuration is major drawback researchers have proposed techniques to speed up the simulation of caches with lru replacement policy these techniques are of little use in the majority of embedded processors as these processors utilize round robin policy based caches in this paper we propose fast cache simulation approach called scud sorted collection of unique data for caches with the round robin policy scud is single pass cache simulator that can simulate multiple cache configurations with varying set sizes and associativities by reading the application trace once utilizing fast binary searches in novel data structure scud simulates an application trace significantly faster than widely used single configuration cache simulator dinero iv we show scud can simulate set of cache configurations up to times faster than dinero iv scud shows an average speed up of times over dinero iv for mediabench applications and an average speed up of over times for spec cpu applications
the frequent items problem is to process stream of items and find all items occurring more than given fraction of the time it is one of the most heavily studied problems in data stream mining dating back to the many applications rely directly or indirectly on finding the frequent items and implementations are in use in large scale industrial systems however there has not been much comparison of the different methods under uniform experimental conditions it is common to find papers touching on this topic in which important related work is mischaracterized overlooked or reinvented in this paper we aim to present the most important algorithms for this problem in common framework we have created baseline implementations of the algorithms and used these to perform thorough experimental study of their properties we give empirical evidence that there is considerable variation in the performance of frequent items algorithms the best methods can be implemented to find frequent items with high accuracy using only tens of kilobytes of memory at rates of millions of items per second on cheap modern hardware
constructing correct concurrent garbage collection algorithms is notoriously hard numerous such algorithms have been proposed implemented and deployed and yet the relationship among them in terms of speed and precision is poorly understood and the validation of one algorithm does not carry over to othersas programs with low latency requirements written in garbagecollected languages become part of society's mission critical infrastructure it is imperative that we raise the level of confidence in the correctness of the underlying system and that we understand the trade offs inherent in our algorithmic choicein this paper we present correctness preserving transformations that can be applied to an initial abstract concurrent garbage collection algorithm which is simpler more precise and easier to prove correct than algorithms used in practice but also more expensive and with less concurrency we then show how both pre existing and new algorithms can be synthesized from the abstract algorithm by series of our transformations we relate the algorithms formally using new definition of precision and informally with respect to overhead and concurrencythis provides many insights about the nature of concurrent collection allows the direct synthesis of new and useful algorithms reduces the burden of proof to single simple algorithm and lays the groundwork for the automated synthesis of correct concurrent collectors
moving and resizing desktop windows are frequently performed but largely unexplored interaction tasks the standard title bar and border dragging techniques used for window manipulation have not changed much over the years we studied three new methods to move and resize windows the new methods are based on proxy and goal crossing techniques to eliminate the need of long cursor movements and acquiring narrow window borders instead moving and resizing actions are performed by manipulating proxy objects close to the cursor and by sweeping cursor motions across window borders we compared these techniques with the standard techniques the results indicate that further investigations and redesigns of window manipulation techniques are worthwhile all new techniques were faster than the standard techniques with task completion times improving more than in some cases also the new resizing techniques were found to be less error prone than the traditional click and drag method
permissive nominal logic pnl is an extension of first order logic where term formers can bind names in their arguments this allows for direct axiomatisations with binders such as the quantifier of first order logic itself and the binder of the lambda calculus this also allows us to finitely axiomatise arithmetic like first and higher order logic and unlike other nominal logics equality reasoning is not necessary to alpha rename all this gives pnl much of the expressive power of higher order logic but terms derivations and models of pnl are first order in character and the logic seems to strike good balance between expressivity and simplicity
computing environments on cellphones especially smartphones are becoming more open and general purpose thus they also become attractive targets of malware cellphone malware not only causes privacy leakage extra charges and depletion of battery power but also generates malicious traffic and drains down mobile network and service capacity in this work we devise novel behavior based malware detection system named pbmds which adopts probabilistic approach through correlating user inputs with system calls to detect anomalous activities in cellphones pbmds observes unique behaviors of the mobile phone applications and the operating users on input and output constrained devices and leverages hidden markov model hmm to learn application and user behaviors from two major aspects process state transitions and user operational patterns built on these pbdms identifies behavioral differences between malware and human users through extensive experiments on major smartphone platforms we show that pbmds can be easily deployed to existing smartphone hardware and it achieves high detection accuracy and low false positive rates in protecting major applications in smartphones
context the technology acceptance model tam was proposed in as means of predicting technology usage however it is usually validated by using measure of behavioural intention to use bi rather than actual usage objective this review examines the evidence that the tam predicts actual usage using both subjective and objective measures of actual usage method we performed systematic literature review based on search of six digital libraries along with vote counting meta analysis to analyse the overall results results the search identified relevant empirical studies in articles the results show that bi is likely to be correlated with actual usage however the tam variables perceived ease of use peu and perceived usefulness pu are less likely to be correlated with actual usage conclusion care should be taken using the tam outside the context in which it has been validated
recent years have transformed the web from web of content to web of applications and social content thus it has become crucial to be able to tap on this social aspect of the web whenever possible in addition to its content particularly for focused crawling in this paper we present novel profile based focused crawling system for dealing with the increasingly popular social media sharing web sites without assuming any privileged access to the internal private databases of such websites nor any requirement for the existence of apis for the extraction of social data our experiments prove the robustness of our profile based focused crawler as well as significant improvement in harvest ratio compared to breadth first and opic crawlers when crawling the flickr web site for two different topics
the combination of sgml and database technology allows to refine both declarative and navigational access mechanisms for structured document collection with regard to declarative access the user can formulate complex information needs without knowing query language the respective document type definition dtd or the underlying modelling navigational access is eased by hyperlink rendition mechanisms going beyond plain link integrity checking with our approach the database internal representation of documents is configurable it allows for an efficient implementation of operations because dtd knowledge is not needed for document structure recognition we show how the number of method invocations and the cost of parsing can be significantly reduced
information shown on tabletop display can appear distorted when viewed by seated user even worse the impact of this distortion is different depending on the location of the information on the display in this paper we examine how this distortion affects the perception of the basic graphical elements of information visualization shown on displays at various angles we first examine perception of these elements on single display and then compare this to perception across displays in order to evaluate the effectiveness of various elements for use in tabletop and multi display environment we found that the perception of some graphical elements is more robust to distortion than others we then develop recommendations for building data visualizations for these environments
the paper proposes new knowledge representation language called dlp which extends disjunctive logic programming with strong negation by inheritance the addition of inheritance enhances the knowledge modeling features of the language providing natural representation of default reasoning with exceptions declarative model theoretic semantics of dlp is provided which is shown to generalize the answer set semantics of disjunctive logic programs the knowledge modeling features of the language are illustrated by encoding classical nonmonotonic problems in dlp the complexity of dlp is analyzed proving that inheritance does not cause any computational overhead as reasoning in dlp has exactly the same complexity as reasoning in disjunctive logic programming this is confirmed by the existence of an efficient translation from dlp to plain disjunctive logic programming using this translation an advanced kr system supporting the dlp language has been implemented on top of the dlv system and has subsequently been integrated into dlv
the event service of the common object request broker architecture corba is useful in supporting decoupled and asynchronous communication between distributed object components however the specification of the event service standard does not require implementation to provide facilities to guarantee efficient event data delivery consequently applications in which large number of objects need to communicate via an event service channel may suffer from poor performance in this paper generic corba based framework is proposed to tackle this scalability problem two techniques are applied namely event channel federation and load balancing the solution is transparent in the sense that it exports the same idl interface as the original event service we explore three critical dimensions underlying the design of the load balancing algorithm and conduct experiments to evaluate their impact on the overall performance of the framework the results provide some useful insights into the improvement of the scalability of the event service
the development of efficient techniques for transforming massive volumes of remotely sensed hyperspectral data into scientific understanding is critical for space based earth science and planetary exploration although most available parallel processing strategies for information extraction and mining from hyperspectral imagery assume homogeneity in the underlying computing platform heterogeneous networks of computers hnocs have become promising cost effective solution expected to play major role in many on going and planned remote sensing missions in this paper we develop new morphological parallel algorithm for hyperspectral image classification using heterompi an extension of mpi for programming high performance computations on hnocs the main idea of heterompi is to automate and optimize the selection of group of processes that executes heterogeneous algorithm faster than any other possible group in heterogeneous environment in order to analyze the impact of many to one gather communication operations introduced by our proposed algorithm we resort to recently proposed collective communication model the parallel algorithm is validated using two heterogeneous clusters at university college dublin and massively parallel beowulf cluster at nasa's goddard space flight center
this paper presents many core heterogeneous computational platform that employs gals compatible circuit switched on chip network the platform targets streaming dsp and embedded applications that have high degree of task level parallelism among computational kernels the test chip was fabricated in nm cmos consisting of simple small programmable cores three dedicated purpose accelerators and three shared memory modules all processors are clocked by their own local oscillators and communication is achieved through simple yet effective source synchronous communication technique that allows each interconnection link between any two processors to sustain peak throughput of one data word per cycle complete wlan baseband receiver was implemented on this platform it has real time throughput of mbps with all processors running at mhz and and consumes an average mw with mw or dissipated by its interconnection links we can fully utilize the benefit of the gals architecture and by adjusting each processor's oscillator to run at workload based optimal clock frequency with the chip's dual supply voltages set at and the receiver consumes only mw in power reduction measured results of its power consumption on the real chip come within the difference of only compared with the estimated results showing our design to be highly reliable and efficient
despite the fact that global software development gsd is steadily becoming the standard engineering mode in the software industry commercial projects still struggle with how to effectively manage it recent research and our own experiences from numerous gsd projects at capgemini sd indicate that staging the development process with handover checkpoints is promising practice in order to tackle many of the encountered problems in practice in this paper we discuss typical management problems in gsd we describe how handover checkpoints are used at capgemini sd to control and safely manage large gsd projects we show how these handover checkpoints and the use of cohesive and self contained work packages effectively mitigate the discussed management problems we are continuously refining and improving our handover checkpoint approach by applying it within large scale commercial gsd projects we thus believe that the presented results can serve the practitioner as fundament for implementing and customizing handover checkpoints within his own organisation
play on demand is usually regarded as feasible access mode for web content including streaming video web pages and so on web services and some software as service saas applications but not for common desktop applications this paper presents such solution for windows desktop applications based on lightweight virtualization and network transportation technologies which allows user to run her personalized software on any compatible computer across the internet even though they do not exist on local disks of the host in our approach the user's data and their configurations are stored on portable usb device at run time the desktop applications are downloaded from the internet and run in lightweight virtualization environment in which some resource accessing apis such as registry files directories environment variables and the like are intercepted and redirected to the portable device or network as needed because applications are played without installation like streaming media they can be called streaming software moreover to protect software vendors rights access control technologies are used to block any illegal access in the current implementation pp transportation is used as the transport method however our design actually does not rely on pp and another data delivery mechanism like dedicated file server could be employed instead to make the system more predictable this paper describes the design and technical details for this system presents demo application and evaluates it performance the proposed solution is shown to be more efficient in performance and storage capacity than some of the existing solutions based on vm techniques
in an application where sparse matching of feature points is used towards fast scene reconstruction the choice of the type of features to be matched has an important impact on the quality of the resulting model in this work method is presented for quickly and reliably selecting and matching points from three views of scene the selected points are based on epipolar gradients and consist of stable image features relevant to reconstruction then the selected points are matched using edge transfer measure of geometric consistency for point triplets and the edges on which they lie this matching scheme is tolerant to image deformations due to changes in viewpoint models drawn from matches obtained by the proposed technique are shown to demonstrate its usefulness
several approaches to collaborative filtering have been studied but seldom have studies been reported for large several millionusers and items and dynamic the underlying item set is continually changing settings in this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of google news we generate recommendations using three approaches collaborative filtering using minhash clustering probabilistic latent semantic indexing plsi and covisitation counts we combine recommendations from different algorithms using linear model our approach is content agnostic and consequently domain independent making it easily adaptable for other applications and languages with minimal effort this paper will describe our algorithms and system setup in detail and report results of running the recommendations engine on google news
this article presents an approach to identify abstract data types adt and abstract state encapsulations ase also called abstract objects in source code this approach named similarity clustering groups together functions types and variables into adt and ase candidates according to the proportion of features they share the set of features considered includes the context of these elements the relationships to their environment and informal information prototype tool has been implemented to support this approach it has been applied to three systems each between ndash kloc the adts and ases identified by the approach are compared to those identified by software engineers who did not know the proposed approach or other automatic approaches within this case study this approach has been shown to have higher detection quality and to identify in most of the cases more adts and ases than the other techniques in all other cases its detection quality is second best nb this article reports on work in progress on this approach which has evolved since it was presented in the original ase conference paper
competitive native solvers for answer set programming asp perform backtracking search by assuming the truth of literals the choice of literals the heuristic is fundamental for the performance of these systems most of the efficient asp systems employ heuristic based on look ahead that is literal is tentatively assumed and its heuristic value is based on its deterministic consequences however looking ahead is costly operation and indeed look ahead often accounts for the majority of time taken by asp solvers for satisfiability sat radically different approach called look back heuristic proved to be quite successful instead of looking ahead one uses information gathered during the computation performed so far thus looking back in this approach atoms which have been frequently involved in inconsistencies are preferred in this paper we carry over this approach to the framework of disjunctive asp we design number of look back heuristics exploiting peculiarities of asp and implement them in the asp system dlv we compare their performance on collection of hard asp programs both structured and randomly generated these experiments indicate that very basic approach works well outperforming all of the prominent disjunctive asp systems dlv with its traditional heuristic gnt and cmodels on many of the instances considered
the widespread presence of simd devices in today's microprocessors has made compiler techniques for these devices tremendously important one of the most important and difficult issues that must be addressed by these techniques is the generation of the data permutation instructions needed for non contiguous and misaligned memory references these instructions are expensive and therefore it is of crucial importance to minimize their number to improve performance and in many cases enable speedups over scalar codealthough it is often difficult to optimize an isolated data reorganization operation collection of related data permutations can often be manipulated to reduce the number of operations this paper presents strategy to optimize all forms of data permutations the strategy is organized into three steps first all data permutations in the source program are converted into generic representation these permutations can originate from vector accesses to non contiguous and misaligned memory locations or result from compiler transformations second an optimization algorithm is applied to reduce the number of data permutations in basic block by propagating permutations across statements and merging consecutive permutations whenever possible the algorithm can significantly reduce the number of data permutations finally code generation algorithm translates generic permutation operations into native permutation instructions for the target platform experiments were conducted on various kinds of applications the results show that up to of the permutation instructions are eliminated and as result the average performance improvement is on vmx and on sse for several applications near perfect speedups have been achieved on both platforms
this paper presents an approach that uses special purpose rbac constraints to base certain access control decisions on context information in our approach context constraint is defined as dynamic rbac constraint that checks the actual values of one or more contextual attributes for predefined conditions if these conditions are satisfied the corresponding access request can be permitted accordingly conditional permission is an rbac permission which is constrained by one or more context constraints we present an engineering process for context constraints that is based on goal oriented requirements engineering techniques and describe how we extended the design and implementation of an existing rbac service to enable the enforcement of context constraints with our approach we aim to preserve the advantages of rbac and offer an additional means for the definition and enforcement of fine grained context dependent access control policies
publish subscribe systems are used increasingly often as communication mechanism in loosely coupled distributed applications with their gradual adoption in mission critical areas it is essential that systems are subjected to rigorous performance analysis before they are put into production however existing approaches to performance modeling and analysis of publish subscribe systems suffer from many limitations that seriously constrain their practical applicability in this paper we present set of generalized and comprehensive analytical models of publish subscribe systems employing different peer to peer and hierarchical routing schemes the proposed analytical models address the major limitations underlying existing work in this area and are the first to consider all major performance relevant system metrics including the expected broker and link utilization the expected notification delay the expected time required for new subscriptions to become fully active as well as the expected routing table sizes and message rates to illustrate our approach and demonstrate its effectiveness and practicality we present case study showing how our models can be exploited for capacity planning and performance prediction in realistic scenario
speed up techniques that exploit given node coordinates have proven useful for shortest path computations in transportation networks and geographic information systems to facilitate the use of such techniques when coordinates are missing from some or even all of the nodes in network we generate artificial coordinates using methods from graph drawing experiments on large set of german train timetables indicate that the speed up achieved with coordinates from our drawings is close to that achieved with the true coordinates and in some special cases even better
this paper proposes novel method for phrase based statistical machine translation based on the use of pivot language to translate between languages and with limited bilingual resources we bring in third language called the pivot language for the language pairs and there exist large bilingual corpora using only and bilingual corpora we can build translation model for the advantage of this method lies in the fact that we can perform translation between and even if there is no bilingual corpus available for this language pair using bleu as metric our pivot language approach significantly outperforms the standard model trained on small bilingual corpus moreover with small bilingual corpus available our method can further improve translation quality by using the additional and bilingual corpora
media spaces and videoconference systems are beneficial for connecting separated co workers and providing rich contextual information however image sharing communication tools may also touch on sensitive spots of the human psyche related to personal perceived image issues eg appearance self image self presentation and vanity we conducted two user studies to examine the impact of self image concerns on the use of media spaces and videoconference systems our results suggest that personal perceived image concerns have considerable impact on the comfort level of users and may hinder effective communication we also found that image filtering techniques can help users feel more comfortable our results revealed that distortion filters which are frequently cited to help preserve privacy do not tend to be the ones preferred by users instead users seemed to favor filters that make subtle changes to their appearance or in some instances they preferred to use surrogate instead
xml documents have recently become ubiquitous because of their varied applicability in number of applications classification is an important problem in the data mining domain but current classification methods for xml documents use ir based methods in which each document is treated as bag of words such techniques ignore significant amount of information hidden inside the documents in this paper we discuss the problem of rule based classification of xml data by using frequent discriminatory substructures within xml documents such technique is more capable of finding the classification characteristics of documents in addition the technique can also be extended to cost sensitive classification we show the effectiveness of the method with respect to other classifiers we note that the methodology discussed in this paper is applicable to any kind of semi structured data
in this paper we present new algorithm named turbosyn for fpga synthesis with retiming and pipelining to minimizethe clock period for sequential circuitsfor target clockperiod since pipelining can eliminate all critical paths but not critical loops we concentrate on fpga synthesis toeliminate the critical loopswe combine the combinationalfunctional decomposition technique with retiming to performthe sequential functional decomposition and incorporate itin the label computation of turbomap to eliminate allcritical loopsthe results show significant improvementover the state of the art fpga mapping and resynthesis algorithms times reduction on the clock period moreover we develop novel approach for positive loop detectionwhich leads to over times speedup of the algorithmas result turbosyn can optimize sequential circuits ofover gates and flipflops in reasonable time
real time database management systems rt dbms have the necessary characteristics for providing efficient support to develop applications in which both data and transactions have temporal constraints however in the last decade new applications were identified and are characterized by large geographic distribution high heterogeneity lack of global control partial failures and lack of safety besides they need to manage large data volumes with real time constraints scheduling algorithms should consider transactions with soft deadlines and the concurrency control protocols should allow conflicting transactions to execute in parallel the last ones should be based in their requirements which are specified through both quality of services functions and performance metrics in this work method to model and develop applications that execute in open and unpredictable environments is proposed based on this model it is possible to perform analysis and simulations of systems to guide the decision making process and to identify solutions for improving it for validating the model case study considering the application domain of sensors network is discussed
in this paper we describe new via configurable routing architecture which shows much better throughput and performance than the previous structures we demonstrate how to construct single via mask fabric to reduce the mask cost further and we analyze the penalties which it incurs to solve the routability problem commonly existing in fabric based designs an efficient white space allocation and an incremental cell movement scheme are suggested which help to provide fast design convergence and early prediction of circuit's mappability to given fabric
array redistribution is usually needed for more efficiently executing data parallel program on distributed memory multicomputers to minimize the redistribution data transfer cost processor mapping techniques were proposed to reduce the amount of redistributed data elements theses techniques demand that the beginning data elements on processor not be redistributed in the redistribution on the other hand for satisfying practical computation needs programmer may require other data elements to be un redistributed localized in the redistribution in this paper we propose flexible processor mapping technique for the block cyclic redistribution to allow the programmer to localize the required data elements in the redistribution we also present an efficient redistribution method for the redistribution employing our proposed technique the data transfer cost reduction and system performance improvement for the redistributions with data localization are analyzed and presented in our experimental results
consider data warehouses as large data repositories queried for analysis and data mining in variety of application contexts query over such data may take large amount of time to be processed in regular pc consider partitioning the data into set of pcs nodes with either parallel database server or any database server at each node and an engine independent middleware nodes and network may even not be fully dedicated to the data warehouse in such scenario care must be taken for handling processing heterogeneity and availability so we study and propose efficient solutions for this we concentrate on three main contributions performance wise index measuring relative performance replication degree flexible chunk wise organization with on demand processing these contributions extend the previous work on de clustering and replication and are generic in the sense that they can be applied in very different contexts and with different data partitioning approaches we evaluate their merits with prototype implementation of the system
some of the most difficult questions to answer when designing distributed application are related to mobility what information to transfer between sites and when and how to transfer it network transparent distribution the property that program's behavior is independent of how it is partitioned among sites does not directly address these questions therefore we propose to extend all language entities with network behavior that enables efficient distributed programming by giving the programmer simple and predictable control over network communication patterns in particular we show how to give objects an arbitrary mobility behavior that is independent of the objects definition in this way the syntax and semantics of objects are the same regardless of whether they are used as stationary servers mobile agents or simply as caches these ideas have been implemented in distributed oz concurrent object oriented language that is state aware and has dataflow synchronization we prove that the implementation of objects in distributed oz is network transparent to satisfy the predictability condition the implementation avoids forwarding chains through intermediate sites the implementation is an extension to the publicly available dfki oz system
many spatiotemporal applications store moving object data in the form of trajectories various recent works have addressed interesting queries on trajectorial data mainly focusing on range queries and nearest neighbor queries here we examine another interesting query the time relaxed spatiotemporal trajectory join trstj which effectively finds groups of moving objects that have followed similar movements in different times we first attempt to address the trstj problem using symbolic representation algorithm which we have recently proposed for trajectory joins however we show experimentally that this solution produces false positives that grow rapidly with the increase of the problem size as result it is inefficient for trstj queries as it leads to large query time overhead in order to improve query performance we propose two important heuristics that turn the symbolic represenation approach effective for trstj queries our first improvement allows the use of multiple origins when processing strings representing trajectories the experimental evaluation shows that the multiple origin approach drastically reduces query performance we then present divide and conquer approach to further reduce false positives through symbolic class separation the proposed solutions can be combined together which leads to even better query performance we present an experimental study revealing the advantages of using these approaches for solving time relaxed spatiotemporal trajectory join queries
divide and conquer programs are easily parallelized by letting the programmer annotate potential parallelism in the form of spawn and sync constructs to achieve efficient program execution the generated work load has to be balanced evenly among the available cpus for single cluster systems random stealing rs is known to achieve optimal load balancing however rs is inefficient when applied to hierarchical wide area systems where multiple clusters are connected via wide area networks wans with high latency and low bandwidthin this paper we experimentally compare rs with existing load balancing strategies that are believed to be efficient for multi cluster systems random pushing and two variants of hierarchical stealing we demonstrate that in practice they obtain less than optimal results we introduce novel load balancing algorithm cluster aware random stealing crs which is highly efficient and easy to implement crs adapts itself to network conditions and job granularities and does not require manually tuned parameters although crs sends more data across the wans it is faster than its competitors for out of test applications with various wan configurations it has at most overhead in run time compared to rs on single large cluster even with high wide area latencies and low wide area bandwidths these strong results suggest that divide and conquer parallelism is useful model for writing distributed supercomputing applications on hierarchical wide area systems
counterexample guided abstraction refinement cegar is key technique for the verification of computer programs grumberg et al developed cegar based algorithm for the modal calculus there every abstract state is split in refinement step in this paper the work of grumberg et al is generalized by presenting new cegar based algorithm for the calculus it is based on more expressive abstract model and applies refinement only locally at single abstract state ie the lazy abstraction technique for safety properties is adapted to the calculus furthermore it separates refinement determination from the valued based model checking three different heuristics for refinement determination are presented and illustrated
trust management systems are frameworks for authorization in modern distributed systems allowing remotely accessible resources to be protected by providers by allowing providers to specify policy and access requesters to possess certain access rights trust management automates the process of determining whether access should be allowed on the basis of policy rights and an authorization semantics in this paper we survey modern state of the art in trust management authorization focusing on features of policy and rights languages that provide the necessary expressiveness for modern practice we characterize systems in light of generic structure that takes into account components of practical implementations we emphasize systems that have formal foundation since security properties of them can be rigorously guaranteed underlying formalisms are reviewed to provide necessary background
we show how range of role based access control rbac models may be usefully represented as constraint logic programs executable logical specifications the rbac models that we define extend the standard rbac models that are described by sandhu et al and enable security administrators to define range of access policies that may include features like denials of access and temporal authorizations that are often useful in practice but which are not widely supported in existing access control models representing access policies as constraint logic programs makes it possible to support certain policy options constraint checks and administrator queries that cannot be represented by using related methods like logic programs representing an access control policy as constraint logic program also enables access requests and constraint checks to be efficiently evaluated
this paper contributes to growing body of design patterns in interaction design for cooperative work while also describing how to go from field studies to design patterns it focuses on sociable face to face situations the patterns are based on field studies and design work in three sociable settings where desirable use qualities were identified and translated into forces in three design patterns for controlling information visibility on the basis of the patterns the design of multiple device multimedia platform is described it is shown that desirable qualities of systems in use can be utilised as forces in patterns which means that traditional qualitative research is highly valuable when documenting design knowledge in patterns three classes of interaction design patterns are identified environments for interactions means for interaction and interfaces for interaction these classes describe types of patterns within hierarchical model of interaction design
the wireless network community has become increasingly aware of the benefits of data driven link estimation and routing as compared with beacon based approaches but the issue of biased link sampling bls has not been well studied even though it affects routing convergence in the presence of network and environment dynamics focusing on traffic induced dynamics we examine the open unexplored question of how serious the bls issue is and how to effectively address it when the routing metric etx is used for wide range of traffic patterns and network topologies and using both node oriented and network wide analysis and experimentation we discover that the optimal routing structure remains quite stable even though the properties of individual links and routes vary significantly as traffic pattern changes in cases where the optimal routing structure does change data driven link estimation and routing is either guaranteed to converge to the optimal structure or empirically shown to converge to close to optimal structure these findings provide the foundation for addressing the bls issue in the presence of traffic induced dynamics and suggest approaches other than existing ones these findings also demonstrate that it is possible to maintain an optimal stable routing structure despite the fact that the properties of individual links and paths vary in response to network dynamics
mobile computation in which executing computations can move from one physical computing device to another is recurring theme from os process migration to language level mobility to virtual machine migration this article reports on the design implementation and verification of overlay networks to support reliable communication between migrating computations in the nomadic pict project we define two levels of abstraction as calculi with precise semantics low level nomadic pi calculus with migration and location dependent communication and high level calculus that adds location independent communication implementations of location independent communication as overlay networks that track migrations and forward messages can be expressed as translations of the high level calculus into the low we discuss the design space of such overlay network algorithms and define three precisely as such translations based on the calculi we design and implement the nomadic pict distributed programming language to let such algorithms and simple applications above them to be quickly prototyped we go on to develop the semantic theory of the nomadic pi calculi proving correctness of one example overlay network this requires novel equivalences and congruence results that take migration into account and reasoning principles for agents that are temporarily immobile eg waiting on lock elsewhere in the system the whole stands as demonstration of the use of principled semantics to address challenging system design problems
choosing good variable order is crucial for making symbolic state space generation algorithms truly efficient one such algorithm is the mdd based saturation algorithm for petri nets implemented in smart whose efficiency relies on exploiting event locality this paper presents novel static ordering heuristic that considers place invariants of petri nets in contrast to related work we use the functional dependencies encoded by invariants to merge decision diagram variables rather than to eliminate them we prove that merging variables always yields smaller mdds and improves event locality while eliminating variables may increase mdd sizes and break locality combining this idea of merging with heuristics for maximizing event locality we obtain an algorithm for static variable order which outperforms competing approaches regarding both time efficiency and memory efficiency as we demonstrate by extensive benchmarking
in this paper we consider the problem of processor allocation on mesh based multiprocessor systems we employ the idea of using migration to minimize fragmentation and the overall processing time of the tasks in our schemes we consider the use of task migration whenever required to improve the problem of fragmentation to this end we propose three efficient schemes to improve the performance of first fit allocation strategies commonly used in practice the first scheme called the first fit mesh bifurcation ffmb scheme attempts to start the search for free submesh from either the bottom left corner or the top left corner of the mesh so as to reduce the amount of fragmentation in the mesh the next two schemes called the online dynamic compaction single corner odc sc and online dynamic compaction four corners odc fc schemes use task migration to improve the performance of existing submesh allocation strategies we perform rigorous simulation experiments based on practical workloads as reported in the literature to quantify all our proposed schemes and compare them against standard schemes existing in the literature based on the results we make clear recommendations on the choice of the strategies
data exchange and virtual data integration have been the subject of several investigations in the recent literature at the same time the notion of peer data management has emerged as powerful abstraction of many forms of flexible and dynamic data centere ddistributed systems although research on the above issues has progressed considerably in the last years clear understanding on how to combine data exchange and data integration in peer data management is still missing this is the subject of the present paper we start our investigation by first proposing novel framework for peer data exchange showing that it is generalization of the classical data exchange setting we also present algorithms for all the relevant data exchange tasks and show that they can all be done in polynomial time with respect to data complexity based on the motivation that typical mappings and integrity constraints found in data integration are not captured by peer data exchange we extend the framework to incorporate these features one of the main difficulties is that the constraints of this new class are not amenable to materialization we address this issue by resorting to suitable combination of virtual and materialized data exchange showing that the resulting framework is generalization of both classical data exchange and classical data integration and that the new setting incorporates the most expressive types of mapping and constraints considered in the two contexts finally we present algorithms for all the relevant data management tasks also in the new setting and show that again their data complexity is polynomial
radiance transfer represents how generic source lighting is shadowed and scattered by an object to produce view dependent appearance we generalize by rendering transfer at two scales macro scale is coarsely sampled over an object's surface providing global effects like shadows cast from an arm onto body meso scale is finely sampled over small patch to provide local texture low order spherical harmonics represent low frequency lighting dependence for both scales to render coefficient vector representing distant source lighting is first transformed at the macro scale by matrix at each vertex of coarse mesh the resulting vectors represent spatially varying hemisphere of lighting incident to the meso scale function called radiance transfer texture rtt then specifies the surface's meso scale response to each lighting basis component as function of spatial index and view direction finally dot product of the macro scale result vector with the vector looked up from the rtt performs the correct shading integral we use an id map to place rtt samples from small patch over the entire object only two scalars are specified at high spatial resolution results show that bi scale decomposition makes preprocessing practical and efficiently renders self shadowing and interreflection effects from dynamic low frequency light sources at both scales
we present multiple pass streaming algorithms for basic clustering problem for massive data sets if our algorithm is allotted passes it will produce an approximation with error at most epsilon using otilde epsilon bits of memory the most critical resource for streaming computation we demonstrate that this tradeoff between passes and memory allotted is intrinsic to the problem and model of computation by proving lower bounds on the memory requirements of any pass randomized algorithm that are nearly matched by our upper bounds to the best of our knowledge this is the first time nearly matching bounds have been proved for such an exponential tradeoff for randomized computationin this problem we are given set of points drawn randomly according to mixture of uniform distributions and wish to approximate the density function of the mixture the points are placed in datastream possibly in adversarial order which may only be read sequentially by the algorithm we argue that this models among others the datastream produced by national census of the incomes of all citizens
we present unified feature representation of pointclouds and apply it to face recognition the representation integrates local and global geometrical cues in single compact representation which makes matching probe to large database computationally efficient the global cues provide geometrical coherence for the local cues resulting in better descriptiveness of the unified representation multiple rank tensors scalar features are computed at each point from its local neighborhood and from the global structure of the pointcloud forming multiple rank tensor fields the pointcloud is then represented by the multiple rank tensor fields which are invariant to rigid transformations each local tensor field is integrated with every global field in histogram which is indexed by local field in one dimension and global field in the other dimension finally pca coefficients of the histograms are concatenated into single feature vector the representation was tested on frgc data set and achieved identification and verification rate at far
we present method for learning model of human body shape variation from corpus of range scans our model is the first to capture both identity dependent and pose dependent shape variation in correlated fashion enabling creation of variety of virtual human characters with realistic and non linear body deformations that are customized to the individual our learning method is robust to irregular sampling in pose space and identity space and also to missing surface data in the examples our synthesized character models are based on standard skinning techniques and can be rendered in real time
unconstrained consumer photos pose great challenge for content based image retrieval unlike professional images or domain specific images consumer photos vary significantly more often than not the objects in the photos are ill posed occluded and cluttered with poor lighting focus and exposure in this paper we propose cascading framework for combining intra image and inter class similarities in image retrieval motivated from probabilistic bayesian principles support vector machines are employed to learn local view based semantics based on just in time fusion of color and texture features new detection driven block based segmentation algorithm is designed to extract semantic features from images the detection based indexes also serve as input for support vector learning of image classifiers to generate class relative indexes during image retrieval both intra image and inter class similarities are combined to rank images experiments using query by example on genuine heterogeneous consumer photos with semantic queries show that the combined matching approach is better than matching with single index it also outperformed the method of combining color and texture features by in average precision
compiler based auto parallelization is much studied area yet has still not found wide spread application this is largely due to the poor exploitation of application parallelism subsequently resulting in performance levels far below those which skilled expert programmer could achieve we have identified two weaknesses in traditional parallelizing compilers and propose novel integrated approach resulting in significant performance improvements of the generated parallel code using profile driven parallelism detection we overcome the limitations of static analysis enabling us to identify more application parallelism and only rely on the user for final approval in addition we replace the traditional target specific and inflexible mapping heuristics with machine learning based prediction mechanism resulting in better mapping decisions while providing more scope for adaptation to different target architectures we have evaluated our parallelization strategy against the nas and spec omp benchmarks and two different multi core platforms dual quad core intel xeon smp and dual socket qs cell blade we demonstrate that our approach not only yields significant improvements when compared with state of the art parallelizing compilers but comes close to and sometimes exceeds the performance of manually parallelized codes on average our methodology achieves of the performance of the hand tuned openmp nas and spec parallel benchmarks on the intel xeon platform and gains significant speedup for the ibm cell platform demonstrating the potential of profile guided and machine learning based parallelization for complex multi core platforms
we present new variational method for multi view stereovision and non rigid three dimensional motion estimation from multiple video sequences our method minimizes the prediction error of the shape and motion estimates both problems then translate into generic image registration task the latter is entrusted to global measure of image similarity chosen depending on imaging conditions and scene properties rather than integrating matching measure computed independently at each surface point our approach computes global image based matching score between the input images and the predicted images the matching process fully handles projective distortion and partial occlusions neighborhood as well as global intensity information can be exploited to improve the robustness to appearance changes due to non lambertian materials and illumination changes without any approximation of shape motion or visibility moreover our approach results in simpler more flexible and more efficient implementation than in existing methods the computation time on large datasets does not exceed thirty minutes on standard workstation finally our method is compliant with hardware implementation with graphics processor units our stereovision algorithm yields very good results on variety of datasets including specularities and translucency we have successfully tested our motion estimation algorithm on very challenging multi view video sequence of non rigid scene
interfaces based on recognition technologies are used extensively in both the commercial and research worlds but recognizers are still error prone and this results in human performance problems brittle dialogues and other barriers to acceptance and utility of recognition systems interface techniques specialized to recognition systems can help reduce the burden of recognition errors but building these interfaces depends on knowledge about the ambiguity inherent in recognition we have extended user interface toolkit in order to model and to provide structured support for ambiguity at the input event level this makes it possible to build re usable interface components for resolving ambiguity and dealing with recognition errors these interfaces can help to reduce the negative effects of recognition errors by providing these components at toolkit level we make it easier for application writers to provide good support for error handling further with this robust support we are able to explore new types of interfaces for resolving more varied range of ambiguity
we present an efficient approach for end to end out of core construction and interactive inspection of very large arbitrary surface models the method tightly integrates visibility culling and out of core data management with level of detail framework at preprocessing time we generate coarse volume hierarchy by binary space partitioning the input triangle soup leaf nodes partition the original data into chunks of fixed maximum number of triangles while inner nodes are discretized into fixed number of cubical voxels each voxel contains compact direction dependent approximation of the appearance of the associated volumetric sub part of the model when viewed from distance the approximation is constructed by visibility aware algorithm that fits parametric shaders to samples obtained by casting rays against the full resolution dataset at rendering time the volumetric structure maintained off core is refined and rendered in front to back order exploiting vertex programs for gpu evaluation of view dependent voxel representations hardware occlusion queries for culling occluded subtrees and asynchronous for detecting and avoiding data access latencies since the granularity of the multiresolution structure is coarse data management traversal and occlusion culling cost is amortized over many graphics primitives the efficiency and generality of the approach is demonstrated with the interactive rendering of extremely complex heterogeneous surface models on current commodity graphics platforms
paraphrasing van rijsbergen the time is ripe for another attempt at using natural language processing nlp for information retrieval ir this paper introduces my dissertation study which will explore methods for integrating modern nlp with state of the art ir techniques in addition to text will also apply retrieval to conversational speech data which poses unique set of considerations in comparison to text greater use of nlp has potential to improve both text and speech retrieval
on line analytical processing olap is technology basically created to provide users with tools in order to explore and navigate into data cubes unfortunately in huge and sparse data exploration becomes tedious task and the simple user's intuition or experience does not lead to efficient results in this paper we propose to exploit the results of the multiple correspondence analysis mca in order to enhance data cube representations and make them more suitable for visualization and thus easier to analyze our approach addresses the issues of organizing data in an interesting way and detects relevant facts our purpose is to help the interpretation of multidimensional data by efficient and simple visual effects to validate our approach we compute its efficiency by measuring the quality of resulting multidimensional data representations in order to do so we propose an homogeneity criterion to measure the visual relevance of data representations this criterion is based on the concept of geometric neighborhood and similarity between cells experimental results on real data have shown the interest of using our approach on sparse data cubes
there is growing demand for provisioning of different levels of quality of service qos on scalable web servers to meet changing resource availability and to satisfy different client requirements in this paper we investigate the problem of providing proportional qos differentiation with respect to response time on web servers we first present processing rate allocation scheme based on the foundations of queueing theory it provides different processing rates to requests of different client classes so as to achieve the differentiation objective at application level process is used as the resource allocation principal for achieving processing rates on apache web servers we design and implement an adaptive process allocation approach guided by the queueing theoretical rate allocation scheme on an apache server this application level implementation however shows weak qos predictability because it does not have fine grained control over the consumption of resources that the kernel consumes and hence the processing rate is not strictly proportional to the number of processes allocated we then design feedback controller and integrate it with the queueing theoretical approach it adjusts process allocations according to the difference between the target response time and the achieved response time using proportional integral derivative controller experimental results demonstrate that this integrated approach can enable web servers to provide robust proportional response time differentiation
we present hybridpointing technique that lets users easily switch between absolute and relative pointing with direct input device such as pen our design includes new graphical element the trailing widget which remains close at hand but does not interfere with normal cursor operation the use of visual feedback to aid the user's understanding of input state is discussed and several novel visual aids are presented an experiment conducted on large wall sized display validates the benefits of hybridpointing under certain conditions we also discuss other situations in which hybridpointing may be useful finally we present an extension to our technique that allows for switching between absolute and relative input in the middle of single drag operation
many adaptive routing algorithms have been proposed for wormhole routed interconnection networks comparatively little work however has been done on determining how the selection function routing policy affects the performance of an adaptive routing algorithm in this paper we present detailed simulation study of various selection functions for fully adaptive wormhole routing on two dimensional meshes the simulation results show that the choice of selection function has significant effect on the average message latency in addition it is possible to find single selection function that exhibits excellent performance across wide range of traffic patterns network sizes and number of virtual channels thus well chosen selection function for an adaptive routing algorithm can lead to consistently better performance than an arbitrary selection function one of the selection functions considered is theoretically optimal selection function ieee trans comput october we show that although theoretically optimal the actual performance of the optimal selection function is not always best an explanation and interpretation of the results is provided
imperative and object oriented programs make ubiquitous use of shared mutable objects updating shared object can and often does transgress boundary that was supposed to be established using static constructs such as class with private fields this paper shows how auxiliary fields can be used to express two state dependent encapsulation disciplines ownership kind of separation and friendship kind of sharing methodology is given for specification and modular verification of encapsulated object invariants and shown sound for class based language as an example the methodology is used to specify iterators which are problematic for previous ownership systems
energy consumption is of significant concern in battery operated embedded systems in the processors of such systems the instruction cache consumes significant fraction of the total energy one of the most popular methods to reduce the energy consumption is to shut down idle cache banks however we observe that operating idle cache banks at reduced voltage frequency level along with the active banks in pipelined manner can potentially achieve even better energy savings in this paper we propose novel dvs based pipelined reconfigurable instruction memory hierarchy called prim canonical example of our proposed prim consists of four cache banks two of these cache banks can be configured at runtime to operate at lower voltage and frequency levels than that of the normal cache instruction fetch throughput is maintained by pipelining the accesses to the low voltage banks we developed profile driven compilation framework that analyzes applications and inserts the appropriate cache reconfiguration points our experimental results show that prim can significant reduce the energy consumption for popular embedded benchmarks with minimal performance overhead we obtained and energy savings for aggressive and conservative vdd settings respectively at the expense of performance overhead